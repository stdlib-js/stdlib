@techreport{beebe:2002,
	abstract = {These notes describe an implementation of an algorithm for accurate computation of \\( \operatname{expm1}(x) = \operatname{exp}(x) − 1 \\), one of the new elementary functions introduced in the 1999 ISO C Standard, but already available in most UNIX C implementations. A test package modeled after the Cody and Waite Elementary Function Test Package, ELEFUNT, is developed to evaluate the accuracy of implementations of \\( \operatorname{expm1}(x) \\).},
	author = {Nelson H.F. Beebe},
	keywords = {mathematics, math, special function, exponential, exp},
	institution = {University of Utah},
	title = {{Computation of expm1(x) = exp(x) − 1}},
	url = {http://www.math.utah.edu/~beebe/reports/expm1.pdf},
	year = {2002},
}

@article{blair:1976,
	abstract = {This report presents near-minimax rational approximations for the inverse of the error function \\( \operatorname{inverf}(x) \\), for  \\( 0 \leqslant x \leqslant 1 - {10^{ - 10000}} \\), with relative errors ranging down to  \\( {10^{ - 23}}\\). An asymptotic formula for the region \\( x \to 1\\) is also given.},
	author = {J.M. Blair and C.A. Edwards and J.H. Johnson},
	doi = {10.1090/S0025-5718-1976-0421040-7},
	journal = {Mathematics of Computation},
	keywords = {Rational Chebyshev approximations, inverse error function, minimal Newton form, erf, erfinv, mathematics, math, special function},
	pages = {827--830},
	title = {{Rational Chebyshev approximations for the inverse of the error function}},
	url = {http://www.ams.org/journals/mcom/1976-30-136/S0025-5718-1976-0421040-7/},
	volume = {30},
	year = {1976},
}

@article{blei:2003,
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	acmid = {944937},
	author = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {topic modeling, machine learning, natural language processing, mixed-membership},
	month = {mar},
	pages = {993--1022},
	title = {{Latent dirichlet allocation}},
	url = {http://dl.acm.org/citation.cfm?id=944937},
	volume = {3},
	year = {2003},
}

@inproceedings{borwein:1991,
	abstract = {A very simple class of algorithms for the computation of the Riemann-zeta function to arbitrary precision in arbitrary domains is proposed. These algorithms compete with the standard methods based on Euler-Maclaurin summation, are easier to implement and are easier to analyze.},
	author = {P. Borwein},
	booktitle = {Conference Proceedings},
	keywords = {Riemann zeta function, computation, high precision, algorithm, mathematics, math, special function, riemann, zeta},
	publisher = {Canadian Mathematical Society},
	title = {{An Efficient Algorithm for the Riemann Zeta Function}},
	year = {1991},
}

@article{carlitz:1963,
	author = {L. Carlitz},
	journal = {Pacific Journal of Mathematics},
	keywords = {mathematics, math, error function, erf},
	number = {2},
	pages = {459--470},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{The inverse of the error function.}},
	url = {http://msp.org/pjm/1963/13-2/pjm-v13-n2-p06-s.pdf},
	volume = {13},
	year = {1963},
}

@mastersthesis{segarra:2006,
	abstract = {Identified as one of the 7 Millennium Problems, the Riemann zeta hypothesis has successfully evaded mathematicians for over 100 years. Simply stated, Riemann conjectured that all of the nontrivial zeroes of his zeta function have real part equal to 1/2. This thesis attempts to explore the theory behind Riemann’s zeta function by first starting with Euler’s zeta series and building up to Riemann’s function. Along the way we will develop the math required to handle this theory in hopes that by the end the reader will have immersed themselves enough to pursue their own exploration and research into this fascinating subject.},
	author = {Elan Segarra},
	keywords = {math, mathematics, riemann zeta, riemann, zeta, special function},
	school = {Harvey Mudd College},
	title = {{An Exploration of the Riemann Zeta Function and its Application to the Theory of Prime Number Distribution}},
	url = {https://www.math.hmc.edu/seniorthesis/archives/2006/esegarra/esegarra-2006-thesis.pdf},
	year = {2006},
}

@book{fishman:1973,
	address = {New York, NY, USA},
	author = {George S. Fishman},
	isbn = {9780471261551},
	keywords = {system analysis, simulation, science, math, random number generation},
	lccn = {73005713},
	month = {sep},
	publisher = {Wiley},
	series = {A Wiley-Interscience publication},
	title = {{Concepts and methods in discrete event digital simulation}},
	year = {1973},
}

@article{fransen:1980,
	abstract = {In this paper we determine numerical values to 80D of the coefficients in the Taylor series expansion \\( {\Gamma ^m}(s + x) = \Sigma _0^\infty {g_k}(m,s){x^k}\\) for certain values of \\(m\\) and \\(s\\) and use these values to calculate  \\( \Gamma (p/q)\;(p,q = 1,2, \ldots ,10;\;p < q)\\) and  \\({\min _{x > 0}}\Gamma (x)\\) to 80D. Finally, we obtain a high-precision value of the integral  \\(\smallint _0^\infty {(\Gamma (x))^{ - 1}}\;dx\\).},
	author = {Arne Frans{\'{e}}n and Staffan Wrigge},
	doi = {10.1090/S0025-5718-1980-0559204-5},
	journal = {Mathematics of Computation},
	keywords = {Special functions, Gamma function, Riemann Zeta function, gamma, mathematics, math, special function},
	pages = {553--566},
	title = {{High-precision values of the gamma function and of some related coefficients}},
	url = {http://www.ams.org/journals/mcom/1980-34-150/S0025-5718-1980-0559204-5/},
	volume = {34},
	year = {1980},
}

@article{goldberg:1991,
	abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
	acmid = {103163},
	address = {New York, NY, USA},
	author = {David Goldberg},
	doi = {10.1145/103162.103163},
	journal = {ACM Comput. Surv.},
	issn = {0360-0300},
	issue_date = {March 1991},
	keywords = {NaN, denormalized number, exception, floating-point, floating-point standard, gradual underflow, guard digit, overflow, relative error, rounding error, rounding mode, ulp, underflow, computation, ieee754, math, double, float},
	month = {mar},
	number = {1},
	numpages = {44},
	pages = {5--48},
	publisher = {ACM},
	title = {What Every Computer Scientist Should Know About Floating-point Arithmetic},
	url = {http://doi.acm.org/10.1145/103162.103163},
	volume = {23},
	year = {1991},
}

@unpublished{gourdon:2003,
	author = {Xavier Gourdon and Pascal Sebah},
	keywords = {riemann zeta, riemann, zeta, math, mathematics, special function},
	note = {Numerical evaluation of the Riemann Zeta-function.},
	title = {{Numerical evaluation of the Riemann Zeta-function}},
	year = {2003},
}

@article{griffiths:2004,
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	author = {Thomas L Griffiths and Mark Steyvers},
	doi = {10.1073/pnas.0307752101},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {topic modeling, bayesian, documentation, models, statistical, monte carlo method, markov chain, gibbs},
	month = {apr},
	pages = {5228--35},
	pmid = {14872004},
	title = {{Finding scientific topics}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC387300},
	volume = {101 Suppl 1},
	year = {2004},
}

@article{khajah:1994,
	abstract = {We describe a machine independent Fortran subroutine which performs the four basic arithmetic operations with a degree of accuracy prescribed by the user. Tables of Chebyshev expansions of orders 48 and 50 for some basic mathematical functions are obtained as a result of applying this subroutine in conjunction with the recursive formulation of the Tau Method. A recently devised technique for the sharp determination of upper and lower error bounds for Tau Method approximations (see [1]) enables us to find the degree n required to achieve a prescribed accuracy ϵ over a given interval [a,b]. A number of practical illustrations are given.},
	author = {H.G. Khajah and E.L. Ortiz},
	doi = {10.1016/0898-1221(94)90148-1},
	issn = {0898-1221},
	journal = {Computers & Mathematics with Applications},
	keywords = {numeric computing, precision, programming, computation},
	number = {7},
	pages = {41--57},
	title = {Ultra-high precision computations},
	url = {http://www.sciencedirect.com/science/article/pii/0898122194901481},
	volume = {27},
	year = {1994},
}

@phdthesis{pugh:2004,
	abstract = {This thesis is an analysis of C. Lanczos’ approximation of the classical gamma function \\( \Gamma(z+1) \\) as given in his 1964 paper \textit{A Precision Approximation of the Gamma Function}. The purposes of this study are: (i) to explain the details of Lanczos’ paper, including proofs of all claims made by the author; (ii) to address the question of how best to implement the approximation method in practice; and (iii) to generalize the methods used in the derivation of the approximation.},
	author = {Glendon Ralph Pugh},
	keywords = {gamma, lanczos, math, mathematics, special function},
	school = {The University of British Columbia},
	title = {{An Analysis of the Lanczos Gamma Approximation}},
	url = {https://web.viu.ca/pughg/phdThesis/phdThesis.pdf},
	year = {2004}
}

@article{meurer:2016,
	abstract = {SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become the standard symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select domain specific submodules. The supplementary materials provide additional examples and further outline details of the architecture and features of SymPy.},
	author = {A. Meurer and C.P. Smith and M. Paprocki and O. {\u{C}}ert{\'{i}}k and S.B. Kirpichev and M. Rocklin and A. Kumar and S. Ivanov and J.K. Moore and S. Singh and T. Rathnayake and S. Vig and B.E. Granger and R.P. Muller and F. Bonazzi and H. Gupta and S. Vats and F. Johansson and F. Pedregosa and M.J. Curry and A.R. Terrel and {\u{S}}. Rou{\u{c}}ka and A. Saboo and I. Fernando and S. Kulal and R. Cimrman and A. Scopatz},
	doi = {10.7287/peerj.preprints.2083v3},
	journal = {PeerJ Preprints},
	keywords = {symbolic, python, computer algebra system},
	month = {jun},
	number = {2083},
	title = {{SymPy: Symbolic computing in Python.}},
	url = {https://github.com/sympy/sympy-paper},
	volume = {4},
	year = {2016},
}

@unpublished{cao:high,
	abstract = {This paper presents the design and implementation of several fundamental dense linear algebra (DLA) algorithms in OpenCL. In particular, these are linear system solvers and eigenvalue problem solvers. Further, we give an overview of the clMAGMA library, an open source, high performance OpenCL library that incorporates the developments presented, and in general provides to heterogeneous architectures the DLA functionality of the popular LAPACK library. The LAPACK-compliance and use of OpenCL simplify the use of clMAGMA in applications, while providing them with portably performant DLA. High performance is obtained through use of the high-performance OpenCL BLAS, hardware and OpenCL-specific tuning, and a hybridization methodology where we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware components by minimizing data movements and mapping algorithmic requirements to the architectural strengths of the various heterogeneous hardware components.},
    author = {Chongxiao Cao and Jack Dongarra and Peng Du and Mark Gates and Piotr Luszczek and Stanimire Tomov},
    keywords = {gpu, opencl, blas, numeric computing, computation, lapack},
    note = {netlib},
    title = {{clMAGMA: High Performance Dense Linear Algebra with OpenCL}},
}

@misc{dawson:2013,
	author = {Bruce Dawson},
	keywords = {ieee754, floating-point, math, mathematics, numeric computing, computation},
	title = {{Floating-Point Determinism}},
	url = {https://randomascii.wordpress.com/2013/07/16/floating-point-determinism/},
	year = {2013}
}

@misc{godfrey:2001,
	author = {Paul Godfrey},
	keywords = {lanczos, gamma, special function, math, mathematics},
	title = {{A note on the computation of the convergent Lanczos complex Gamma approximation.}},
	url = {http://my.fit.edu/~gabdo/gamma.txt},
	year = {2001}
}

@misc{toth:gamma,
	author = {Viktor T. Toth},
	keywords = {lanczos, gamma, special function, math, mathematics},
	title = {{The Gamma function}},
	url = {http://www.rskey.org/CMS/index.php/the-library/11},
}

@article{vigna:2014,
	abstract = {xorshift\sup{*} generators are a variant of Marsaglia's xorshift generators that eliminate linear artifacts typical of generators based on \\(Z/2Z\\)-linear operations using multiplication by a suitable constant. Shortly after high-dimensional xorshift\sup{*} generators were introduced, Saito and Matsumoto suggested a different way to eliminate linear artifacts based on addition in \\(Z/2^{32}Z\\), leading to the XSadd generator. Starting from the observation that the lower bits of XSadd are very weak, as its reverse fails systematically several statistical tests, we explore xorshift+, a variant of XSadd using 64-bit operations, which leads, in small dimension, to extremely fast high-quality generators.},
	author = {Sebastiano Vigna},
	doi = {},
	journal = {CoRR},
	keywords = {random, prng, rng, marsaglia, xorshift, uniform, rand},
	month = {apr},
	number = {},
	pages = {},
	title = {{Further scramblings of Marsaglia's xorshift generators}},
	url = {https://arxiv.org/abs/1404.0390},
	volume = {abs/1404.0390},
	year = {2014},
}

@article{vose:1991,
	abstract = {Let \\(\xi\\) be a random variable over a finite set with an arbitrary probability distribution. Improvements to a fast method of generating sample values for \\(\xi\\) in constant time are suggested.},
	author = {Michael D. Vose},
	doi = {10.1109/32.92917},
	issn = {00985589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {random, rand, prng, rng, discrete},
	month = {sep},
	number = {9},
	pages = {972--975},
	title = {{A linear algorithm for generating random numbers with a given distribution}},
	volume = {17},
	year = {1991},
}

@article{marsaglia:2003,
	abstract = {Description of a class of simple, extremely fast random number generators (RNGs) with periods \\(2^k −1\\) for \\(k = 32, 64, 96, 128, 160, 192\\). These RNGs seem to pass tests of randomness very well.},
	author = {George Marsaglia},
	doi = {10.18637/jss.v008.i14},
	journal = {Journal of Statistical Software},
	keywords = {random, rand, marsaglia, prng, rng, xorshift, uniform},
	note = {xoshift},
	number = {14},
	title = {{Xorshift RNGs}},
	url = {https://www.jstatsoft.org/article/view/v008i14},
	volume = {8},
	year = {2003},
}

@article{panneton:2005,
	abstract = {G. Marsaglia recently introduced a class of very fast xorshift random number generators, whose implementation uses three “xorshift” operations. They belong to a large family of generators based on linear recurrences modulo 2, which also includes shift-register generators, the Mersenne twister, and several others. In this article, we analyze the theoretical properties of xorshift generators, search for the best ones with respect to the equidistribution criterion, and test them empirically. We find that the vast majority of xorshift generators with only three xorshift operations, including those having good equidistribution, fail several simple statistical tests. We also discuss generators with more than three xorshifts.},
	acmid = {1113319},
	address = {New York, NY, USA},
	author = {Fran{\c{c}}ois Panneton and Pierre L'Ecuyer},
	doi = {10.1145/1113316.1113319},
	issn = {1049-3301},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	issue_date = {October 2005},
	keywords = {random number generation, linear feedback shift register, linear recurrence modulo 2, xorshift, prng, rng, random, rand},
	month = {oct},
	number = {4},
	numpages = {16},
	pages = {346--361},
	publisher = {ACM},
	title = {On the Xorshift Random Number Generators},
	url = {http://doi.acm.org/10.1145/1113316.1113319},
	volume = {15},
	year = {2005},
}

@article{hellekalek:1998,
	abstract = {Every random number generator has its advantages and deficiencies. There are no "safe" generators. The practitioner's problem is how to decide which random number generator will suit his needs best. In this paper, we will discuss criteria for good random number generators: theoretical support, empirical evidence and practical aspects. We will study several recent algorithms that perform better than most generators in actual use. We will compare the different methods and supply numerical results as well as selected pointers and links to important literature and other sources. Additional information on random number generation, including the code of most algorithms discussed in this paper is available from our web-server under the address http://random.mat.sbg.ac.at/.},
	acmid = {284161},
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {P. Hellekalek},
	doi = {10.1016/S0378-4754(98)00078-0},
	issn = {0378-4754},
	issue_date = {June 1998},
	journal = {Mathematics and Computers in Simulation},
	month = {jun},
	number = {5-6},
	numpages = {21},
	pages = {485--505},
	publisher = {Elsevier Science Publishers B. V.},
	title = {{Good Random Number Generators Are (Not So) Easy to Find}},
	url = {http://dx.doi.org/10.1016/S0378-4754(98)00078-0},
	volume = {46},
	year = {1998},
}

@article{ahrens:1974,
	abstract = {Accurate computer methods are evaluated which transform uniformly distributed random numbers into quantities that follow gamma, beta, Poisson, binomial and negative-binomial distributions. All algorithms are designed for variable parameters. The known convenient methods are slow when the parameters are large. Therefore new procedures are introduced which can cope efficiently with parameters of all sizes. Some algorithms require sampling from the normal distribution as an intermediate step. In the reported computer experiments the normal deviates were obtained from a recent method which is also described.},
	author = {J.H. Ahrens and U. Dieter},
	doi = {10.1007/BF02293108},
	issn = {1436-5057},
	journal = {Computing},
	keywords = {random, prng, rng, rand, beta, computation, pseudorandom, number, generator},
	number = {3},
	pages = {223--246},
	title = {{Computer methods for sampling from gamma, beta, poisson and bionomial distributions}},
	url = {http://dx.doi.org/10.1007/BF02293108},
	volume = {12},
	year = {1974},
}

@article{hormann:1993a,
	abstract = {The transformed rejection method, a combination of inversion and rejection, which can be applied to various continuous distributions, is well suited to generate binomial random variates as well. The resulting algorithms are simple and fast, and need only a short set-up. Among the many possible variants two algorithms are described and tested: BTRS a short but nevertheless fast rejection algorithm and BTRD which is more complicated as the idea of decomposition is utilized. For BTRD the average number of uniforms required to return one binomial deviate less between 2.5 and 1.4 which is considerably lower than for any of the known uniformly fast algorithms. Timings for a C-implementation show that for the case that the parameters of the binomial distribution vary from call to call BTRD is faster than the current state of the art algorithms. Depending on the computer, the speed of the uniform generator used and the binomial parameters the savings are between 5 and 40 percent.},
	author = {Wolfgang H{\"{o}}rmann},
	doi = {10.1080/00949659308811496},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rand, prng, rng, binomial, pseudorandom, number, generator},
	number = {1-2},
	pages = {101-110},
	title = {{The generation of binomial random variates}},
	url = {http://dx.doi.org/10.1080/00949659308811496},
	volume = {46},
	year = {1993},
}

@article{box:1958,
	abstract = {},
	author = {G. E. P. Box and Mervin E. Muller},
	doi = {10.1214/aoms/1177706645},
	issn = {00034851},
	journal = {The Annals of Mathematical Statistics},
	keywords = {random, prng, rng, pseudorandom, rand, normal, gaussian, number, generator},
	month = {jun},
	number = {2},
	pages = {610--611},
	publisher = {The Institute of Mathematical Statistics},
	title = {{A Note on the Generation of Random Normal Deviates}},
	url = {http://dx.doi.org/10.1214/aoms/1177706645},
	volume = {29},
	year = {1958}
}

@article{bell:1968,
	abstract = {},
	acmid = {363547},
	address = {New York, NY, USA},
	author = {James R. Bell},
	doi = {10.1145/363397.363547},
	issn = {0001-0782},
	issue_date = {July 1968},
	journal = {Communications of the ACM},
	keywords = {frequency distribution, normal deviates, normal distribution, probability distribution, random, random number, random number generator, simulation, prng, rng},
	month = {jul},
	number = {7},
	pages = {498--},
	publisher = {ACM},
	title = {{Algorithm 334: Normal Random Deviates}},
	url = {http://doi.acm.org/10.1145/363397.363547},
	volume = {11},
	year = {1968},
}

@article{knop:1969,
	abstract = {},
	acmid = {362996},
	address = {New York, NY, USA},
	author = {R. Knop},
	doi = {10.1145/362946.362996},
	issn = {0001-0782},
	issue_date = {May 1969},
	journal = {Communications of the ACM},
	keywords = {frequency distribution, normal deviates, normal distribution, probability distribution, random, random number, random number generator, simulation},
	month = {may},
	number = {5},
	pages = {281--},
	publisher = {ACM},
	title = {{Remark on Algorithm 334 [G5]: Normal Random Deviates}},
	url = {http://doi.acm.org/10.1145/362946.362996},
	volume = {12},
	year = {1969},
}

@article{marsaglia:1964a,
	abstract = {A normal random variable \\(X\\) may be generated in terms of uniform random variables \\(u_1\\), \\(u_2\\), in the following simple way: 86 percent of the time, put \\(X = 2(u_1 + u_2 + u_3 - 1.5)\\),11 percent of the time, put \\(X = 1.5(u_1 + u_2 - 1)\\), and the remaining 3 percent of the time, use a more complicated procedure so that the resulting mixture is correct. This method takes only half again as long as the very fastest methods, is much simpler, and requires very little storage space.},
	author = {G. Marsaglia and T. A. Bray},
	doi = {10.1137/1006063},
	issn = {00361445},
	journal = {SIAM Review},
	keywords = {prng, rng, random, rand, pseudorandom, number, generator, normal, gaussian},
	month = {sep},
	number = {3},
	pages = {260--264},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{A Convenient Method for Generating Normal Variables}},
	url = {http://epubs.siam.org/doi/abs/10.1137/1006063},
	volume = {6},
	year = {1964},
}

@article{thomas:2007,
	abstract = {Rapid generation of high quality Gaussian random numbers is a key capability for simulations across a wide range of disciplines. Advances in computing have brought the power to conduct simulations with very large numbers of random numbers and with it, the challenge of meeting increasingly stringent requirements on the quality of Gaussian random number generators (GRNG). This article describes the algorithms underlying various GRNGs, compares their computational requirements, and examines the quality of the random numbers with emphasis on the behaviour in the tail region of the Gaussian probability density function.},
	acmid = {1287622},
	address = {New York, NY, USA},
	articleno = {11},
	author = {David B. Thomas and Wayne Luk and Philip H.W. Leong and John D. Villasenor},
	doi = {10.1145/1287620.1287622},
	issn = {0360-0300},
	issue_date = {2007},
	journal = {ACM Computing Surveys},
	keywords = {gaussian, random numbers, normal, simulation, random, rand, prng, rng, pseudorandom},
	month = {nov},
	number = {4},
	publisher = {ACM},
	title = {{Gaussian Random Number Generators}},
	url = {http://doi.acm.org/10.1145/1287620.1287622},
	volume = {39},
	year = {2007},
}

@article{marsaglia:2000a,
	abstract = {We offer a procedure for generating a gamma variate as the cube of a suitably scaled normal variate. It is fast and simple, assuming one has a fast way to generate normal variables. In brief: generate a normal variate \\(x\\) and a uniform variate \\(U\\) until \\(\ln(U) < 0.5 + d - dv + \ln(v)\\), then return \\(dv\\). Here, the gamma parameter is \\(\alpha \geq 1\\), and \\(v = (1 + x/ *** with \\(d = \alpha - 1/3\\). The efficiency is high, exceeding \\(0.951, 0.981, 0.992, 0.996\\) at \\(\alpha = 1,2,4,8\\). The procedure can be made to run faster by means of a simple squeeze that avoids the two logarithms most of the time; return \\(dv\\) if \\(U < 1 - 0.0331\\). We give a short C program for any \\(\alpha \geq 1\\), and show how to boost an \\(\alpha \lt 1\\) into an \\(\alpha \gt 1\\). The gamma procedure is particularly fast for C implementation if the normal variate is generated in-line, via the #define feature. We include such an inline version, based on our ziggurat method. With it, and an inline uniform generator, gamma variates can be produced in 400MHz CPUs at better than 1.3 million per second, with the parameter \\(\alpha\\) changing from call to call.},
	acmid = {358414},
	address = {New York, NY, USA},
	author = {George Marsaglia and Wai Wan Tsang},
	doi = {10.1145/358407.358414},
	issue_date = {Sept. 2000},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {gamma distribution, random number generation, ziggurat method, random, prng, rng, pseudorandom, gamma},
	month = {sep},
	number = {3},
	numpages = {10},
	pages = {363--372},
	publisher = {ACM},
	issn = {0098-3500},
	title = {{A Simple Method for Generating Gamma Variables}},
	url = {http://doi.acm.org/10.1145/358407.358414},
	volume = {26},
	year = {2000},
}

@article{kachitvichyanukul:1985,
    abstract = {The paper presents an exact, uniformly fast algorithm for generating random variates from the hypergeometric distribution. The overall algorithm framework is acceptance/ rejection and is implemented via composition. Three subdensities are used, one is uniform and the other two are exponential. The algorithm is compared with algorithms based on sampling without replacement, inversion, and aliasing. A comprehensive survey of existing algorithms is also given.},
	author = {Voratas Kachitvichyanukul and Burce Schmeiser},
	doi = {10.1080/00949658508810839},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rand, prng, rng, pseudorandom, number, generator, hypergeometric},
	number = {2},
	pages = {127-145},
	title = {{Computer generation of hypergeometric random variates}},
	url = {http://dx.doi.org/10.1080/00949658508810839},
	volume = {22},
	year = {1985},
}

@article{nadler:2006,
	abstract = {\\(\em{Ziggurat}\\) and \\(\em{Monty Python}\\) are two fast and elegant methods proposed by Marsaglia and Tsang to transform uniform random variables to random variables with normal, exponential and other common probability distributions. While the proposed methods are theoretically correct, we show that there are various design flaws in the uniform pseudo random number generators (PRNG's) of their published implementations for both the normal and Gamma distributions. These flaws lead to non-uniformity of the resulting pseudo-random numbers and consequently to noticeable deviations of their outputs from the required distributions. In addition, we show that the underlying uniform PRNG of the published implementation of MATLAB's \texttt{randn}, which is also based on the Ziggurat method, is not uniformly distributed with correlations between consecutive pairs. Also, we show that the simple linear initialization of the registers in MATLAB's \texttt{randn} may lead to non-trivial correlations between output sequences initialized with different (related or even random unrelated) seeds. These, in turn, may lead to erroneous results for stochastic simulations.},
	author = {Boaz Nadler},
	doi = {},
	journal = {{arXiv}},
	keywords = {random, prng, rng, marsaglia, ziggurat, gaussian, monty python, uniform, rand},
	month = {mar},
	number = {},
	pages = {},
	title = {{Design Flaws in the Implementation of the Ziggurat and Monty Python methods (and some remarks on MATLAB randn)}},
	url = {https://arxiv.org/abs/math/0603058},
	volume = {abs/math/0603058},
	year = {2006},
}

@article{mcfarland:2016,
    abstract = {The ziggurat algorithm is a very fast rejection sampling method for generating pseudorandom numbers (PRNs) from statistical distributions. In the algorithm, rectangular sampling domains are layered on top of each other (resembling a ziggurat) to encapsulate the desired probability density function. Random values within these layers are sampled and then returned if they lie beneath the graph of the probability density function. Here, we present an implementation where ziggurat layers reside completely beneath the probability density function, thereby eliminating the need for any rejection test within the ziggurat layers. In the new algorithm, small overhanging segments of probability density remain to the right of each ziggurat layer, which can be efficiently sampled with triangularly shaped sampling domains. Median runtimes of the new algorithm for exponential and normal variates is reduced to 58\% and 53\%, respectively (collective range: 41–93\%). An accessible C library, along with extensions into Python and MATLAB/Octave are provided.},
	author = {Christopher D. McFarland},
	doi = {10.1080/00949655.2015.1060234},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rang, prng, rng, pseudorandom, ziggurat, gaussian, normal},
	number = {7},
	pages = {1281--1294},
	title = {{A modified ziggurat algorithm for generating exponentially and normally distributed pseudorandom numbers}},
	url = {http://dx.doi.org/10.1080/00949655.2015.1060234},
	volume = {86},
	year = {2016},
}

@unpublished{doornik:2005,
	abstract = {The ziggurat is an efficient method to generate normal random samples. It is shown that the standard Ziggurat fails a commonly used test. An improved version that passes the test is introduced. Flexibility is enhanced by using a plug-in uniform random number generator. An efficient double-precision version of the ziggurat algorithm is developed that has a very high period.},
	author = {Jurgen A. Doornik},
	keywords = {random, rand, prng, rng, ziggurat, normal, gaussian, pseudorandom},
	note = {Provides an improved algorithm for generating normally distributed pseudorandom numbers via a ziggurat algorithm.},
	title = {{An Improved Ziggurat Method to Generate Normal Random Samples}},
	url = {https://www.doornik.com/research/ziggurat.pdf},
	year = {2005},
}

@article{marsaglia:2000b,
	abstract = {We provide a new version of our ziggurat method for generating a random variable from a given decreasing density. It is faster and simpler than the original, and will produce, for example, normal or exponential variates at the rate of 15 million per second with a C version on a 400MHz PC. It uses two tables, integers \\(k_i\\), and reals \\(w_i\\). Some 99% of the time, the required \\(x\\) is produced by: Generate a random 32-bit integer \\(j\\) and let \\(i\\) be the index formed from the rightmost 8 bits of j. If \\(j < k\\), return \\(x = j \cdot w_i\\). We illustrate with C code that provides for inline generation of both normal and exponential variables, with a short procedure for setting up the necessary tables.},
	author = {George Marsaglia and Wai Wan Tsang},
	doi = {10.18637/jss.v005.i08},
	issn = {1548-7660},
	journal = {Journal of Statistical Software},
	keywords = {random, rand, prng, rng, pseudorandom, number, generator, ziggurat, normal, gaussian},
	number = {1},
	pages = {1--7},
	title = {{The Ziggurat Method for Generating Random Variables}},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v005i08},
	volume = {5},
	year = {2000},
}

@article{marsaglia:1964b,
	abstract = {},
	author = {George Marsaglia},
	doi = {10.1080/00401706.1964.10490150},
	journal = {Technometrics},
	keywords = {random, prng, rng, pseudorandom, number, generator, rand, normal, gaussian},
	number = {1},
	pages = {101-102},
	title = {{Generating a Variable from the Tail of the Normal Distribution}},
	url = {http://dx.doi.org/10.1080/00401706.1964.10490150},
	volume = {6},
	year = {1964},
}

@article{park:1988,
	abstract = {Practical and theoretical issues are presented concerning the design, implementation, and use of a good, minimal standard random number generator that will port to virtually all systems.},
	acmid = {63042},
	address = {New York, NY, USA},
	author = {S. K. Park and K. W. Miller},
	doi = {10.1145/63039.63042},
	issn = {0001-0782},
	issue_date = {Oct. 1988},
	journal = {Communications of the ACM},
	keywords = {random, prng, rng, pseudorandom, number, generator},
	month = {oct},
	number = {10},
	numpages = {10},
	pages = {1192--1201},
	publisher = {ACM},
	title = {{Random Number Generators: Good Ones Are Hard to Find}},
	url = {http://doi.acm.org/10.1145/63039.63042},
	volume = {31},
	year = {1988},
}

@book{press:1992,
	author = {William H. Press and Brian P. Flannery and Saul A. Teukolsky and William T. Vetterling},
	isbn = {0521431085},
	keywords = {computing, software, programming},
	publisher = {Cambridge University Press},
	title = {{Numerical Recipes in C: The Art of Scientific Computing, Second Edition}},
	year = {1992},
}

@article{bays:1976,
	abstract = {},
	acmid = {355670},
	address = {New York, NY, USA},
	author = {Carter Bays and S. D. Durham},
	doi = {10.1145/355666.355670},
	issue_date = {March 1976},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {lcg, random, rnd, minstd, rand, rng, prng, pseudorandom, number, generator, linear congruential generator},
	month = {mar},
	number = {1},
	numpages = {6},
	issn = {0098-3500},
	pages = {59--64},
	publisher = {ACM},
	title = {{Improving a Poor Random Number Generator}},
	url = {http://doi.acm.org/10.1145/355666.355670},
	volume = {2},
	year = {1976},
}

@book{herzog:2002,
	author = {T.N. Herzog and G. Lord},
	isbn = {9781566984331},
	keywords = {finance, monte carlo, prng, rng, random, minstd, minstd-shuffle},
	lccn = {2002015049},
	pages = {19--},
	publisher = {ACTEX Publications},
	title = {{Applications of Monte Carlo Methods to Finance and Insurance}},
	url = {https://books.google.com/books?id=vC7I\_gdX-A0C},
	year = {2002},
}

@book{knuth:1997,
	address = {Boston, MA, USA},
	author = {Donald E. Knuth},
	isbn = {0-201-89684-2},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	title = {{The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms}},
	year = {1997},
}

@article{hormann:1993b,
	abstract = {The transformed rejection method, a combination of the inversion and the rejection method, which is used to generate non-uniform random numbers from a variety of continuous distributions can be applied to discrete distributions as well. For the Poisson distribution a short and simple algorithm is obtained which is well suited for large values of the Poisson parameter \\(\mu\\), even when \\(\mu\\) may vary from call to call. The average number of uniform deviates required is lower than for any of the known uniformly fast algorithms. Timings for a C implementation show that the algorithm needs only half of the code but is - for \\(\mu\\) not too small - at least as fast as the current state-of-the-art algorithms.},
	author = {W. H{\"{o}}rmann},
	doi = {10.1016/0167-6687(93)90997-4},
	issn = {0167-6687},
	journal = {Insurance: Mathematics and Economics},
	keywords = {Rejection method, Inversion, Decomposition, Poisson variate generation, Uniformly fast algorithm},
	note = {},
	number = {1},
	pages = {39--45},
	title = {{The transformed rejection method for generating Poisson random variables}},
	url = {http://www.sciencedirect.com/science/article/pii/0167668793909974},
	volume = {12},
	year = {1993},
}

@article{riehle:2012,
	abstract = {The design of software development tools follows from what the developers of such tools believe is true about software development. A key aspect of such beliefs is the size of code contributions (commits) to a software project. In this paper, we show that what tool developers think is true about the size of code contributions is different by more than an order of magnitude from reality. We present this reality, called the commit size distribution, for a large sample of open source and selected closed source projects. We suggest that these new empirical insights will help improve software development tools by aligning underlying design assumptions closer with reality.},
	author = {Dirk Riehle and Carsten Kolassa and Michel A. Salim},
	issn = {1617-5468},
	journal = {Software Engineering 2012, GI-Edition Lecture Notes in Informatics},
	keywords = {software, git, commits, developer, computer science},
	pages ={59--70},
	title = {{Developer Belief vs. Reality: The Case of the Commit Size Distribution}},
	url = {http://arxiv.org/abs/1408.4644},
	volume = {abs/1408.4644},
	year = {2012},
}

@inproceedings{arafat:2009a,
	abstract = {With the growing economic importance of open source, we need to improve our understanding of how open source software development processes work. The analysis of code contributions to open source projects is an important part of such research. In this paper we analyze the size of code contributions to more than 9,000 open source projects. We review the total distribution and distinguish three categories of code contributions using a size-based heuristic: single focused commits, aggregate team contributions, and repository refactorings. We find that both the overall distribution and the individual categories follow a power law. We also suggest that distinguishing these commit categories by size will benefit future analyses.},
	author = {Oliver Arafat and Dirk Riehle},
	booktitle = {Proceedings of the 42nd Hawaii International Conference on System Sciences},
	keywords = {software, computer science, metrics, development, developer, git, commits},
	publisher = {IEEE Computer Society},
	title = {{The Commit Size Distribution of Open Source Software}},
	year = {2009},
}

@inbook{hofmann:2009,
	abstract = {The quantitative analysis of software projects can provide insights that let us better understand open source and other software development projects. An important variable used in the analysis of software projects is the amount of work being contributed, the commit size. Unfortunately, post-facto, the commit size can only be estimated, not measured. This paper presents several algorithms for estimating the commit size. Our performance evaluation shows that simple, straightforward heuristics are superior to the more complex text-analysis-based algorithms. Not only are the heuristics significantly faster to compute, they also deliver more accurate results when estimating commit sizes. Based on this experience, we design and present an algorithm that improves on the heuristics, can be computed equally fast, and is more accurate than any of the prior approaches.},
	address = {Berlin, Heidelberg},
	author = {Philipp Hofmann and Dirk Riehle},
	bookTitle = {{Open Source Ecosystems: Diverse Communities Interacting: 5th IFIP WG 2.13 International Conference on Open Source Systems, OSS 2009, Sk\"{o}vde, Sweden, June 3-6, 2009. Proceedings}},
	doi = {10.1007/978-3-642-02032-2_11},
	editor = {Cornelia Boldyreff and Kevin Crowston and Bj\"{o}rn Lundell and Anthony I. Wasserman},
	isbn = {978-3-642-02032-2},
	pages = {105--115},
	publisher = {Springer Berlin Heidelberg},
	title = {{Estimating Commit Sizes Efficiently}},
	url = {http://dx.doi.org/10.1007/978-3-642-02032-2_11},
	year = {2009},
}

@article {joanes:1998,
	abstract = {Over the years, various measures of sample skewness and kurtosis have been proposed. Comparisons are made between those measures adopted by well-known statistical computing packages, focusing on bias and mean-squared error for normal samples, and presenting some comparisons from simulation results for non-normal samples.},
	author = {D. N. Joanes and C. A. Gill},
	doi = {10.1111/1467-9884.00122},
	issn = {1467-9884},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	keywords = {bias, kurtosis, mean-squared error, skewness, algorithms, math, mathematics, statistics, stats},
	number = {1},
	pages = {183--189},
	publisher = {Blackwell Publishers Ltd},
	title = {{Comparing measures of sample skewness and kurtosis}},
	url = {http://dx.doi.org/10.1111/1467-9884.00122},
	volume = {47},
	year = {1998},
}

@inproceedings{hattori:2008,
	abstract = {Information contained in versioning system commits has been frequently used to support software evolution research. Concomitantly, some researchers have tried to relate commits to certain activities, e.g., large commits are more likely to be originated from code management activities, while small ones are related to development activities. However, these characterizations are vague, because there is no consistent definition of what is a small or a large commit. In this paper, we study the nature of commits in two dimensions. First, we define the size of commits in terms of number of files, and then we classify commits based on the content of their comments. To perform this study, we use the history log of nine large open source projects.},
	author = {Lile P. Hattori and Michele Lanza},
	booktitle = {ASE Workshops 2008. 23rd IEEE/ACM International Conference on Automated Software Engineering - Workshops, 2008.},
	doi = {10.1109/ASEW.2008.4686322},
	keywords = {history, software systems, frequency, statistical distributions, inspection, informatics, research and development management, content management, project management, engineering management, metrics, git, commits},
	publisher = {IEEE},
	title = {{On the nature of commits}},
	year = {2008},
}

@manual{cocomo2,
	author = {Barry Boehm},
	edition = {1.4},
	keywords = {software, estimation, management, development},
	organization = {University of Southern California},
	title = {{COCOMO II Model Definition Manual}},
	url = {http://www.dmi.usherb.ca/~frappier/IFT721/COCOMOII.PDF}
}

@inproceedings{arafat:2009b,
	abstract = {The development processes of open source soft-ware are different from traditional closed source development processes. Still, open source software is frequently of high quality. This raises the question of how and why open source software creates high quality and whether it can maintain this quality for ever larger project sizes. In this paper, we look at one particular quality indicator, the density of comments in open source software code. We find that successful open source projects follow a consistent practice of documenting their source code, and we find that the comment density is independent of team and project size.},
	acmid = {1640047},
	address = {New York, NY, USA},
	author = {Oliver Arafat and Dirk Riehle},
	booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
	doi = {10.1145/1639950.1640047},
	isbn = {978-1-60558-768-4},
	keywords = {comment density, data mining, empirical software engineering, open source, software evolution},
	location = {Orlando, Florida, USA},
	numpages = {8},
	pages = {857--864},
	publisher = {ACM},
	series = {OOPSLA '09},
	title = {{The Commenting Practice of Open Source}},
	url = {http://doi.acm.org/10.1145/1639950.1640047},
	year = {2009},
}

@inbook{kolassa:2013a,
	abstract = {A fundamental unit of work in programming is the code contribution (“commit”) that a developer makes to the code base of the project in work. We use statistical methods to derive a model of the probabilistic distribution of commit sizes in open source projects and we show that the model is applicable to different project sizes. We use both graphical as well as statistical methods to validate the goodness of fit of our model. By measuring and modeling a fundamental dimension of programming we help improve software development tools and our understanding of software development.},
	address = {Berlin, Heidelberg},
	author = {Carsten Kolassa and Dirk Riehle and Michel A. Salim},
	bookTitle = {SOFSEM 2013: Theory and Practice of Computer Science: 39th International Conference on Current Trends in Theory and Practice of Computer Science, \v{S}pindler\r{u}v Ml\'{y}n, Czech Republic, January 26-31, 2013. Proceedings},
	editor = {Peter van Emde Boas and Frans C. A. Groen and Giuseppe F. Italiano and Jerzy Nawrocki and Harald Sack},
	doi = {10.1007/978-3-642-35843-2_6},
	isbn = {978-3-642-35843-2},
	keywords = {software, development, git, commits, management, estimation},
	pages = {52--66},
	publisher = {Springer Berlin Heidelberg},
	title = {{A Model of the Commit Size Distribution of Open Source}},
	url = {http://dx.doi.org/10.1007/978-3-642-35843-2_6},
	year = {2013},
}

@inproceedings{kolassa:2013b,
	abstract = {A fundamental unit of work in programming is the code contribution ("commit") that a developer makes to the code base of the project in work. An author's commit frequency describes how often that author commits. Knowing the distribution of all commit frequencies is a fundamental part of understanding software development processes. This paper presents a detailed quantitative analysis of commit frequencies in open-source software development. The analysis is based on a large sample of open source projects, and presents the overall distribution of commit frequencies.\n\nWe analyze the data to show the differences between authors and projects by project size; we also includes a comparison of successful and non successful projects and we derive an activity indicator from these analyses. By measuring a fundamental dimension of programming we help improve software development tools and our understanding of software development. We also validate some fundamental assumptions about software development.},
	acmid = {2491073},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Carsten Kolassa and Dirk Riehle and Michel A. Salim},
	booktitle = {Proceedings of the 9th International Symposium on Open Collaboration},
	doi = {10.1145/2491055.2491073},
	isbn = {978-1-4503-1852-5},
	keywords = {software, development, management, estimation, commits, git, metrics},
	location = {Hong Kong, China},
	numpages = {8},
	pages = {18:1--18:8},
	publisher = {ACM},
	series = {WikiSym '13},
	title = {{The Empirical Commit Frequency Distribution of Open Source Projects}},
	url = {http://doi.acm.org/10.1145/2491055.2491073},
	year = {2013},
}

@article{hope:1968,
	abstract = {The use of Monte Carlo test procedures for significance testing, with smaller reference sets than are now generally used, is advocated. It is shown that, for given \\(\alpha = 1/n\\), \\(n\\) a positive integer, the power of the Monte Carlo test procedure is a monotone increasing function of the size of the reference set, the limit of which is the power of the corresponding uniformly most powerful test. The power functions and efficiency of the Monte Carlo test to the uniformly most powerful test are discussed in detail for the case where the test criterion is \\(N(\gamma, 1)\\). The cases when the test criterion is Student's t-statistic and when the test statistic is exponentially distributed are considered also.},
	author = {Adery C. A. Hope},
	issn = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	keywords = {chisq, chisquare, chi-squared, chi-square, goodness-of-fit, monte carlo, simulate, simulation},
	number = {3},
	pages = {582--598},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {{A Simplified Monte Carlo Significance Test Procedure}},
	url = {http://www.jstor.org/stable/2984263},
	volume = {30},
	year = {1968}
}

@article{patefield:1981,
	author = {W. M. Patefield},
	issn = {1467-9876},
	journal = {Applied Statistics},
	keywords = {algorithm, algo, statistics, stats, chi-square, chi2, chi-squared, chisquare, chisquared, contingency table},
	month = {mar},
	number = {1},
	pages = {91--97},
	title = {{Statistical Algorithms: {Algorithm AS 159}: An Efficient Method of Generating Random \\(R \times C\\) Tables with Given Row and Column Totals}},
	url = {http://lib.stat.cmu.edu/apstat/159},
	volume = {30},
	year = {1981},
}

@book{devroye:1986,
	author = {Luc Devroye},
	doi = {10.1007/978-1-4613-8643-8},
	isbn = {978-1-4613-8643-8},
	keywords = {random, pseudorandom, prng, rng, rand, generator, algorithms},
	publisher = {Springer New York},
	title = {{Non-Uniform Random Variate Generation}},
	url = {http://www.nrbook.com/devroye/},
	year = {1986},
}

@article{stein:1967,
	abstract = {In a program written by the author, for performing various calculations in the Racah algebra, some new calculational methods were used; these methods are described in this paper. They include: (1) a representation of numbers alternative to the Rotenberg form; (2) a new algorithm for calculating the greatest common devisor of two odd integers, which was developed for reducing the calculating time in the new representation; (3) methods for shortening the computation time of 3-j and 6-j symbols.},
	author = {Josef Stein},
	doi = {10.1016/0021-9991(67)90047-2},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	keywords = {gcd, gcf, hcf, gcm, math, mathematics, algorithm, arithmetic, greatest, common, divisor},
	number = {3},
	pages = {397--405},
	title = {{Computational problems associated with Racah algebra}},
	url = {http://www.sciencedirect.com/science/article/pii/0021999167900472},
	volume = {1},
	year = {1967},
}

@mastersthesis{steinarsson:2013,
	abstract = {As human beings, we often wish to visualize certain information in order to make better sense of it. This can be a somewhat challenging enterprise for large amounts of data and might require downsampling the data, retaining only the important visual characteristics. The focus of this thesis is to explore methods for downsampling data which can be visualized in the form of a line chart, for example, time series. Several algorithms are put forth in the thesis and their features are discussed. Also, an online survey was conducted where participants were asked to compare downsampled line charts against a non-downsampled chart. Some of the algorithms are based on a well-known technique in cartography which involves forming triangles between adjacent data points and using the area of the triangles to determine the perceptual importance of the individual points. According to the survey, algorithms based on a triangle area approach consistently proved to be effective, and one in particular when efficiency is also taken into account.},
	author = {Svein Steinarsson},
	keywords = {downsample, timeseries, plot, visualize, visualization, dataviz, datavis, time-series, time, series},
	school = {University of Iceland},
	title = {{Downsampling Time Series for Visual Representation}},
	url = {http://skemman.is/handle/1946/15343},
	year = {2013}
}

@unpublished{junod:1999,
	abstract = {Random numbers are critical in every aspect of cryptography. We need such numbers to encrypt e-mails, to digitally sign documents, for electronic payment systems, and so on. Unfortunately, true random numbers are very difficult to generate, especially on computers that are typically designed to be deterministic. This brings us to the concept of pseudo-random numbers, which are numbers generated from some random internal values, and that are very hard for an observer to distinguish from true random numbers. It is important to see the difference between the meaning of pseudo-random numbers in normal programming contexts, like simulation, e.g., where these numbers merely need to be reasonably random-looking and have good statistical properties, and in the context of cryptography, where they must be indistinguishable from real random numbers, even to observers with huge amount of computational resources. In the context of cryptography, a random number is a number that cannot be predicted by an observer before it is generated. Typically, if the number is to be in the range \\([0..n − 1]\\), an observer cannot predict that number with probability "slightly" better than \\(1/n\\). Or, we will see that the following is equivalent, if \\(m\\) random numbers are generated in a row, an observer given any \\(m − 1\\) of them still cannot predict the \\(m\\)'th with a probability significantly greater than \\(1/n\\_. In this work, we present first the notion of cryptographic secure pseudorandom bit generators (PRBG) in a formal way by using two different definitions. Then a theorem of Yao proving the equivalence of these two definitions is treated. In a second part, the Blum-Blum-Shub generator, a very simple and provably secure PRBG, is presented, with all the mathematical background needed to understand it. In the third part, the proof of its security is treated in details.},
	author = {Pascal Junod},
	keywords = {prng, rng, blum blum shub, random, pseudorandom, crypto, cryptographic, secure},
	note = {},
	title = {{Cryptographic Secure Pseudo-Random Bits Generation : The Blum-Blum-Shub Generator}},
	url = {http://www.cs.miami.edu/home/burt/learning/Csc609.062/docs/bbs.pdf},
	year = {1999},
}

@inproceedings{sidorenko:2005,
	acmid = {2179250},
	address = {Berlin, Heidelberg},
	abstract = {The asymptotic security of the Blum-Blum-Shub (BBS) pseudorandom generator has been studied by Alexi et al. and Vazirani and Vazirani, who proved independently that O(log log N) bits can be extracted on each iteration, where \\(N\\) is the modulus (a Blum integer). The concrete security of this generator has been analyzed previously by Fischlin and Schnorr and by Knuth. In this paper we continue to analyse the concrete security the BBS generator. We show how to select both the size of the modulus and the number of bits extracted on each iteration such that a desired level of security is reached, while minimizing the computational effort per output bit. We will assume a concrete lower bound on the hardness of integer factoring, which is obtained by extrapolating the best factorization results to date. While for asymptotic security it suffices to give a polynomial time reduction a successful attack to factoring, we need for concrete security a reduction that is as efficient as possible. Our reduction algorithm relies on the techniques of Fischlin and Schnorr, as well as ideas of Vazirani and Vazirani, but combining these in a novel way for the case that more than one bit is output on each iteration.},
	author = {Andrey Sidorenko and Berry Schoenmakers},
	booktitle = {Proceedings of the 10th International Conference on Cryptography and Coding},
	doi = {10.1007/11586821_24},
	isbn = {3-540-30276-X, 978-3-540-30276-6},
	keywords = {prng, rng, random, pseudorandom, crypto, cryptographic, secure, blum blum shub},
	location = {Cirencester, UK},
	numpages = {21},
	pages = {355--375},
	publisher = {Springer-Verlag},
	series = {IMA'05},
	title = {{Concrete Security of the Blum-blum-shub Pseudorandom Generator}},
	url = {http://dx.doi.org/10.1007/11586821_24},
	year = {2005},
}

@article{blum:1983a,
	abstract = {Two closely-related pseudo-random sequence generators are presented: The \\(1 / P\\) generator, with input \\(P\\) a prime, outputs the quotient digits obtained on dividing \\(1\\) by \\(P\\). The \\(x^2 \bmod N\\) generator with inputs \\(N\\), \\(x_0\\) (where \\(N = P \cdot Q\\) is a product of distinct primes, each congruent to \\(3 \mod 4\\), and \\(x_0\\) is a quadratic residue \\(\bmod N\\)), outputs \\(b_0 b_1 b_2 \cdots \\) where \\(b_i = \operatorname{parity}(x_i )\\) and \\(x_{i + 1} = x_i^2 \bmod N\\). From short seeds each generator efficiently produces long well-distributed sequences. Moreover, both generators have computationally hard problems at their core. The first generator's sequences, however, are completely predictable (from any small segment of \\(2|P| + 1\\) consecutive digits one can infer the "seed," \\(P\\), and continue the sequence backwards and forwards), whereas the second, under a certain intractability assumption, is unpredictable in a precise sense. The second generator has additional interesting properties: from knowledge of \\(x_0\\) and \\(N\\) but not \\(P\\) or \\(Q\\), one can generate the sequence forwards, but, under the above-mentioned intractability assumption, one can not generate the sequence backwards. From the additional knowledge of \\(P\\) and \\(Q\\), one can generate the sequence backwards; one can even "jump" about from any point in the sequence to any other. Because of these properties, the \\(x^2 \bmod N\\) generator promises many interesting applications, e.g., to public-key cryptography. To use these generators in practice, an analysis is needed of various properties of these sequences such as their periods. This analysis is begun here.},
	author = {Lenore Blum and Manuel Blum and Mike Shub},
	doi = {10.1137/0215025},
	journal = {SIAM Journal on Computing},
	keywords = {prng, rng, random, rand, blum blum shub, crypto, cryptographic, secure},
	month = {Aug},
	number = {2},
	pages = {364–-383},
	title = {{A Simple Unpredictable Pseudo-Random Number Generator}},
	url = {http://epubs.siam.org/doi/abs/10.1137/0215025},
	volume = {15},
	year = {1983},
}

@inbook{blum:1983b,
	abstract = {What do we want from a pseudo-random sequence generator? Ideally, we would like a pseudo-random sequence generator to quickly produce, from short seeds, long sequences (of bits) that appear in every way to be generated by successive flips of a fair coin.},
	address = {Boston, MA},
	author = {Lenore Blum and Manuel Blum and Mike Shub},
	booktitle = {Advances in Cryptology: Proceedings of Crypto 82},
	doi = {10.1007/978-1-4757-0602-4_6},
	editor= {David Chaum and Ronald L. Rivest and Alan T. Sherman},
	isbn = {978-1-4757-0602-4},
	keywords = {prng, rng, random, rand, blum blum shub, crypto, cryptographic, secure},
	publisher = {Springer US},
	pages = {61--78},
	title = {{Comparison of Two Pseudo-Random Number Generators}},
	url = {http://dx.doi.org/10.1007/978-1-4757-0602-4_6},
	year = {1983},
}

@article{lamport:1999,
	abstract = {Most specification languages have a type system. Type systems are hard to get right, and getting them wrong can lead to inconsistencies. Set theory can serve as the basis for a specification language without types. This possibility, which has been widely overlooked, offers many advantages. Untyped set theory is simple and is more flexible than any simple typed formalism. Polymorphism, overloading, and subtyping can make a type system more powerful, but at the cost of increased complexity, and such refinements can never attain the flexibility of having no types at all. Typed formalisms have advantages, too, stemming from the power of mechanical type checking. While types serve little purpose in hand proofs, they do help with mechanized proofs. In the absence of verification, type checking can catch errors in specifications. It may be possible to have the best of both worlds by adding typing annotations to an untyped specification language. We consider only specification languages, not programming languages.},
	acmid = {319317},
	address = {New York, NY, USA},
	author = {Leslie Lamport and 	Lawrence C. Paulson},
	doi = {10.1145/319301.319317},
	issn = {0164-0925},
	issue_date = {May 1999},
	journal = {ACM Transactions on Programming Languages and Systems},
	keywords = {set theory, specification, types},
	month = {may},
	number = {3},
	numpages = {25},
	pages = {502--526},
	publisher = {ACM},
	title = {{Should Your Specification Language Be Typed}},
	url = {http://doi.acm.org/10.1145/319301.319317},
	volume = {21},
	year = {1999},
}

@book{lyons:2011,
	author = {Richard G. Lyons},
	keywords = {digital signal processing, dsp},
	publisher = {Prentice Hall},
	title = {{Understanding Digital Signal Processing, 3rd Edition}},
	year = {2011},
}

@techreport{selakovic:2015,
	abstract = {As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-efficiency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that inefficient usage of APIs is the most prevalent root cause. Furthermore, we find that most issues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns.},
	author = {Marija Selakovic and Michael Pradel},
	institution = {TU Darmstadt, Department of Computer Science},
	keywords = {javascript, js, performance, perf, optimization, optimize, v8, chrome, firefox, spidermonkey},
	title = {{Performance Issues and Optimizations in JavaScript: An Empirical Study}},
	url = {http://mp.binaervarianz.de/JS_perf_study_TR_Oct2015.pdf},
	year = {2015},
}

@article{avelino:2016a,
	abstract = {Truck Factor (TF) is a metric proposed by the agile community as a tool to identify concentration of knowledge in software development environments. It states the minimal number of developers that have to be hit by a truck (or quit) before a project is incapacitated. In other words, TF helps to measure how prepared is a project to deal with developer turnover. Despite its clear relevance, few studies explore this metric. Altogether there is no consensus about how to calculate it, and no supporting evidence backing estimates for systems in the wild. To mitigate both issues, we propose a novel (and automated) approach for estimating TF-values, which we execute against a corpus of 133 popular project in GitHub. We later survey developers as a means to assess the reliability of our results. Among others, we find that the majority of our target systems (65%) have TF <= 2. Surveying developers from 67 target systems provides confidence towards our estimates; in 84% of the valid answers we collect, developers agree or partially agree that the TF's authors are the main authors of their systems; in 53% we receive a positive or partially positive answer regarding our estimated truck factors.},
	author = {Guilherme Avelino and Leonardo Passos and Andr{\'{e}} Hora and Marco Tulio Valente},
	doi = {},
	journal = {CoRR},
	keywords = {bus, truck, factor, software, metrics, measure, sustainability, development, meta},
	month = {apr},
	note = {For discussion, see https://news.ycombinator.com/item?id=9874503.},
	number = {},
	pages = {},
	title = {{A Novel Approach for Estimating Truck Factors}},
	url = {http://arxiv.org/abs/1604.06766},
	volume = {abs/1604.06766},
	year = {2016},
}

@article{muller:2005a,
	abstract = {Function ulp (acronym for unit in the last place) is frequently used for expressing errors in floating-point computations. We present several previously suggested definitions of that function, and analyse some of their properties.},
	author = {Jean-Michel Muller},
	doi = {},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {floating-point, arithmetic, computer arithmetic, unit in the last place, ulp, ieee754, precision, accuracy, math, mathematics, epsilon},
	month = {nov},
	number = {N},
	pages = {},
	title = {{On the definition of ulp(x)}},
	url = {http://ljk.imag.fr/membres/Carine.Lucas/TPScilab/JMMuller/ulp-toms.pdf},
	volume = {V},
	year = {2005},
}

@book{chall:1995a,
	author = {Jeanne Sternlicht Chall and Edgar Dale},
	isbn = {9781571290083},
	keywords = {nlp, english, language, lang, readability, readable, difficulty},
	publisher = {Brookline Books},
	title = {{Readability revisited: the new Dale-Chall readability formula}},
	url = {https://books.google.com/books?id=2nbuAAAAMAAJ},
	year = {1995},
}

@article{klare:1974a,
	abstract = {One of the problems in public education and mass communication is how to tell whether a particular piece of writing is likely to be readable to a particular group of readers. Two major solutions are possible: measuring and predicting readability. Measuring, by judgments or tests, involves using readers. Predicting by readability formulas, does not involve readers but instead uses counts of language elements in the piece of writing. This article reviews formulas and related predictive devices since 1960. Four categories are presented: 1) recalculations and revisions of existing formulas; 2) new formulas, for general purpose or special purpose use; 3) application aids, for both manual and machine use; and, 4) predictions of readability for foreign languages. It concludes with suggestions for choosing a formula, based upon the following considerations: 1) special versus general needs, 2) manual versus machine application, 3) simple versus complex formulas, 4) word length versus word list formulas; and, 5) sentence length versus sentence complexity. Finally, the article stresses that formulas provide good indices of difficulty, but do not indicate causes of difficulty or say how to write readably.},
	author = {George R. Klare},
	doi = {},
	issn = {00340553},
	journal = {Reading Research Quarterly},
	keywords = {readability, nlp, language, lang, english, en, readable, words, data},
	number = {1},
	pages = {62--102},
	publisher = {Wiley, International Reading Association},
	title = {{Assessing Readability}},
	url = {http://www.jstor.org/stable/747086},
	volume = {10},
	year = {1974},
}

@article{spache:1953a,
	author = {George Spache},
	doi = {10.1086/458513},
	journal = {The Elementary School Journal},
	keywords = {readability, readable, language, lang, nlp, english, en, dataset, data, words, simple},
	number = {7},
	pages = {410--413},
	title = {{A New Readability Formula for Primary-Grade Reading Materials}},
	url = {http://dx.doi.org/10.1086/458513},
	volume = {53},
	year = {1953},
}

@article{stone:1956a,
	author = {Clarence R. Stone},
	issn = {00135984, 15548279},
	journal = {The Elementary School Journal},
	keywords = {spache, readability, readable, language, lang, english, metric, formula, measure},
	number = {1},
	pages = {36--41},
	publisher = {University of Chicago Press},
	title = {{Measuring Difficulty of Primary Reading Material: A Constructive Criticism of Spache's Measure}},
	url = {http://www.jstor.org/stable/999700},
	volume = {57},
	year = {1956},
}

@incollection{perera:2012a,
	author = {Katherine Perera},
	booktitle = {{Linguistics and the Teacher}},
	chapter = {7},
	editor = {Ronald Carter},
	isbn = {9781136515422},
	keywords = {readability, readable, difficulty, english, en, metrics, measures, language, lang, linguistics, nlp},
	pages = {101--113},
	publisher = {Taylor \& Francis},
	series = {Routledge Library Editions: Education},
	title = {{The assessment of linguistic difficulty in reading material}},
	url = {https://books.google.com/books?id=oNXFQ9Gn6XIC},
	year = {2012},
}

@unpublished{miller:2008a,
	abstract = {For large UNIX projects, the traditional method of building the project is to use recursive make. On some projects, this results in build times which are unacceptably large, when all you want to do is change one file. In examining the source of the overly long build times, it became evident that a number of apparently unrelated problems combine to produce the delay, but on analysis all have the same root cause. This paper explores a number of problems regarding the use of recursive make, and shows that they are all symptoms of the same problem. Symptoms that the UNIX community have long accepted as a fact of life, but which need not be endured any longer. These problems include recursive makes which take "forever" to work out that they need to do nothing, recursive makes which do too much, or too little, recursive makes which are overly sensitive to changes in the source code and require constant Makefile intervention to keep them working. The resolution of these problems can be found by looking at what make does, from first principles, and then analyzing the effects of introducing recursive make to this activity. The analysis shows that the problem stems from the artificial partitioning of the build into separate subsets. This, in turn, leads to the symptoms described. To avoid the symptoms, it is only necessary to avoid the separation; to use a single make session to build the whole project, which is not quite the same as a single Makefile. This conclusion runs counter to much accumulated folk wisdom in building large projects on UNIX. Some of the main objections raised by this folk wisdom are examined and shown to be unfounded. The results of actual use are far more encouraging, with routine development performance improvements significantly faster than intuition may indicate, and without the intuitively expected compromise of modularity. The use of a whole project make is not as difficult to put into practice as it may at first appear.},
	author = {Peter Miller},
	keywords = {make, makefile, build},
	note = {Discussion on recursive make.},
	title = {{Recursive Make Considered Harmful}},
	url = {http://aegis.sourceforge.net/auug97.pdf},
	year = {2008},
}

@article{hand:2006a,
	abstract = {A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm.},
	author = {David J Hand},
	doi = {10.1214/088342306000000060},
	journal = {Statistical Science},
	keywords = {ml, machine, learning, ai, classifier, supervised, unsupervised, math, data, science, statistics, stats},
	month = {02},
	number = {1},
	pages = {1--14},
	publisher = {The Institute of Mathematical Statistics},
	title = {{Classifier Technology and the Illusion of Progress}},
	url = {http://dx.doi.org/10.1214/088342306000000060},
	volume = {21},
	year = {2006},
}

@inproceedings{nielson:2011a,
	abstract = {Sentiment analysis of microblogs such as Twitter has recently gained a fair amount of attention. One of the simplest sentiment analysis approaches compares the words of a posting against a labeled word list, where each word has been scored for valence, -- a 'sentiment lexicon' or 'affective word lists'. There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs. I used manually labeled postings from Twitter scored for sentiment. Using a simple word matching I show that the new word list may perform better than ANEW, though not as good as the more elaborate approach found in SentiStrength.},
	author = {Finn {\AA}rup Nielsen},
	booktitle = {{Proceedings of the {ESWC}2011 Workshop on 'Making Sense of Microposts': Big things come in small packages.}},
	doi = {},
	keywords = {datasets, data, sentiment, analysis, nlp, microblogs, twitter},
	month = {05},
	note = {arXiv link: http://arxiv.org/abs/1103.2903},
	pages = {93--98},
	publisher = {CEUR Workshop Proceedings},
	title = {{A new ANEW: Evaluation of a word list for sentiment analysis in microblogs}},
	url = {http://ceur-ws.org/Vol-718/paper_16.pdf},
	volume = {718},
	year = {2011},
}

@inbook{hansen:2011a,
	abstract = {The link between affect, defined as the capacity for sentimental arousal on the part of a message, and virality, defined as the probability that it be sent along, is of significant theoretical and practical importance, e.g. for viral marketing. The basic measure of virality in Twitter is the probability of retweet and we are interested in which dimensions of the content of a tweet leads to retweeting. We hypothesize that negative news content is more likely to be retweeted, while for non-news tweets positive sentiments support virality. To test the hypothesis we analyze three corpora: A complete sample of tweets about the COP15 climate summit, a random sample of tweets, and a general text corpus including news. The latter allows us to train a classifier that can distinguish tweets that carry news and non-news information. We present evidence that negative sentiment enhances virality in the news segment, but not in the non-news segment. Our findings may be summarized 'If you want to be cited: Sweet talk your friends or serve bad news to the public'.},
	address = {Berlin, Heidelberg},
	author = {Lars Kai Hansen and Adam Arvidsson and Finn {\AA}rup Nielsen and Elanor Colleoni and Michael Etter},
	bookTitle = {{Future Information Technology: 6th International Conference, FutureTech 2011, Loutraki, Greece, June 28-30, 2011, Proceedings, Part II}},
	doi = {10.1007/978-3-642-22309-9_5},
	editor = {James J. Park and Laurence T. Yang and Changhoon Lee},
	isbn = {978-3-642-22309-9},
	keywords = {datasets, data, sentiment, analysis, nlp, microblogs, twitter},
	pages = {34--43},
	publisher = {Springer Berlin Heidelberg},
	title = {{Good Friends, Bad News - Affect and Virality in Twitter}},
	url = {http://dx.doi.org/10.1007/978-3-642-22309-9_5},
	year = {2011},
}

@article{higham:1993a,
	abstract = {The usual recursive summation technique is just one of several ways of computing the sum of \\(n\\) floating point numbers. Five summation methods and their variations are analyzed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases.},
	author = {Nicholas J. Higham},
	doi = {10.1137/0914050},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {numeric, computing, accuracy, precision, summation, algorithms, numerical, floating-point, double, float, ieee754},
	month = {07},
	number = {4},
	pages = {783--799},
	title = {{The Accuracy of Floating Point Summation}},
	url = {http://dx.doi.org/10.1137/0914050},
	volume = {14},
	year = {1993},
}

@book{ueberhuber:1997a,
	author = {Christoph W. Ueberhuber},
	doi = {10.1007/978-3-642-59118-1},
	isbn = {978-3-540-62058-7},
	keywords = {numerical, computation, compute, math, mathematics, methods},
	publisher = {Springer-Verlag Berlin Heidelberg},
	title = {{Numerical Computation 1: Methods, Software, and Analysis}},
	year = {1997},
}

@article{chen:2016a,
	abstract = {We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.},
	author = {Jiahao Chen and Jarrett Revels},
	doi = {},
	journal = {CoRR},
	keywords = {performance, perf, benchmarks, benchmark, measure, numeric computing, julia},
	month = {08},
	number = {},
	pages = {},
	title = {{Robust benchmarking in noisy environments}},
	url = {http://arxiv.org/abs/1608.04295},
	volume = {abs/1608.04295},
	year = {2016},
}

@article{joehnk:1964,
	abstract= {},
	author = {M.D. J{\u{o}}hnk},
	doi = {},
	journal = {Metrika},
	keywords = {probability theory, sampling, random numbers},
	month = {},
	pages = {5-15},
	title = {{Erzeugung von betaverteilten und gammaverteilten Zufallszahlen}},
	url = {http://eudml.org/doc/175224},
	volume = {8},
	year = {1964},
}

@article{baudin:2012a,
	abstract = {The most widely used algorithm for floating point complex division, known as Smith's method, may fail more often than expected. This document presents two improved complex division algorithms. We present a proof of the robustness of the first improved algorithm. Numerical simulations show that this algorithm performs well in practice and is significantly more robust than other known implementations. By combining additional scaling methods with this first algorithm, we were able to create a second algorithm, which rarely fails.},
	author = {Michael Baudin and Robert L. Smith},
	doi = {},
	journal = {CoRR},
	keywords = {complex, mathematics, math, numeric computing, julia, python, complex numbers, arithmetic, division},
	month = {10},
	pages = {},
	title = {{A Robust Complex Division in Scilab}},
	url = {https://arxiv.org/abs/1210.4539},
	volume = {abs/1210.4539},
	year = {2012},
}

@misc{ward:2002a,
	author = {Grady Ward},
	keywords = {names, list, english, american, british, first name, given name, female, woman, girl},
	note = {},
	title = {{Moby Word II}},
	url = {http://www.gutenberg.org/files/3201/3201.txt},
	year = {2002}
}

@misc{frbsf:wagerigidity,
	author = {{Federal Reserve Bank of San Francisco}},
	keywords = {wages, wage rigidity, federal reserve, employment, data, dataset, economics, economy, workers},
	note = {Wage rates for U.S. workers that have not changed jobs within the year.},
	title = {{Wage Rigidity}},
	url = {http://www.frbsf.org/economic-research/indicators-data/nominal-wage-rigidity/},
	year = {2017}
}

@misc{azari:1929a,
	author = {F{\'{e}}d{\`{e}}le Azari},
	keywords = {image, img, data, dataset, airplane, flight, flying, transport, transportation},
	note = {F{\'{e}}d{\`{e}}le Azari's gelatin silver print of an airplane, viewed from above looking down. Sourced from Getty's Open Content Program.},
	title = {{(no title)}},
	url = {http://www.getty.edu/art/collection/objects/134512/fedele-azari-airplane-viewed-from-above-looking-down-italian-1914-1929/},
	year = {1929}
}

@misc{blossfeldt:1928a,
	author = {Karl Blossfeldt},
	keywords = {image, img, data, dataset, flower, plant},
	note = {Karl Blossfeldt's gelatin silver print Acanthus mollis. Sourced from Getty's Open Content Program.},
	title = {{Acanthus mollis}},
	url = {http://www.getty.edu/art/collection/objects/35443/karl-blossfeldt-acanthus-mollis-german-1928/},
	year = {1928}
}

@misc{blossfeldt:1928b,
	author = {Karl Blossfeldt},
	keywords = {image, img, data, dataset, flower, plant},
	note = {Karl Blossfeldt's gelatin silver print Allium ostrowskianum. Sourced from Getty's Open Content Program.},
	title = {{Allium ostrowskianum}},
	url = {http://www.getty.edu/art/collection/objects/35448/karl-blossfeldt-allium-ostrowskianum-knoblauchpflanze-german-1928/},
	year = {1928}
}

@misc{osullivan:1871a,
	author = {Timothy H. O'Sullivan},
	keywords = {image, img, data, dataset, river, us, united states, canyon, place, nature},
	note = {Timothy H. O'Sullivan's albumen silver print Black Ca{\~{n}}on, Colorado River, From Camp 8, Looking Above. Sourced from Getty's Open Content Program.},
	title = {{Black Ca{\~{n}}on, Colorado River, From Camp 8, Looking Above}},
	url = {http://www.getty.edu/art/collection/objects/40209/timothy-h-o'sullivan-black-canon-colorado-river-from-camp-8-looking-above-american-1871/},
	year = {1871}
}

@misc{lange:1940a,
	author = {Dorothea Lange},
	keywords = {image, img, data, dataset, home, house, dust bowl, west},
	note = {Dorothea Lange's gelatin silver print of an abandoned Dust Bowl home. Sourced from Getty's Open Content Program.},
	title = {{Abandoned Dust Bowl Home}},
	url = {http://www.getty.edu/art/collection/objects/128362/dorothea-lange-abandoned-dust-bowl-home-american-about-1935-1940/},
	year = {1940}
}

@misc{braun:1870a,
	author = {Adolphe Braun},
	keywords = {image, img, data, dataset, alpine, landscape, french, france, nature, place},
	note = {Adolphe Braun's carbon print of a French alpine landscape. Sourced from Getty's Open Content Program.},
	title = {{(no title)}},
	url = {http://www.getty.edu/art/collection/objects/54324/adolphe-braun-alpine-landscape-french-1865-1870/},
	year = {1870}
}

@misc{muybridge:1887a,
	author = {Eadweard J. Muybridge},
	keywords = {image, img, data, dataset, cat, animal, moving},
	note = {Eadweard J. Muybridge's collotype of a house cat (in twenty-four views). Sourced from Getty's Open Content Program.},
	title = {{Animal Locomotion}},
	url = {http://www.getty.edu/art/collection/objects/40918/eadweard-j-muybridge-animal-locomotion-american-1887/},
	year = {1887}
}

@misc{muybridge:1887b,
	author = {Eadweard J. Muybridge},
	keywords = {image, img, data, dataset, male, nude, human, man, moving},
	note = {Eadweard J. Muybridge's collotype of a nude male moving in place (in forty-eight views). Sourced from Getty's Open Content Program.},
	title = {{Animal Locomotion}},
	url = {http://www.getty.edu/art/collection/objects/40907/eadweard-j-muybridge-animal-locomotion-american-1887/},
	year = {1887}
}

@misc{emerson:1888a,
	author = {Peter Henry Emerson},
	keywords = {image, img, data, dataset, animals, sheep, pastoral, countryside, england, uk, united kingdom, britain, english, british},
	note = {Peter Henry Emerson's photogravure of sheep in a pastoral setting. Sourced from Getty's Open Content Program.},
	title = {{A March Pastoral}},
	url = {http://www.getty.edu/art/collection/objects/141994/peter-henry-emerson-a-march-pastoral-suffolk-british-1888/},
	year = {1888}
}

@misc{beato:1865a,
	author = {Felice Beato},
	keywords = {image, img, data, dataset, boats, boat, river, japan, nagasaki},
	note = {Felice Beato's albumen silver print of boats in a river in Nagasaki. Sourced from Getty's Open Content Program.},
	title = {{(no title)}},
	url = {http://www.getty.edu/art/collection/objects/241797/felice-beato-boats-in-river-nagasaki-british-about-1865/},
	year = {1865}
}

@inproceedings{hu:2004a,
	abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
	acmid = {1014073},
	address = {New York, NY, USA},
	author = {Minqing Hu and Bing Liu},
	booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/1014052.1014073},
	isbn = {1-58113-888-1},
	keywords = {reviews, sentiment classification, summarization, text mining},
	location = {Seattle, WA, USA},
	numpages = {10},
 	pages = {168--177},
	publisher = {ACM},
	series = {KDD '04},
	title = {{Mining and Summarizing Customer Reviews}},
	url = {http://doi.acm.org/10.1145/1014052.1014073},
	year = {2004},
}

@inproceedings{liu:2005a,
	abstract = {The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.},
	acmid = {1060797},
	address = {New York, NY, USA},
	author = {Bing Liu and Minqing Hu and Junsheng Cheng},
	booktitle = {Proceedings of the 14th International Conference on World Wide Web},
	doi = {10.1145/1060745.1060797},
	isbn = {1-59593-046-9},
	keywords = {information extraction, opinion analysis, sentiment analysis, visualization},
	location = {Chiba, Japan},
	numpages = {10},
 	pages = {342--351},
	publisher = {ACM},
	series = {WWW '05},
	title = {{Opinion Observer: Analyzing and Comparing Opinions on the Web}},
	url = {http://doi.acm.org/10.1145/1060745.1060797},
	year = {2005},
}

@inbook{liu:2010a,
	author = {Bing Liu},
	booktitle = {Handbook of Natural Language Processing},
	edition = {2nd},
	editor = {Nitin Indurkhya and Fred J. Damerau},
	isbn = {9781420085921},
	keywords = {natural language processing, nlp, sentiment, subjectivity, text mining, opinion analysis, information extraction},
	pages = {627--66},
	publisher = {Chapman \& Hall/CRC},
	title = {{Sentiment Analysis and Subjectivity}},
	url = {https://www.crcpress.com/Handbook-of-Natural-Language-Processing-Second-Edition/Indurkhya-Damerau/p/book/9781420085921},
	year = {2010},
}

@book{minard:1869a,
	author = {Charles Joseph Minard},
	keywords = {cartography, napoleon, visualization, dataviz, graphics, charts, maps},
	publisher = {Ecole nationale des ponts et chauss{\'{e}}es},
	title = {{Tableaux graphiques et cartes figuratives}},
	url = {http://patrimoine.enpc.fr/document/ENPC01_Fol_10975?image=54#bibnum},
	year = {1869},
}

@book{wilkinson:2005a,
	author = {Leland Wilkinson},
	doi = {10.1007/0-387-28695-0},
	isbn = {978-0-387-24544-7},
	keywords = {visualization, dataviz, graphics, charts, information},
	publisher = {Springer-Verlag New York},
	title = {{The Grammar of Graphics}},
	url = {http://www.springer.com/gp/book/9780387245447},
	year = {2005},
}

@book{nightingale:1859a,
	address = {London, United Kingdom},
	author = {Florence Nightingale},
	keywords = {visualization, dataviz, graphics, charts, war, infographics},
	publisher = {{John W. Parker and Son}},
	title = {{A contribution to the sanitary history of the British army during the late war with Russia}},
	url = {http://ocp.hul.harvard.edu/dl/contagion/010164675},
	year = {1859},
}

@misc{savoy:2005a,
	author = {Jacques Savoy},
	keywords = {text mining, nlp, natural language processing, language, lang, stop words},
	title = {{IR Multilingual Resources at UniNE}},
	url = {http://members.unine.ch/jacques.savoy/clef/},
	year = {2005}
}

@misc{mason:spamassassin,
	author = {Justin Mason},
	keywords = {text mining, nlp, natural language processing, language, lang, email, e-mail, mail, corpus, spam, ham, classification},
	title = {{Spam Assassin Public Mail Corpus}},
	url = {http://spamassassin.apache.org/old/publiccorpus/readme.html},
	year = {2006}
}

@article{gould:1965a,
	abstract = {},
	author = {H.W. Gould},
	doi = {},
	journal = {Fibonacci Quarterly},
	keywords = {fibonacci,fibo,integer,sequences,number theory},
	month = {},
	number = {3},
	pages = {177--183},
	title = {{Non-Fibonacci Numbers}},
	url = {http://www.fq.math.ca/Scanned/3-3/gould.pdf},
	volume = {},
	year = {1965},
}

@article{farhi:2011a,
	abstract = {We show among others that the formula: \\( \left \lfloor{ n + \log_\varphi \biggl( \sqrt{5}(\log_\varphi (n\sqrt{5}) + n) − 5 + \tfrac{3}{n} \biggr) − 2} \right \rfloor,\ n \geq 2 \\), (where \\(\varphi\\) denotes the golden ratio and \\( \left \lfloor \right \rfloor \\) denotes the integer part) generates the non-Fibonacci numbers.},
	author = {Bakir Farhi},
	doi = {},
	journal = {{arXiv}},
	keywords = {fibonacci,fibo,integer,sequences,number theory},
	month = {may},
	number = {},
	pages = {1--5},
	title = {{An explicit formula generating the non-Fibonacci numbers}},
	url = {https://arxiv.org/abs/1105.1127},
	volume = {abs/1105.1127 [math.NT]},
	year = {2011},
}

@mastersthesis{holloway:1988a,
	abstract = {A study of the running time of several known algorithms and several new algorithms to compute the nth element of the Fibonacci sequence is presented. Since the size of the \\( n^{th} \\) Fibonacci number grows exponentially with \\( n \\), the number of bit operations, instead of the number of integer operations, was used as the unit of time. The number of bit operations used to compute \\( f_n \\) is reduced to less than \\( \tfrac{1}{2} \\) of the number of bit operations used to multiply two \\( n \\) bit numbers. The algorithms were programmed in Ibuki Common Lisp and timing runs were made on a Sequent Balance 21000. Multiplication was implemented using the standard \\( n^2 \\) algorithm. Times for the various algorithms are reported as various constants times \\( n^2 \\). An algorithm based on generating factors of Fibonacci numbers had the smallest constant. The Fibonacci sequence, arranged in various ways, is searched for redundant information that could be eliminated to reduce the number of operations. Cycles in the \\( b^{th} \\) bit of \\( f_n \\) were discovered but are not yet completely understood.},
	author = {James L. Holloway},
	keywords = {fibonacci, fibo, fib, sequence, algorithm, algo},
	school = {Oregon State University},
	title = {{Algorithms for Computing Fibonacci Numbers Quickly}},
	url = {http://ir.library.oregonstate.edu/xmlui/bitstream/handle/1957/39942/HollowayJamesL1989.pdf},
	year = {1988}
}

@article{baudin:2012,
	abstract = {The most widely used algorithm for floating point complex division, known as Smith's method, may fail more often than expected. This document presents two improved complex division algorithms. We present a proof of the robustness of the first improved algorithm. Numerical simulations show that this algorithm performs well in practice and is significantly more robust than other known implementations. By combining additionnal scaling methods with this first algorithm, we were able to create a second algorithm, which rarely fails.},
	author = {Michael Baudin and Robert L. Smith},
	keywords = {floating-point, arithmetic, complex, division, algorithm, mathematics, math},
	month = {oct},
	number = {},
	pages = {},
	title = {{A Robust Complex Division in Scilab}},
	url = {https://arxiv.org/abs/1210.4539},
	volume = {abs/1210.4539},
	year = {2012},
}
