@techreport{beebe:2002,
	abstract = {These notes describe an implementation of an algorithm for accurate computation of \\( \operatname{expm1}(x) = \operatname{exp}(x) − 1 \\), one of the new elementary functions introduced in the 1999 ISO C Standard, but already available in most UNIX C implementations. A test package modeled after the Cody and Waite Elementary Function Test Package, ELEFUNT, is developed to evaluate the accuracy of implementations of \\( \operatorname{expm1}(x) \\).},
	author = {Nelson H.F. Beebe},
	keywords = {mathematics, math, special function, exponential, exp},
	institution = {University of Utah},
	title = {{Computation of expm1(x) = exp(x) − 1}},
	url = {http://www.math.utah.edu/~beebe/reports/expm1.pdf},
	year = {2002},
}

@article{blair:1976,
	abstract = {This report presents near-minimax rational approximations for the inverse of the error function \\( \operatorname{inverf}(x) \\), for  \\( 0 \leqslant x \leqslant 1 - {10^{ - 10000}} \\), with relative errors ranging down to  \\( {10^{ - 23}}\\). An asymptotic formula for the region \\( x \to 1\\) is also given.},
	author = {J.M. Blair and C.A. Edwards and J.H. Johnson},
	doi = {10.1090/S0025-5718-1976-0421040-7},
	journal = {Mathematics of Computation},
	keywords = {Rational Chebyshev approximations, inverse error function, minimal Newton form, erf, erfinv, mathematics, math, special function},
	pages = {827--830},
	title = {{Rational Chebyshev approximations for the inverse of the error function}},
	url = {http://www.ams.org/journals/mcom/1976-30-136/S0025-5718-1976-0421040-7/},
	volume = {30},
	year = {1976},
}

@article{blei:2003,
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	acmid = {944937},
	author = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	keywords = {topic modeling, machine learning, natural language processing, mixed-membership},
	month = {mar},
	pages = {993--1022},
	title = {{Latent dirichlet allocation}},
	url = {http://dl.acm.org/citation.cfm?id=944937},
	volume = {3},
	year = {2003},
}

@inproceedings{borwein:1991,
	abstract = {A very simple class of algorithms for the computation of the Riemann-zeta function to arbitrary precision in arbitrary domains is proposed. These algorithms compete with the standard methods based on Euler-Maclaurin summation, are easier to implement and are easier to analyze.},
	author = {P. Borwein},
	booktitle = {Conference Proceedings},
	keywords = {Riemann zeta function, computation, high precision, algorithm, mathematics, math, special function, riemann, zeta},
	publisher = {Canadian Mathematical Society},
	title = {{An Efficient Algorithm for the Riemann Zeta Function}},
	year = {1991},
}

@article{carlitz:1963,
	author = {L. Carlitz},
	journal = {Pacific Journal of Mathematics},
	keywords = {mathematics, math, error function, erf},
	number = {2},
	pages = {459--470},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{The inverse of the error function.}},
	url = {http://msp.org/pjm/1963/13-2/pjm-v13-n2-p06-s.pdf},
	volume = {13},
	year = {1963},
}

@mastersthesis{segarra:2006,
	abstract = {Identified as one of the 7 Millennium Problems, the Riemann zeta hypothesis has successfully evaded mathematicians for over 100 years. Simply stated, Riemann conjectured that all of the nontrivial zeroes of his zeta function have real part equal to 1/2. This thesis attempts to explore the theory behind Riemann’s zeta function by first starting with Euler’s zeta series and building up to Riemann’s function. Along the way we will develop the math required to handle this theory in hopes that by the end the reader will have immersed themselves enough to pursue their own exploration and research into this fascinating subject.},
	author = {Elan Segarra},
	keywords = {math, mathematics, riemann zeta, riemann, zeta, special function},
	school = {Harvey Mudd College},
	title = {{An Exploration of the Riemann Zeta Function and its Application to the Theory of Prime Number Distribution}},
	url = {https://www.math.hmc.edu/seniorthesis/archives/2006/esegarra/esegarra-2006-thesis.pdf},
	year = {2006},
}

@book{fishman:1973,
	address = {New York, NY, USA},
	author = {George S. Fishman},
	isbn = {9780471261551},
	keywords = {system analysis, simulation, science, math, random number generation},
	lccn = {73005713},
	month = {sep},
	publisher = {Wiley},
	series = {A Wiley-Interscience publication},
	title = {{Concepts and methods in discrete event digital simulation}},
	year = {1973},
}

@article{fransen:1980,
	abstract = {In this paper we determine numerical values to 80D of the coefficients in the Taylor series expansion \\( {\Gamma ^m}(s + x) = \Sigma _0^\infty {g_k}(m,s){x^k}\\) for certain values of \\(m\\) and \\(s\\) and use these values to calculate  \\( \Gamma (p/q)\;(p,q = 1,2, \ldots ,10;\;p < q)\\) and  \\({\min _{x > 0}}\Gamma (x)\\) to 80D. Finally, we obtain a high-precision value of the integral  \\(\smallint _0^\infty {(\Gamma (x))^{ - 1}}\;dx\\).},
	author = {Arne Frans{\'{e}}n and Staffan Wrigge},
	doi = {10.1090/S0025-5718-1980-0559204-5},
	journal = {Mathematics of Computation},
	keywords = {Special functions, Gamma function, Riemann Zeta function, gamma, mathematics, math, special function},
	pages = {553--566},
	title = {{High-precision values of the gamma function and of some related coefficients}},
	url = {http://www.ams.org/journals/mcom/1980-34-150/S0025-5718-1980-0559204-5/},
	volume = {34},
	year = {1980},
}

@article{goldberg:1991,
	abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
	acmid = {103163},
	address = {New York, NY, USA},
	author = {David Goldberg},
	doi = {10.1145/103162.103163},
	journal = {ACM Comput. Surv.},
	issn = {0360-0300},
	issue_date = {March 1991},
	keywords = {NaN, denormalized number, exception, floating-point, floating-point standard, gradual underflow, guard digit, overflow, relative error, rounding error, rounding mode, ulp, underflow, computation, ieee754, math, double, float},
	month = {mar},
	number = {1},
	numpages = {44},
	pages = {5--48},
	publisher = {ACM},
	title = {What Every Computer Scientist Should Know About Floating-point Arithmetic},
	url = {http://doi.acm.org/10.1145/103162.103163},
	volume = {23},
	year = {1991},
}

@unpublished{gourdon:2003,
	author = {Xavier Gourdon and Pascal Sebah},
	keywords = {riemann zeta, riemann, zeta, math, mathematics, special function},
	note = {Numerical evaluation of the Riemann Zeta-function.},
	title = {{Numerical evaluation of the Riemann Zeta-function}},
	year = {2003},
}

@article{griffiths:2004,
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	author = {Thomas L Griffiths and Mark Steyvers},
	doi = {10.1073/pnas.0307752101},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {topic modeling, bayesian, documentation, models, statistical, monte carlo method, markov chain, gibbs},
	month = {apr},
	pages = {5228--35},
	pmid = {14872004},
	title = {{Finding scientific topics}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC387300},
	volume = {101 Suppl 1},
	year = {2004},
}

@article{shaw:2006,
	abstract = {With the current interest in copula methods, and fat-tailed or other non-normal distributions, it is appropriate to investigate technologies for managing marginal distributions of interest. We explore "Student's T distribution, survey its simulation, and present some new techniques for simulation. In particular, for a given real (not necessarily integer) value n of the number of degrees of freedom, we give a pair of power series approximations for the inverse, \\( F{\_}n{\^{}}{\{}-1{\}} \\), of the cumulative distribution function (CDF), \\( F_n \\).We also give some simple and very fast exact and iterative techniques for defining this function when n is an even integer, based on the observation that for such cases the calculation of \\( F{\_}n{\^{}}{\{}-1{\}} \\) amounts to the solution of a reduced-form polynomial equation of degree n-1. We also explain the use of Cornish-Fisher expansions to define the inverse CDF as the composition of the inverse CDF for the normal case with a simple polynomial map. The methods presented are well adapted for use with copula and quasi-Monte-Carlo techniques.},
	author = {William T. Shaw},
	journal = {Journal of Computational Finance},
	keywords = {student's t distribution, inverse CDF, copula},
	number = {4},
	pages = {37--73},
	title = {{Sampling Student's T distribution -- use of the inverse cumulative distribution function}},
	url = {http://www.mth.kcl.ac.uk/~shaww/web_page/papers/Tdistribution06.pdf},
	volume = {9},
	year = {2006}
}

@article{khajah:1994,
	abstract = {We describe a machine independent Fortran subroutine which performs the four basic arithmetic operations with a degree of accuracy prescribed by the user. Tables of Chebyshev expansions of orders 48 and 50 for some basic mathematical functions are obtained as a result of applying this subroutine in conjunction with the recursive formulation of the Tau Method. A recently devised technique for the sharp determination of upper and lower error bounds for Tau Method approximations (see [1]) enables us to find the degree n required to achieve a prescribed accuracy ϵ over a given interval [a,b]. A number of practical illustrations are given.},
	author = {H.G. Khajah and E.L. Ortiz},
	doi = {10.1016/0898-1221(94)90148-1},
	issn = {0898-1221},
	journal = {Computers & Mathematics with Applications},
	keywords = {numeric computing, precision, programming, computation},
	number = {7},
	pages = {41--57},
	title = {Ultra-high precision computations},
	url = {http://www.sciencedirect.com/science/article/pii/0898122194901481},
	volume = {27},
	year = {1994},
}

@phdthesis{pugh:2004,
	abstract = {This thesis is an analysis of C. Lanczos’ approximation of the classical gamma function \\( \Gamma(z+1) \\) as given in his 1964 paper \textit{A Precision Approximation of the Gamma Function}. The purposes of this study are: (i) to explain the details of Lanczos’ paper, including proofs of all claims made by the author; (ii) to address the question of how best to implement the approximation method in practice; and (iii) to generalize the methods used in the derivation of the approximation.},
	author = {Glendon Ralph Pugh},
	keywords = {gamma, lanczos, math, mathematics, special function},
	school = {The University of British Columbia},
	title = {{An Analysis of the Lanczos Gamma Approximation}},
	url = {https://web.viu.ca/pughg/phdThesis/phdThesis.pdf},
	year = {2004}
}

@article{meurer:2016,
	abstract = {SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become the standard symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select domain specific submodules. The supplementary materials provide additional examples and further outline details of the architecture and features of SymPy.},
	author = {A. Meurer and C.P. Smith and M. Paprocki and O. {\u{C}}ert{\'{i}}k and S.B. Kirpichev and M. Rocklin and A. Kumar and S. Ivanov and J.K. Moore and S. Singh and T. Rathnayake and S. Vig and B.E. Granger and R.P. Muller and F. Bonazzi and H. Gupta and S. Vats and F. Johansson and F. Pedregosa and M.J. Curry and A.R. Terrel and {\u{S}}. Rou{\u{c}}ka and A. Saboo and I. Fernando and S. Kulal and R. Cimrman and A. Scopatz},
	doi = {10.7287/peerj.preprints.2083v3},
	journal = {PeerJ Preprints},
	keywords = {symbolic, python, computer algebra system},
	month = {jun},
	number = {2083},
	title = {{SymPy: Symbolic computing in Python.}},
	url = {https://github.com/sympy/sympy-paper},
	volume = {4},
	year = {2016},
}

@unpublished{cao:high,
	abstract = {This paper presents the design and implementation of several fundamental dense linear algebra (DLA) algorithms in OpenCL. In particular, these are linear system solvers and eigenvalue problem solvers. Further, we give an overview of the clMAGMA library, an open source, high performance OpenCL library that incorporates the developments presented, and in general provides to heterogeneous architectures the DLA functionality of the popular LAPACK library. The LAPACK-compliance and use of OpenCL simplify the use of clMAGMA in applications, while providing them with portably performant DLA. High performance is obtained through use of the high-performance OpenCL BLAS, hardware and OpenCL-specific tuning, and a hybridization methodology where we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware components by minimizing data movements and mapping algorithmic requirements to the architectural strengths of the various heterogeneous hardware components.},
	author = {Chongxiao Cao and Jack Dongarra and Peng Du and Mark Gates and Piotr Luszczek and Stanimire Tomov},
	keywords = {gpu, opencl, blas, numeric computing, computation, lapack},
	note = {netlib},
	title = {{clMAGMA: High Performance Dense Linear Algebra with OpenCL}},
}

@misc{dawson:2013,
	author = {Bruce Dawson},
	keywords = {ieee754, floating-point, math, mathematics, numeric computing, computation},
	title = {{Floating-Point Determinism}},
	url = {https://randomascii.wordpress.com/2013/07/16/floating-point-determinism/},
	year = {2013}
}

@misc{godfrey:2001,
	author = {Paul Godfrey},
	keywords = {lanczos, gamma, special function, math, mathematics},
	title = {{A note on the computation of the convergent Lanczos complex Gamma approximation.}},
	url = {http://my.fit.edu/~gabdo/gamma.txt},
	year = {2001}
}

@misc{toth:gamma,
	author = {Viktor T. Toth},
	keywords = {lanczos, gamma, special function, math, mathematics},
	title = {{The Gamma function}},
	url = {http://www.rskey.org/CMS/index.php/the-library/11},
}

@article{vigna:2014,
	abstract = {xorshift\sup{*} generators are a variant of Marsaglia's xorshift generators that eliminate linear artifacts typical of generators based on \\(Z/2Z\\)-linear operations using multiplication by a suitable constant. Shortly after high-dimensional xorshift\sup{*} generators were introduced, Saito and Matsumoto suggested a different way to eliminate linear artifacts based on addition in \\(Z/2^{32}Z\\), leading to the XSadd generator. Starting from the observation that the lower bits of XSadd are very weak, as its reverse fails systematically several statistical tests, we explore xorshift+, a variant of XSadd using 64-bit operations, which leads, in small dimension, to extremely fast high-quality generators.},
	author = {Sebastiano Vigna},
	doi = {},
	journal = {CoRR},
	keywords = {random, prng, rng, marsaglia, xorshift, uniform, rand},
	month = {apr},
	number = {},
	pages = {},
	title = {{Further scramblings of Marsaglia's xorshift generators}},
	url = {https://arxiv.org/abs/1404.0390},
	volume = {abs/1404.0390},
	year = {2014},
}

@article{vose:1991,
	abstract = {Let \\(\xi\\) be a random variable over a finite set with an arbitrary probability distribution. Improvements to a fast method of generating sample values for \\(\xi\\) in constant time are suggested.},
	author = {Michael D. Vose},
	doi = {10.1109/32.92917},
	issn = {00985589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {random, rand, prng, rng, discrete},
	month = {sep},
	number = {9},
	pages = {972--975},
	title = {{A linear algorithm for generating random numbers with a given distribution}},
	volume = {17},
	year = {1991},
}

@article{marsaglia:2003,
	abstract = {Description of a class of simple, extremely fast random number generators (RNGs) with periods \\(2^k −1\\) for \\(k = 32, 64, 96, 128, 160, 192\\). These RNGs seem to pass tests of randomness very well.},
	author = {George Marsaglia},
	doi = {10.18637/jss.v008.i14},
	journal = {Journal of Statistical Software},
	keywords = {random, rand, marsaglia, prng, rng, xorshift, uniform},
	note = {xoshift},
	number = {14},
	title = {{Xorshift RNGs}},
	url = {https://www.jstatsoft.org/article/view/v008i14},
	volume = {8},
	year = {2003},
}

@article{panneton:2005,
	abstract = {G. Marsaglia recently introduced a class of very fast xorshift random number generators, whose implementation uses three “xorshift” operations. They belong to a large family of generators based on linear recurrences modulo 2, which also includes shift-register generators, the Mersenne twister, and several others. In this article, we analyze the theoretical properties of xorshift generators, search for the best ones with respect to the equidistribution criterion, and test them empirically. We find that the vast majority of xorshift generators with only three xorshift operations, including those having good equidistribution, fail several simple statistical tests. We also discuss generators with more than three xorshifts.},
	acmid = {1113319},
	address = {New York, NY, USA},
	author = {Fran{\c{c}}ois Panneton and Pierre L'Ecuyer},
	doi = {10.1145/1113316.1113319},
	issn = {1049-3301},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	issue_date = {October 2005},
	keywords = {random number generation, linear feedback shift register, linear recurrence modulo 2, xorshift, prng, rng, random, rand},
	month = {oct},
	number = {4},
	numpages = {16},
	pages = {346--361},
	publisher = {ACM},
	title = {On the Xorshift Random Number Generators},
	url = {http://doi.acm.org/10.1145/1113316.1113319},
	volume = {15},
	year = {2005},
}

@article{hellekalek:1998,
	abstract = {Every random number generator has its advantages and deficiencies. There are no "safe" generators. The practitioner's problem is how to decide which random number generator will suit his needs best. In this paper, we will discuss criteria for good random number generators: theoretical support, empirical evidence and practical aspects. We will study several recent algorithms that perform better than most generators in actual use. We will compare the different methods and supply numerical results as well as selected pointers and links to important literature and other sources. Additional information on random number generation, including the code of most algorithms discussed in this paper is available from our web-server under the address http://random.mat.sbg.ac.at/.},
	acmid = {284161},
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {P. Hellekalek},
	doi = {10.1016/S0378-4754(98)00078-0},
	issn = {0378-4754},
	issue_date = {June 1998},
	journal = {Mathematics and Computers in Simulation},
	month = {jun},
	number = {5-6},
	numpages = {21},
	pages = {485--505},
	publisher = {Elsevier Science Publishers B. V.},
	title = {{Good Random Number Generators Are (Not So) Easy to Find}},
	url = {http://dx.doi.org/10.1016/S0378-4754(98)00078-0},
	volume = {46},
	year = {1998},
}

@article{ahrens:1974,
	abstract = {Accurate computer methods are evaluated which transform uniformly distributed random numbers into quantities that follow gamma, beta, Poisson, binomial and negative-binomial distributions. All algorithms are designed for variable parameters. The known convenient methods are slow when the parameters are large. Therefore new procedures are introduced which can cope efficiently with parameters of all sizes. Some algorithms require sampling from the normal distribution as an intermediate step. In the reported computer experiments the normal deviates were obtained from a recent method which is also described.},
	author = {J.H. Ahrens and U. Dieter},
	doi = {10.1007/BF02293108},
	issn = {1436-5057},
	journal = {Computing},
	keywords = {random, prng, rng, rand, beta, computation, pseudorandom, number, generator},
	number = {3},
	pages = {223--246},
	title = {{Computer methods for sampling from gamma, beta, poisson and bionomial distributions}},
	url = {http://dx.doi.org/10.1007/BF02293108},
	volume = {12},
	year = {1974},
}

@article{hormann:1993a,
	abstract = {The transformed rejection method, a combination of inversion and rejection, which can be applied to various continuous distributions, is well suited to generate binomial random variates as well. The resulting algorithms are simple and fast, and need only a short set-up. Among the many possible variants two algorithms are described and tested: BTRS a short but nevertheless fast rejection algorithm and BTRD which is more complicated as the idea of decomposition is utilized. For BTRD the average number of uniforms required to return one binomial deviate less between 2.5 and 1.4 which is considerably lower than for any of the known uniformly fast algorithms. Timings for a C-implementation show that for the case that the parameters of the binomial distribution vary from call to call BTRD is faster than the current state of the art algorithms. Depending on the computer, the speed of the uniform generator used and the binomial parameters the savings are between 5 and 40 percent.},
	author = {Wolfgang H{\"{o}}rmann},
	doi = {10.1080/00949659308811496},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rand, prng, rng, binomial, pseudorandom, number, generator},
	number = {1-2},
	pages = {101-110},
	title = {{The generation of binomial random variates}},
	url = {http://dx.doi.org/10.1080/00949659308811496},
	volume = {46},
	year = {1993},
}

@article{box:1958,
	abstract = {},
	author = {G. E. P. Box and Mervin E. Muller},
	doi = {10.1214/aoms/1177706645},
	issn = {00034851},
	journal = {The Annals of Mathematical Statistics},
	keywords = {random, prng, rng, pseudorandom, rand, normal, gaussian, number, generator},
	month = {jun},
	number = {2},
	pages = {610--611},
	publisher = {The Institute of Mathematical Statistics},
	title = {{A Note on the Generation of Random Normal Deviates}},
	url = {http://dx.doi.org/10.1214/aoms/1177706645},
	volume = {29},
	year = {1958}
}

@article{bell:1968,
	abstract = {},
	acmid = {363547},
	address = {New York, NY, USA},
	author = {James R. Bell},
	doi = {10.1145/363397.363547},
	issn = {0001-0782},
	issue_date = {July 1968},
	journal = {Communications of the ACM},
	keywords = {frequency distribution, normal deviates, normal distribution, probability distribution, random, random number, random number generator, simulation, prng, rng},
	month = {jul},
	number = {7},
	pages = {498--},
	publisher = {ACM},
	title = {{Algorithm 334: Normal Random Deviates}},
	url = {http://doi.acm.org/10.1145/363397.363547},
	volume = {11},
	year = {1968},
}

@article{knop:1969,
	abstract = {},
	acmid = {362996},
	address = {New York, NY, USA},
	author = {R. Knop},
	doi = {10.1145/362946.362996},
	issn = {0001-0782},
	issue_date = {May 1969},
	journal = {Communications of the ACM},
	keywords = {frequency distribution, normal deviates, normal distribution, probability distribution, random, random number, random number generator, simulation},
	month = {may},
	number = {5},
	pages = {281--},
	publisher = {ACM},
	title = {{Remark on Algorithm 334 [G5]: Normal Random Deviates}},
	url = {http://doi.acm.org/10.1145/362946.362996},
	volume = {12},
	year = {1969},
}

@article{marsaglia:1964a,
	abstract = {A normal random variable \\(X\\) may be generated in terms of uniform random variables \\(u_1\\), \\(u_2\\), in the following simple way: 86 percent of the time, put \\(X = 2(u_1 + u_2 + u_3 - 1.5)\\),11 percent of the time, put \\(X = 1.5(u_1 + u_2 - 1)\\), and the remaining 3 percent of the time, use a more complicated procedure so that the resulting mixture is correct. This method takes only half again as long as the very fastest methods, is much simpler, and requires very little storage space.},
	author = {G. Marsaglia and T. A. Bray},
	doi = {10.1137/1006063},
	issn = {00361445},
	journal = {SIAM Review},
	keywords = {prng, rng, random, rand, pseudorandom, number, generator, normal, gaussian},
	month = {sep},
	number = {3},
	pages = {260--264},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{A Convenient Method for Generating Normal Variables}},
	url = {http://epubs.siam.org/doi/abs/10.1137/1006063},
	volume = {6},
	year = {1964},
}

@article{thomas:2007,
	abstract = {Rapid generation of high quality Gaussian random numbers is a key capability for simulations across a wide range of disciplines. Advances in computing have brought the power to conduct simulations with very large numbers of random numbers and with it, the challenge of meeting increasingly stringent requirements on the quality of Gaussian random number generators (GRNG). This article describes the algorithms underlying various GRNGs, compares their computational requirements, and examines the quality of the random numbers with emphasis on the behaviour in the tail region of the Gaussian probability density function.},
	acmid = {1287622},
	address = {New York, NY, USA},
	articleno = {11},
	author = {David B. Thomas and Wayne Luk and Philip H.W. Leong and John D. Villasenor},
	doi = {10.1145/1287620.1287622},
	issn = {0360-0300},
	issue_date = {2007},
	journal = {ACM Computing Surveys},
	keywords = {gaussian, random numbers, normal, simulation, random, rand, prng, rng, pseudorandom},
	month = {nov},
	number = {4},
	publisher = {ACM},
	title = {{Gaussian Random Number Generators}},
	url = {http://doi.acm.org/10.1145/1287620.1287622},
	volume = {39},
	year = {2007},
}

@article{marsaglia:2000a,
	abstract = {We offer a procedure for generating a gamma variate as the cube of a suitably scaled normal variate. It is fast and simple, assuming one has a fast way to generate normal variables. In brief: generate a normal variate \\(x\\) and a uniform variate \\(U\\) until \\(\ln(U) < 0.5 + d - dv + \ln(v)\\), then return \\(dv\\). Here, the gamma parameter is \\(\alpha \geq 1\\), and \\(v = (1 + x/ *** with \\(d = \alpha - 1/3\\). The efficiency is high, exceeding \\(0.951, 0.981, 0.992, 0.996\\) at \\(\alpha = 1,2,4,8\\). The procedure can be made to run faster by means of a simple squeeze that avoids the two logarithms most of the time; return \\(dv\\) if \\(U < 1 - 0.0331\\). We give a short C program for any \\(\alpha \geq 1\\), and show how to boost an \\(\alpha \lt 1\\) into an \\(\alpha \gt 1\\). The gamma procedure is particularly fast for C implementation if the normal variate is generated in-line, via the #define feature. We include such an inline version, based on our ziggurat method. With it, and an inline uniform generator, gamma variates can be produced in 400MHz CPUs at better than 1.3 million per second, with the parameter \\(\alpha\\) changing from call to call.},
	acmid = {358414},
	address = {New York, NY, USA},
	author = {George Marsaglia and Wai Wan Tsang},
	doi = {10.1145/358407.358414},
	issue_date = {Sept. 2000},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {gamma distribution, random number generation, ziggurat method, random, prng, rng, pseudorandom, gamma},
	month = {sep},
	number = {3},
	numpages = {10},
	pages = {363--372},
	publisher = {ACM},
	issn = {0098-3500},
	title = {{A Simple Method for Generating Gamma Variables}},
	url = {http://doi.acm.org/10.1145/358407.358414},
	volume = {26},
	year = {2000},
}

@article{hill:1970,
	acmid = {355600},
	address = {New York, NY, USA},
	author = {G. W. Hill},
	doi = {10.1145/355598.355600},
	issue_date = {Oct. 1970},
	issn = {0001-0782},
	journal = {Communications of the ACM},
	keywords = {asymptotic expansion, quantile, student's t-statistic},
	month = {oct},
	number = {10},
	pages = {619--620},
	publisher = {ACM},
	title = {{Algorithm 396: Student's t-Quantiles}},
	url = {http://doi.acm.org/10.1145/355598.355600},
	volume = {13},
	year = {1970}
}

@article{kachitvichyanukul:1985,
	abstract = {The paper presents an exact, uniformly fast algorithm for generating random variates from the hypergeometric distribution. The overall algorithm framework is acceptance/ rejection and is implemented via composition. Three subdensities are used, one is uniform and the other two are exponential. The algorithm is compared with algorithms based on sampling without replacement, inversion, and aliasing. A comprehensive survey of existing algorithms is also given.},
	author = {Voratas Kachitvichyanukul and Burce Schmeiser},
	doi = {10.1080/00949658508810839},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rand, prng, rng, pseudorandom, number, generator, hypergeometric},
	number = {2},
	pages = {127-145},
	title = {{Computer generation of hypergeometric random variates}},
	url = {http://dx.doi.org/10.1080/00949658508810839},
	volume = {22},
	year = {1985},
}

@article{nadler:2006,
	abstract = {\\(\em{Ziggurat}\\) and \\(\em{Monty Python}\\) are two fast and elegant methods proposed by Marsaglia and Tsang to transform uniform random variables to random variables with normal, exponential and other common probability distributions. While the proposed methods are theoretically correct, we show that there are various design flaws in the uniform pseudo random number generators (PRNG's) of their published implementations for both the normal and Gamma distributions. These flaws lead to non-uniformity of the resulting pseudo-random numbers and consequently to noticeable deviations of their outputs from the required distributions. In addition, we show that the underlying uniform PRNG of the published implementation of MATLAB's \texttt{randn}, which is also based on the Ziggurat method, is not uniformly distributed with correlations between consecutive pairs. Also, we show that the simple linear initialization of the registers in MATLAB's \texttt{randn} may lead to non-trivial correlations between output sequences initialized with different (related or even random unrelated) seeds. These, in turn, may lead to erroneous results for stochastic simulations.},
	author = {Boaz Nadler},
	doi = {},
	journal = {{arXiv}},
	keywords = {random, prng, rng, marsaglia, ziggurat, gaussian, monty python, uniform, rand},
	month = {mar},
	number = {},
	pages = {},
	title = {{Design Flaws in the Implementation of the Ziggurat and Monty Python methods (and some remarks on MATLAB randn)}},
	url = {https://arxiv.org/abs/math/0603058},
	volume = {abs/math/0603058},
	year = {2006},
}

@article{mcfarland:2016,
	abstract = {The ziggurat algorithm is a very fast rejection sampling method for generating pseudorandom numbers (PRNs) from statistical distributions. In the algorithm, rectangular sampling domains are layered on top of each other (resembling a ziggurat) to encapsulate the desired probability density function. Random values within these layers are sampled and then returned if they lie beneath the graph of the probability density function. Here, we present an implementation where ziggurat layers reside completely beneath the probability density function, thereby eliminating the need for any rejection test within the ziggurat layers. In the new algorithm, small overhanging segments of probability density remain to the right of each ziggurat layer, which can be efficiently sampled with triangularly shaped sampling domains. Median runtimes of the new algorithm for exponential and normal variates is reduced to 58\% and 53\%, respectively (collective range: 41–93\%). An accessible C library, along with extensions into Python and MATLAB/Octave are provided.},
	author = {Christopher D. McFarland},
	doi = {10.1080/00949655.2015.1060234},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rang, prng, rng, pseudorandom, ziggurat, gaussian, normal},
	number = {7},
	pages = {1281--1294},
	title = {{A modified ziggurat algorithm for generating exponentially and normally distributed pseudorandom numbers}},
	url = {http://dx.doi.org/10.1080/00949655.2015.1060234},
	volume = {86},
	year = {2016},
}

@unpublished{doornik:2005,
	abstract = {The ziggurat is an efficient method to generate normal random samples. It is shown that the standard Ziggurat fails a commonly used test. An improved version that passes the test is introduced. Flexibility is enhanced by using a plug-in uniform random number generator. An efficient double-precision version of the ziggurat algorithm is developed that has a very high period.},
	author = {Jurgen A. Doornik},
	keywords = {random, rand, prng, rng, ziggurat, normal, gaussian, pseudorandom},
	note = {Provides an improved algorithm for generating normally distributed pseudorandom numbers via a ziggurat algorithm.},
	title = {{An Improved Ziggurat Method to Generate Normal Random Samples}},
	url = {https://www.doornik.com/research/ziggurat.pdf},
	year = {2005},
}

@article{marsaglia:2000b,
	abstract = {We provide a new version of our ziggurat method for generating a random variable from a given decreasing density. It is faster and simpler than the original, and will produce, for example, normal or exponential variates at the rate of 15 million per second with a C version on a 400MHz PC. It uses two tables, integers \\(k_i\\), and reals \\(w_i\\). Some 99% of the time, the required \\(x\\) is produced by: Generate a random 32-bit integer \\(j\\) and let \\(i\\) be the index formed from the rightmost 8 bits of j. If \\(j < k\\), return \\(x = j \cdot w_i\\). We illustrate with C code that provides for inline generation of both normal and exponential variables, with a short procedure for setting up the necessary tables.},
	author = {George Marsaglia and Wai Wan Tsang},
	doi = {10.18637/jss.v005.i08},
	issn = {1548-7660},
	journal = {Journal of Statistical Software},
	keywords = {random, rand, prng, rng, pseudorandom, number, generator, ziggurat, normal, gaussian},
	number = {1},
	pages = {1--7},
	title = {{The Ziggurat Method for Generating Random Variables}},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v005i08},
	volume = {5},
	year = {2000},
}

@article{marsaglia:1964b,
	abstract = {},
	author = {George Marsaglia},
	doi = {10.1080/00401706.1964.10490150},
	journal = {Technometrics},
	keywords = {random, prng, rng, pseudorandom, number, generator, rand, normal, gaussian},
	number = {1},
	pages = {101-102},
	title = {{Generating a Variable from the Tail of the Normal Distribution}},
	url = {https://doi.org/10.1080/00401706.1964.10490150},
	volume = {6},
	year = {1964},
}

@article{park:1988,
	abstract = {Practical and theoretical issues are presented concerning the design, implementation, and use of a good, minimal standard random number generator that will port to virtually all systems.},
	acmid = {63042},
	address = {New York, NY, USA},
	author = {S. K. Park and K. W. Miller},
	doi = {10.1145/63039.63042},
	issn = {0001-0782},
	issue_date = {Oct. 1988},
	journal = {Communications of the ACM},
	keywords = {random, prng, rng, pseudorandom, number, generator},
	month = {oct},
	number = {10},
	numpages = {10},
	pages = {1192--1201},
	publisher = {ACM},
	title = {{Random Number Generators: Good Ones Are Hard to Find}},
	url = {http://doi.acm.org/10.1145/63039.63042},
	volume = {31},
	year = {1988},
}

@book{press:1992,
	author = {William H. Press and Brian P. Flannery and Saul A. Teukolsky and William T. Vetterling},
	isbn = {0521431085},
	keywords = {computing, software, programming},
	publisher = {Cambridge University Press},
	title = {{Numerical Recipes in C: The Art of Scientific Computing, Second Edition}},
	year = {1992},
}

@article{bays:1976,
	abstract = {},
	acmid = {355670},
	address = {New York, NY, USA},
	author = {Carter Bays and S. D. Durham},
	doi = {10.1145/355666.355670},
	issue_date = {March 1976},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {lcg, random, rnd, minstd, rand, rng, prng, pseudorandom, number, generator, linear congruential generator},
	month = {mar},
	number = {1},
	numpages = {6},
	issn = {0098-3500},
	pages = {59--64},
	publisher = {ACM},
	title = {{Improving a Poor Random Number Generator}},
	url = {http://doi.acm.org/10.1145/355666.355670},
	volume = {2},
	year = {1976},
}

@book{herzog:2002,
	author = {T.N. Herzog and G. Lord},
	isbn = {9781566984331},
	keywords = {finance, monte carlo, prng, rng, random, minstd, minstd-shuffle},
	lccn = {2002015049},
	pages = {19--},
	publisher = {ACTEX Publications},
	title = {{Applications of Monte Carlo Methods to Finance and Insurance}},
	url = {https://books.google.com/books?id=vC7I\_gdX-A0C},
	year = {2002},
}

@book{knuth:1997,
	address = {Boston, MA, USA},
	author = {Donald E. Knuth},
	isbn = {0-201-89684-2},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	title = {{The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms}},
	year = {1997},
}

@article{hormann:1993b,
	abstract = {The transformed rejection method, a combination of the inversion and the rejection method, which is used to generate non-uniform random numbers from a variety of continuous distributions can be applied to discrete distributions as well. For the Poisson distribution a short and simple algorithm is obtained which is well suited for large values of the Poisson parameter \\(\mu\\), even when \\(\mu\\) may vary from call to call. The average number of uniform deviates required is lower than for any of the known uniformly fast algorithms. Timings for a C implementation show that the algorithm needs only half of the code but is - for \\(\mu\\) not too small - at least as fast as the current state-of-the-art algorithms.},
	author = {W. H{\"{o}}rmann},
	doi = {10.1016/0167-6687(93)90997-4},
	issn = {0167-6687},
	journal = {Insurance: Mathematics and Economics},
	keywords = {Rejection method, Inversion, Decomposition, Poisson variate generation, Uniformly fast algorithm},
	note = {},
	number = {1},
	pages = {39--45},
	title = {{The transformed rejection method for generating Poisson random variables}},
	url = {http://www.sciencedirect.com/science/article/pii/0167668793909974},
	volume = {12},
	year = {1993},
}

@article{riehle:2012,
	abstract = {The design of software development tools follows from what the developers of such tools believe is true about software development. A key aspect of such beliefs is the size of code contributions (commits) to a software project. In this paper, we show that what tool developers think is true about the size of code contributions is different by more than an order of magnitude from reality. We present this reality, called the commit size distribution, for a large sample of open source and selected closed source projects. We suggest that these new empirical insights will help improve software development tools by aligning underlying design assumptions closer with reality.},
	author = {Dirk Riehle and Carsten Kolassa and Michel A. Salim},
	issn = {1617-5468},
	journal = {Software Engineering 2012, GI-Edition Lecture Notes in Informatics},
	keywords = {software, git, commits, developer, computer science},
	pages = {59--70},
	title = {{Developer Belief vs. Reality: The Case of the Commit Size Distribution}},
	url = {http://arxiv.org/abs/1408.4644},
	volume = {abs/1408.4644},
	year = {2012},
}

@inproceedings{arafat:2009a,
	abstract = {With the growing economic importance of open source, we need to improve our understanding of how open source software development processes work. The analysis of code contributions to open source projects is an important part of such research. In this paper we analyze the size of code contributions to more than 9,000 open source projects. We review the total distribution and distinguish three categories of code contributions using a size-based heuristic: single focused commits, aggregate team contributions, and repository refactorings. We find that both the overall distribution and the individual categories follow a power law. We also suggest that distinguishing these commit categories by size will benefit future analyses.},
	author = {Oliver Arafat and Dirk Riehle},
	booktitle = {Proceedings of the 42nd Hawaii International Conference on System Sciences},
	keywords = {software, computer science, metrics, development, developer, git, commits},
	publisher = {IEEE Computer Society},
	title = {{The Commit Size Distribution of Open Source Software}},
	year = {2009},
}

@inbook{hofmann:2009,
	abstract = {The quantitative analysis of software projects can provide insights that let us better understand open source and other software development projects. An important variable used in the analysis of software projects is the amount of work being contributed, the commit size. Unfortunately, post-facto, the commit size can only be estimated, not measured. This paper presents several algorithms for estimating the commit size. Our performance evaluation shows that simple, straightforward heuristics are superior to the more complex text-analysis-based algorithms. Not only are the heuristics significantly faster to compute, they also deliver more accurate results when estimating commit sizes. Based on this experience, we design and present an algorithm that improves on the heuristics, can be computed equally fast, and is more accurate than any of the prior approaches.},
	address = {Berlin, Heidelberg},
	author = {Philipp Hofmann and Dirk Riehle},
	bookTitle = {{Open Source Ecosystems: Diverse Communities Interacting: 5th IFIP WG 2.13 International Conference on Open Source Systems, OSS 2009, Sk\"{o}vde, Sweden, June 3-6, 2009. Proceedings}},
	doi = {10.1007/978-3-642-02032-2_11},
	editor = {Cornelia Boldyreff and Kevin Crowston and Bj\"{o}rn Lundell and Anthony I. Wasserman},
	isbn = {978-3-642-02032-2},
	pages = {105--115},
	publisher = {Springer Berlin Heidelberg},
	title = {{Estimating Commit Sizes Efficiently}},
	url = {http://dx.doi.org/10.1007/978-3-642-02032-2_11},
	year = {2009},
}

@article {joanes:1998,
	abstract = {Over the years, various measures of sample skewness and kurtosis have been proposed. Comparisons are made between those measures adopted by well-known statistical computing packages, focusing on bias and mean-squared error for normal samples, and presenting some comparisons from simulation results for non-normal samples.},
	author = {D. N. Joanes and C. A. Gill},
	doi = {10.1111/1467-9884.00122},
	issn = {1467-9884},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	keywords = {bias, kurtosis, mean-squared error, skewness, algorithms, math, mathematics, statistics, stats},
	number = {1},
	pages = {183--189},
	publisher = {Blackwell Publishers Ltd},
	title = {{Comparing measures of sample skewness and kurtosis}},
	url = {http://dx.doi.org/10.1111/1467-9884.00122},
	volume = {47},
	year = {1998},
}

@inproceedings{hattori:2008,
	abstract = {Information contained in versioning system commits has been frequently used to support software evolution research. Concomitantly, some researchers have tried to relate commits to certain activities, e.g., large commits are more likely to be originated from code management activities, while small ones are related to development activities. However, these characterizations are vague, because there is no consistent definition of what is a small or a large commit. In this paper, we study the nature of commits in two dimensions. First, we define the size of commits in terms of number of files, and then we classify commits based on the content of their comments. To perform this study, we use the history log of nine large open source projects.},
	author = {Lile P. Hattori and Michele Lanza},
	booktitle = {ASE Workshops 2008. 23rd IEEE/ACM International Conference on Automated Software Engineering - Workshops, 2008.},
	doi = {10.1109/ASEW.2008.4686322},
	keywords = {history, software systems, frequency, statistical distributions, inspection, informatics, research and development management, content management, project management, engineering management, metrics, git, commits},
	publisher = {IEEE},
	title = {{On the nature of commits}},
	year = {2008},
}

@manual{cocomo2,
	author = {Barry Boehm},
	edition = {1.4},
	keywords = {software, estimation, management, development},
	organization = {University of Southern California},
	title = {{COCOMO II Model Definition Manual}},
	url = {http://www.dmi.usherb.ca/~frappier/IFT721/COCOMOII.PDF}
}

@inproceedings{arafat:2009b,
	abstract = {The development processes of open source soft-ware are different from traditional closed source development processes. Still, open source software is frequently of high quality. This raises the question of how and why open source software creates high quality and whether it can maintain this quality for ever larger project sizes. In this paper, we look at one particular quality indicator, the density of comments in open source software code. We find that successful open source projects follow a consistent practice of documenting their source code, and we find that the comment density is independent of team and project size.},
	acmid = {1640047},
	address = {New York, NY, USA},
	author = {Oliver Arafat and Dirk Riehle},
	booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
	doi = {10.1145/1639950.1640047},
	isbn = {978-1-60558-768-4},
	keywords = {comment density, data mining, empirical software engineering, open source, software evolution},
	location = {Orlando, Florida, USA},
	numpages = {8},
	pages = {857--864},
	publisher = {ACM},
	series = {OOPSLA '09},
	title = {{The Commenting Practice of Open Source}},
	url = {http://doi.acm.org/10.1145/1639950.1640047},
	year = {2009},
}

@inbook{kolassa:2013a,
	abstract = {A fundamental unit of work in programming is the code contribution (“commit”) that a developer makes to the code base of the project in work. We use statistical methods to derive a model of the probabilistic distribution of commit sizes in open source projects and we show that the model is applicable to different project sizes. We use both graphical as well as statistical methods to validate the goodness of fit of our model. By measuring and modeling a fundamental dimension of programming we help improve software development tools and our understanding of software development.},
	address = {Berlin, Heidelberg},
	author = {Carsten Kolassa and Dirk Riehle and Michel A. Salim},
	bookTitle = {SOFSEM 2013: Theory and Practice of Computer Science: 39th International Conference on Current Trends in Theory and Practice of Computer Science, \v{S}pindler\r{u}v Ml\'{y}n, Czech Republic, January 26-31, 2013. Proceedings},
	editor = {Peter van Emde Boas and Frans C. A. Groen and Giuseppe F. Italiano and Jerzy Nawrocki and Harald Sack},
	doi = {10.1007/978-3-642-35843-2_6},
	isbn = {978-3-642-35843-2},
	keywords = {software, development, git, commits, management, estimation},
	pages = {52--66},
	publisher = {Springer Berlin Heidelberg},
	title = {{A Model of the Commit Size Distribution of Open Source}},
	url = {http://dx.doi.org/10.1007/978-3-642-35843-2_6},
	year = {2013},
}

@inproceedings{kolassa:2013b,
	abstract = {A fundamental unit of work in programming is the code contribution ("commit") that a developer makes to the code base of the project in work. An author's commit frequency describes how often that author commits. Knowing the distribution of all commit frequencies is a fundamental part of understanding software development processes. This paper presents a detailed quantitative analysis of commit frequencies in open-source software development. The analysis is based on a large sample of open source projects, and presents the overall distribution of commit frequencies.\n\nWe analyze the data to show the differences between authors and projects by project size; we also includes a comparison of successful and non successful projects and we derive an activity indicator from these analyses. By measuring a fundamental dimension of programming we help improve software development tools and our understanding of software development. We also validate some fundamental assumptions about software development.},
	acmid = {2491073},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Carsten Kolassa and Dirk Riehle and Michel A. Salim},
	booktitle = {Proceedings of the 9th International Symposium on Open Collaboration},
	doi = {10.1145/2491055.2491073},
	isbn = {978-1-4503-1852-5},
	keywords = {software, development, management, estimation, commits, git, metrics},
	location = {Hong Kong, China},
	numpages = {8},
	pages = {18:1--18:8},
	publisher = {ACM},
	series = {WikiSym '13},
	title = {{The Empirical Commit Frequency Distribution of Open Source Projects}},
	url = {http://doi.acm.org/10.1145/2491055.2491073},
	year = {2013},
}

@article{hope:1968,
	abstract = {The use of Monte Carlo test procedures for significance testing, with smaller reference sets than are now generally used, is advocated. It is shown that, for given \\(\alpha = 1/n\\), \\(n\\) a positive integer, the power of the Monte Carlo test procedure is a monotone increasing function of the size of the reference set, the limit of which is the power of the corresponding uniformly most powerful test. The power functions and efficiency of the Monte Carlo test to the uniformly most powerful test are discussed in detail for the case where the test criterion is \\(N(\gamma, 1)\\). The cases when the test criterion is Student's t-statistic and when the test statistic is exponentially distributed are considered also.},
	author = {Adery C. A. Hope},
	issn = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	keywords = {chisq, chisquare, chi-squared, chi-square, goodness-of-fit, monte carlo, simulate, simulation},
	number = {3},
	pages = {582--598},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {{A Simplified Monte Carlo Significance Test Procedure}},
	url = {http://www.jstor.org/stable/2984263},
	volume = {30},
	year = {1968},
}

@article{temme:1992,
	abstract = {The incomplete Laplace integral\\ \\( \[ \frac{1}{{\Gamma (\lambda )}}\int_\alpha ^\infty {t^{\lambda - 1} e^{ - zt} f(t)dt} \] \\) is considered for large values of z. Both \\( \lambda \\) and \\( \alpha \\) are uniformity parameters in \\( [0,\infty ) \\). The basic approximant is an incomplete gamma function, that is, the above integral with \\( f = 1 \\). Also, a loop integral in the complex plane is considered with the same asymptotic features. The asymptotic expansions are furnished with error bounds for the remainders in the expansions. The results of the paper combine four kinds of asymptotic problems considered earlier. An application is given for the incomplete beta function. The present investigations are a continuation of earlier works of the author for the above integral with \\( \alpha = 0 \\). The new results are significantly based on the previous case.},
	author = {Nico M. Temme},
	doi = {10.1016/0377-0427(92)90244-R},
	isbn = {0377-0427},
	journal = {Journal of Computational and Applied Mathematics},
	keywords = {incomplete beta function,asymptotic expansion,inversion of beta distribution},
	number = {1--2},
	pages = {1638--1663},
	title = {{Incomplete Laplace Integrals: Uniform Asymptotic Expansion with Application to the Incomplete Beta Function}},
	volume = {41},
	year = {1992},
}

@article{marsaglia_tsang:2003,
	abstract = {Kolmogorov's goodness-of-fit measure, \\( D_n \\), for a sample CDF has consistently been set aside for methods such as the D+n or D-n of Smirnov, primarily, it seems, because of the difficulty of computing the distribution of \\( D_n \\). As far as we know, no easy way to compute that distribution has ever been provided in the 70+ years since Kolmogorov's fundamental paper. We provide one here, a C procedure that provides \\( \Pr(D_n < d) \\) with 13-15 digit accuracy for n ranging from 2 to at least 16000. We assess the (rather slow) approach to limiting form, and because computing time can become excessive for probabilities >.999 with n's of several thousand, we provide a quick approximation that gives accuracy to the 7th digit for such cases.},
	author = {George Marsaglia and Wai Wan Tsang and Jingbo Wang},
	doi = {10.18637/jss.v008.i18},
	journal = {Journal of Statistical Software},
	issn = {1548-7660},
	keywords = {cdf, kolmogorov, goodness-of-fit, distribution},
	number = {18},
	pages = {1--4},
	title = {{Evaluating Kolmogorov's Distribution}},
	url = {https://www.jstatsoft.org/v008/i18},
	volume = {8},
	year = {2003},
}

@article{broucke:1973,
	acmid = {362037},
	address = {New York, NY, USA},
	author = {Roger Broucke},
	doi = {10.1145/362003.362037},
	issn = {0001-0782},
	issue_date = {April 1973},
	journal = {Communications of the ACM},
	keywords = {Chebyshev series, approximations, curve fitting, differentiation, integration, negative powers},
	month = {apr},
	number = {4},
	numpages = {3},
	pages = {254--256},
	publisher = {ACM},
	title = {{Algorithm: Ten Subroutines for the Manipulation of Chebyshev Series}},
	url = {http://doi.acm.org/10.1145/362003.362037},
	volume = {16},
	year = {1973},
}

@book{fox:1968,
	address = {London, United Kingdom},
	author = {Leslie Fox and Ian Bax Parker},
	keywords = {numerical analysis, chebyshev, polynomials},
	lccn = {lc68105838},
	publisher = {Oxford University Press},
	series = {Oxford mathematical handbooks},
	title = {{Chebyshev polynomials in numerical analysis}},
	url = {https://books.google.com/books?id=F8NzsEtJCD0C},
	year = {1968},
}

@book{schneier:1996,
	address = {New York},
	author = {Bruce Schneier},
	edition = {2nd},
	isbn = {0--471--12845--7},
	keywords = {cryptography, crypto, cipher, algorithms, encrypt, decrypt, programming, hacking},
	publisher = {Wiley},
	title = {{Applied Cryptography: Protocols, Algorithms, and Source Code in C}},
	year = {1996},
}

@article{patefield:1981,
	author = {W. M. Patefield},
	issn = {1467-9876},
	journal = {Applied Statistics},
	keywords = {algorithm, algo, statistics, stats, chi-square, chi2, chi-squared, chisquare, chisquared, contingency table},
	month = {mar},
	number = {1},
	pages = {91--97},
	title = {{Statistical Algorithms: {Algorithm AS 159}: An Efficient Method of Generating Random \\(R \times C\\) Tables with Given Row and Column Totals}},
	url = {http://lib.stat.cmu.edu/apstat/159},
	volume = {30},
	year = {1981},
}

@book{devroye:1986,
	author = {Luc Devroye},
	doi = {10.1007/978-1-4613-8643-8},
	isbn = {978-1-4613-8643-8},
	keywords = {random, pseudorandom, prng, rng, rand, generator, algorithms},
	publisher = {Springer New York},
	title = {{Non-Uniform Random Variate Generation}},
	url = {http://www.nrbook.com/devroye/},
	year = {1986},
}

@article{stein:1967,
	abstract = {In a program written by the author, for performing various calculations in the Racah algebra, some new calculational methods were used; these methods are described in this paper. They include: (1) a representation of numbers alternative to the Rotenberg form; (2) a new algorithm for calculating the greatest common devisor of two odd integers, which was developed for reducing the calculating time in the new representation; (3) methods for shortening the computation time of 3-j and 6-j symbols.},
	author = {Josef Stein},
	doi = {10.1016/0021-9991(67)90047-2},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	keywords = {gcd, gcf, hcf, gcm, math, mathematics, algorithm, arithmetic, greatest, common, divisor},
	number = {3},
	pages = {397--405},
	title = {{Computational problems associated with Racah algebra}},
	url = {http://www.sciencedirect.com/science/article/pii/0021999167900472},
	volume = {1},
	year = {1967},
}

@article{lentz:1976,
	abstract = {A new method of generating the Bessel functions and ratios of Bessel functions necessary for Mie calculations is presented. Accuracy is improved while eliminating the need for extended precision word lengths or large storage capability. The algorithm uses a new technique of evaluating continued fractions that starts at the beginning rather than the tail and has a built-in error check. The continued fraction representations for both spherical Bessel functions and ratios of Bessel functions of consecutive order are presented.},
	author = {William J. Lentz},
	doi = {10.1364/AO.15.000668},
	isbn = {0003-6935},
	issn = {0003-6935},
	journal = {Applied Optics},
	keywords = {bessel, continued fraction},
	number = {3},
	pages = {668--671},
	pmid = {20165036},
	title = {{Generating bessel functions in Mie scattering calculations using continued fractions}},
	volume = {15},
	year = {1976},
}

@mastersthesis{steinarsson:2013,
	abstract = {As human beings, we often wish to visualize certain information in order to make better sense of it. This can be a somewhat challenging enterprise for large amounts of data and might require downsampling the data, retaining only the important visual characteristics. The focus of this thesis is to explore methods for downsampling data which can be visualized in the form of a line chart, for example, time series. Several algorithms are put forth in the thesis and their features are discussed. Also, an online survey was conducted where participants were asked to compare downsampled line charts against a non-downsampled chart. Some of the algorithms are based on a well-known technique in cartography which involves forming triangles between adjacent data points and using the area of the triangles to determine the perceptual importance of the individual points. According to the survey, algorithms based on a triangle area approach consistently proved to be effective, and one in particular when efficiency is also taken into account.},
	author = {Svein Steinarsson},
	keywords = {downsample, timeseries, plot, visualize, visualization, dataviz, datavis, time-series, time, series},
	school = {University of Iceland},
	title = {{Downsampling Time Series for Visual Representation}},
	url = {http://skemman.is/handle/1946/15343},
	year = {2013}
}

@unpublished{junod:1999,
	abstract = {Random numbers are critical in every aspect of cryptography. We need such numbers to encrypt e-mails, to digitally sign documents, for electronic payment systems, and so on. Unfortunately, true random numbers are very difficult to generate, especially on computers that are typically designed to be deterministic. This brings us to the concept of pseudo-random numbers, which are numbers generated from some random internal values, and that are very hard for an observer to distinguish from true random numbers. It is important to see the difference between the meaning of pseudo-random numbers in normal programming contexts, like simulation, e.g., where these numbers merely need to be reasonably random-looking and have good statistical properties, and in the context of cryptography, where they must be indistinguishable from real random numbers, even to observers with huge amount of computational resources. In the context of cryptography, a random number is a number that cannot be predicted by an observer before it is generated. Typically, if the number is to be in the range \\([0..n − 1]\\), an observer cannot predict that number with probability "slightly" better than \\(1/n\\). Or, we will see that the following is equivalent, if \\(m\\) random numbers are generated in a row, an observer given any \\(m − 1\\) of them still cannot predict the \\(m\\)'th with a probability significantly greater than \\(1/n\\_. In this work, we present first the notion of cryptographic secure pseudorandom bit generators (PRBG) in a formal way by using two different definitions. Then a theorem of Yao proving the equivalence of these two definitions is treated. In a second part, the Blum-Blum-Shub generator, a very simple and provably secure PRBG, is presented, with all the mathematical background needed to understand it. In the third part, the proof of its security is treated in details.},
	author = {Pascal Junod},
	keywords = {prng, rng, blum blum shub, random, pseudorandom, crypto, cryptographic, secure},
	note = {},
	title = {{Cryptographic Secure Pseudo-Random Bits Generation : The Blum-Blum-Shub Generator}},
	url = {http://www.cs.miami.edu/home/burt/learning/Csc609.062/docs/bbs.pdf},
	year = {1999},
}

@inproceedings{sidorenko:2005,
	acmid = {2179250},
	address = {Berlin, Heidelberg},
	abstract = {The asymptotic security of the Blum-Blum-Shub (BBS) pseudorandom generator has been studied by Alexi et al. and Vazirani and Vazirani, who proved independently that O(log log N) bits can be extracted on each iteration, where \\(N\\) is the modulus (a Blum integer). The concrete security of this generator has been analyzed previously by Fischlin and Schnorr and by Knuth. In this paper we continue to analyse the concrete security the BBS generator. We show how to select both the size of the modulus and the number of bits extracted on each iteration such that a desired level of security is reached, while minimizing the computational effort per output bit. We will assume a concrete lower bound on the hardness of integer factoring, which is obtained by extrapolating the best factorization results to date. While for asymptotic security it suffices to give a polynomial time reduction a successful attack to factoring, we need for concrete security a reduction that is as efficient as possible. Our reduction algorithm relies on the techniques of Fischlin and Schnorr, as well as ideas of Vazirani and Vazirani, but combining these in a novel way for the case that more than one bit is output on each iteration.},
	author = {Andrey Sidorenko and Berry Schoenmakers},
	booktitle = {Proceedings of the 10th International Conference on Cryptography and Coding},
	doi = {10.1007/11586821_24},
	isbn = {3-540-30276-X, 978-3-540-30276-6},
	keywords = {prng, rng, random, pseudorandom, crypto, cryptographic, secure, blum blum shub},
	location = {Cirencester, UK},
	numpages = {21},
	pages = {355--375},
	publisher = {Springer-Verlag},
	series = {IMA'05},
	title = {{Concrete Security of the Blum-blum-shub Pseudorandom Generator}},
	url = {http://dx.doi.org/10.1007/11586821_24},
	year = {2005},
}

@article{blum:1983a,
	abstract = {Two closely-related pseudo-random sequence generators are presented: The \\(1 / P\\) generator, with input \\(P\\) a prime, outputs the quotient digits obtained on dividing \\(1\\) by \\(P\\). The \\(x^2 \bmod N\\) generator with inputs \\(N\\), \\(x_0\\) (where \\(N = P \cdot Q\\) is a product of distinct primes, each congruent to \\(3 \mod 4\\), and \\(x_0\\) is a quadratic residue \\(\bmod N\\)), outputs \\(b_0 b_1 b_2 \cdots \\) where \\(b_i = \operatorname{parity}(x_i )\\) and \\(x_{i + 1} = x_i^2 \bmod N\\). From short seeds each generator efficiently produces long well-distributed sequences. Moreover, both generators have computationally hard problems at their core. The first generator's sequences, however, are completely predictable (from any small segment of \\(2|P| + 1\\) consecutive digits one can infer the "seed," \\(P\\), and continue the sequence backwards and forwards), whereas the second, under a certain intractability assumption, is unpredictable in a precise sense. The second generator has additional interesting properties: from knowledge of \\(x_0\\) and \\(N\\) but not \\(P\\) or \\(Q\\), one can generate the sequence forwards, but, under the above-mentioned intractability assumption, one can not generate the sequence backwards. From the additional knowledge of \\(P\\) and \\(Q\\), one can generate the sequence backwards; one can even "jump" about from any point in the sequence to any other. Because of these properties, the \\(x^2 \bmod N\\) generator promises many interesting applications, e.g., to public-key cryptography. To use these generators in practice, an analysis is needed of various properties of these sequences such as their periods. This analysis is begun here.},
	author = {Lenore Blum and Manuel Blum and Mike Shub},
	doi = {10.1137/0215025},
	journal = {SIAM Journal on Computing},
	keywords = {prng, rng, random, rand, blum blum shub, crypto, cryptographic, secure},
	month = {Aug},
	number = {2},
	pages = {364–-383},
	title = {{A Simple Unpredictable Pseudo-Random Number Generator}},
	url = {http://epubs.siam.org/doi/abs/10.1137/0215025},
	volume = {15},
	year = {1983},
}

@inbook{blum:1983b,
	abstract = {What do we want from a pseudo-random sequence generator? Ideally, we would like a pseudo-random sequence generator to quickly produce, from short seeds, long sequences (of bits) that appear in every way to be generated by successive flips of a fair coin.},
	address = {Boston, MA},
	author = {Lenore Blum and Manuel Blum and Mike Shub},
	booktitle = {Advances in Cryptology: Proceedings of Crypto 82},
	doi = {10.1007/978-1-4757-0602-4_6},
	editor= {David Chaum and Ronald L. Rivest and Alan T. Sherman},
	isbn = {978-1-4757-0602-4},
	keywords = {prng, rng, random, rand, blum blum shub, crypto, cryptographic, secure},
	publisher = {Springer US},
	pages = {61--78},
	title = {{Comparison of Two Pseudo-Random Number Generators}},
	url = {http://dx.doi.org/10.1007/978-1-4757-0602-4_6},
	year = {1983},
}

@article{lamport:1999,
	abstract = {Most specification languages have a type system. Type systems are hard to get right, and getting them wrong can lead to inconsistencies. Set theory can serve as the basis for a specification language without types. This possibility, which has been widely overlooked, offers many advantages. Untyped set theory is simple and is more flexible than any simple typed formalism. Polymorphism, overloading, and subtyping can make a type system more powerful, but at the cost of increased complexity, and such refinements can never attain the flexibility of having no types at all. Typed formalisms have advantages, too, stemming from the power of mechanical type checking. While types serve little purpose in hand proofs, they do help with mechanized proofs. In the absence of verification, type checking can catch errors in specifications. It may be possible to have the best of both worlds by adding typing annotations to an untyped specification language. We consider only specification languages, not programming languages.},
	acmid = {319317},
	address = {New York, NY, USA},
	author = {Leslie Lamport and 	Lawrence C. Paulson},
	doi = {10.1145/319301.319317},
	issn = {0164-0925},
	issue_date = {May 1999},
	journal = {ACM Transactions on Programming Languages and Systems},
	keywords = {set theory, specification, types},
	month = {may},
	number = {3},
	numpages = {25},
	pages = {502--526},
	publisher = {ACM},
	title = {{Should Your Specification Language Be Typed}},
	url = {http://doi.acm.org/10.1145/319301.319317},
	volume = {21},
	year = {1999},
}

@book{lyons:2011,
	author = {Richard G. Lyons},
	keywords = {digital signal processing, dsp},
	publisher = {Prentice Hall},
	title = {{Understanding Digital Signal Processing, 3rd Edition}},
	year = {2011},
}

@techreport{selakovic:2015,
	abstract = {As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-efficiency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that inefficient usage of APIs is the most prevalent root cause. Furthermore, we find that most issues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns.},
	author = {Marija Selakovic and Michael Pradel},
	institution = {TU Darmstadt, Department of Computer Science},
	keywords = {javascript, js, performance, perf, optimization, optimize, v8, chrome, firefox, spidermonkey},
	title = {{Performance Issues and Optimizations in JavaScript: An Empirical Study}},
	url = {http://mp.binaervarianz.de/JS_perf_study_TR_Oct2015.pdf},
	year = {2015},
}

@article{avelino:2016a,
	abstract = {Truck Factor (TF) is a metric proposed by the agile community as a tool to identify concentration of knowledge in software development environments. It states the minimal number of developers that have to be hit by a truck (or quit) before a project is incapacitated. In other words, TF helps to measure how prepared is a project to deal with developer turnover. Despite its clear relevance, few studies explore this metric. Altogether there is no consensus about how to calculate it, and no supporting evidence backing estimates for systems in the wild. To mitigate both issues, we propose a novel (and automated) approach for estimating TF-values, which we execute against a corpus of 133 popular project in GitHub. We later survey developers as a means to assess the reliability of our results. Among others, we find that the majority of our target systems (65%) have TF <= 2. Surveying developers from 67 target systems provides confidence towards our estimates; in 84% of the valid answers we collect, developers agree or partially agree that the TF's authors are the main authors of their systems; in 53% we receive a positive or partially positive answer regarding our estimated truck factors.},
	author = {Guilherme Avelino and Leonardo Passos and Andr{\'{e}} Hora and Marco Tulio Valente},
	doi = {},
	journal = {CoRR},
	keywords = {bus, truck, factor, software, metrics, measure, sustainability, development, meta},
	month = {apr},
	note = {For discussion, see https://news.ycombinator.com/item?id=9874503.},
	number = {},
	pages = {},
	title = {{A Novel Approach for Estimating Truck Factors}},
	url = {http://arxiv.org/abs/1604.06766},
	volume = {abs/1604.06766},
	year = {2016},
}

@article{muller:2005a,
	abstract = {Function ulp (acronym for unit in the last place) is frequently used for expressing errors in floating-point computations. We present several previously suggested definitions of that function, and analyse some of their properties.},
	author = {Jean-Michel Muller},
	doi = {},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {floating-point, arithmetic, computer arithmetic, unit in the last place, ulp, ieee754, precision, accuracy, math, mathematics, epsilon},
	month = {nov},
	number = {N},
	pages = {},
	title = {{On the definition of ulp(x)}},
	url = {http://ljk.imag.fr/membres/Carine.Lucas/TPScilab/JMMuller/ulp-toms.pdf},
	volume = {V},
	year = {2005},
}

@book{chall:1995a,
	author = {Jeanne Sternlicht Chall and Edgar Dale},
	isbn = {9781571290083},
	keywords = {nlp, english, language, lang, readability, readable, difficulty},
	publisher = {Brookline Books},
	title = {{Readability revisited: the new Dale-Chall readability formula}},
	url = {https://books.google.com/books?id=2nbuAAAAMAAJ},
	year = {1995},
}

@article{klare:1974a,
	abstract = {One of the problems in public education and mass communication is how to tell whether a particular piece of writing is likely to be readable to a particular group of readers. Two major solutions are possible: measuring and predicting readability. Measuring, by judgments or tests, involves using readers. Predicting by readability formulas, does not involve readers but instead uses counts of language elements in the piece of writing. This article reviews formulas and related predictive devices since 1960. Four categories are presented: 1) recalculations and revisions of existing formulas; 2) new formulas, for general purpose or special purpose use; 3) application aids, for both manual and machine use; and, 4) predictions of readability for foreign languages. It concludes with suggestions for choosing a formula, based upon the following considerations: 1) special versus general needs, 2) manual versus machine application, 3) simple versus complex formulas, 4) word length versus word list formulas; and, 5) sentence length versus sentence complexity. Finally, the article stresses that formulas provide good indices of difficulty, but do not indicate causes of difficulty or say how to write readably.},
	author = {George R. Klare},
	doi = {},
	issn = {00340553},
	journal = {Reading Research Quarterly},
	keywords = {readability, nlp, language, lang, english, en, readable, words, data},
	number = {1},
	pages = {62--102},
	publisher = {Wiley, International Reading Association},
	title = {{Assessing Readability}},
	url = {http://www.jstor.org/stable/747086},
	volume = {10},
	year = {1974},
}

@article{spache:1953a,
	author = {George Spache},
	doi = {10.1086/458513},
	journal = {The Elementary School Journal},
	keywords = {readability, readable, language, lang, nlp, english, en, dataset, data, words, simple},
	number = {7},
	pages = {410--413},
	title = {{A New Readability Formula for Primary-Grade Reading Materials}},
	url = {http://dx.doi.org/10.1086/458513},
	volume = {53},
	year = {1953},
}

@article{stone:1956a,
	author = {Clarence R. Stone},
	issn = {00135984, 15548279},
	journal = {The Elementary School Journal},
	keywords = {spache, readability, readable, language, lang, english, metric, formula, measure},
	number = {1},
	pages = {36--41},
	publisher = {University of Chicago Press},
	title = {{Measuring Difficulty of Primary Reading Material: A Constructive Criticism of Spache's Measure}},
	url = {http://www.jstor.org/stable/999700},
	volume = {57},
	year = {1956},
}

@incollection{perera:2012a,
	author = {Katherine Perera},
	booktitle = {{Linguistics and the Teacher}},
	chapter = {7},
	editor = {Ronald Carter},
	isbn = {9781136515422},
	keywords = {readability, readable, difficulty, english, en, metrics, measures, language, lang, linguistics, nlp},
	pages = {101--113},
	publisher = {Taylor \& Francis},
	series = {Routledge Library Editions: Education},
	title = {{The assessment of linguistic difficulty in reading material}},
	url = {https://books.google.com/books?id=oNXFQ9Gn6XIC},
	year = {2012},
}

@unpublished{miller:2008a,
	abstract = {For large UNIX projects, the traditional method of building the project is to use recursive make. On some projects, this results in build times which are unacceptably large, when all you want to do is change one file. In examining the source of the overly long build times, it became evident that a number of apparently unrelated problems combine to produce the delay, but on analysis all have the same root cause. This paper explores a number of problems regarding the use of recursive make, and shows that they are all symptoms of the same problem. Symptoms that the UNIX community have long accepted as a fact of life, but which need not be endured any longer. These problems include recursive makes which take "forever" to work out that they need to do nothing, recursive makes which do too much, or too little, recursive makes which are overly sensitive to changes in the source code and require constant Makefile intervention to keep them working. The resolution of these problems can be found by looking at what make does, from first principles, and then analyzing the effects of introducing recursive make to this activity. The analysis shows that the problem stems from the artificial partitioning of the build into separate subsets. This, in turn, leads to the symptoms described. To avoid the symptoms, it is only necessary to avoid the separation; to use a single make session to build the whole project, which is not quite the same as a single Makefile. This conclusion runs counter to much accumulated folk wisdom in building large projects on UNIX. Some of the main objections raised by this folk wisdom are examined and shown to be unfounded. The results of actual use are far more encouraging, with routine development performance improvements significantly faster than intuition may indicate, and without the intuitively expected compromise of modularity. The use of a whole project make is not as difficult to put into practice as it may at first appear.},
	author = {Peter Miller},
	keywords = {make, makefile, build},
	note = {Discussion on recursive make.},
	title = {{Recursive Make Considered Harmful}},
	url = {http://aegis.sourceforge.net/auug97.pdf},
	year = {2008},
}

@article{hand:2006a,
	abstract = {A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm.},
	author = {David J Hand},
	doi = {10.1214/088342306000000060},
	journal = {Statistical Science},
	keywords = {ml, machine, learning, ai, classifier, supervised, unsupervised, math, data, science, statistics, stats},
	month = {02},
	number = {1},
	pages = {1--14},
	publisher = {The Institute of Mathematical Statistics},
	title = {{Classifier Technology and the Illusion of Progress}},
	url = {http://dx.doi.org/10.1214/088342306000000060},
	volume = {21},
	year = {2006},
}

@inproceedings{nielson:2011a,
	abstract = {Sentiment analysis of microblogs such as Twitter has recently gained a fair amount of attention. One of the simplest sentiment analysis approaches compares the words of a posting against a labeled word list, where each word has been scored for valence, -- a 'sentiment lexicon' or 'affective word lists'. There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs. I used manually labeled postings from Twitter scored for sentiment. Using a simple word matching I show that the new word list may perform better than ANEW, though not as good as the more elaborate approach found in SentiStrength.},
	author = {Finn {\AA}rup Nielsen},
	booktitle = {{Proceedings of the {ESWC}2011 Workshop on 'Making Sense of Microposts': Big things come in small packages.}},
	doi = {},
	keywords = {datasets, data, sentiment, analysis, nlp, microblogs, twitter},
	month = {05},
	note = {arXiv link: http://arxiv.org/abs/1103.2903},
	pages = {93--98},
	publisher = {CEUR Workshop Proceedings},
	title = {{A new ANEW: Evaluation of a word list for sentiment analysis in microblogs}},
	url = {http://ceur-ws.org/Vol-718/paper_16.pdf},
	volume = {718},
	year = {2011},
}

@inbook{hansen:2011a,
	abstract = {The link between affect, defined as the capacity for sentimental arousal on the part of a message, and virality, defined as the probability that it be sent along, is of significant theoretical and practical importance, e.g. for viral marketing. The basic measure of virality in Twitter is the probability of retweet and we are interested in which dimensions of the content of a tweet leads to retweeting. We hypothesize that negative news content is more likely to be retweeted, while for non-news tweets positive sentiments support virality. To test the hypothesis we analyze three corpora: A complete sample of tweets about the COP15 climate summit, a random sample of tweets, and a general text corpus including news. The latter allows us to train a classifier that can distinguish tweets that carry news and non-news information. We present evidence that negative sentiment enhances virality in the news segment, but not in the non-news segment. Our findings may be summarized 'If you want to be cited: Sweet talk your friends or serve bad news to the public'.},
	address = {Berlin, Heidelberg},
	author = {Lars Kai Hansen and Adam Arvidsson and Finn {\AA}rup Nielsen and Elanor Colleoni and Michael Etter},
	bookTitle = {{Future Information Technology: 6th International Conference, FutureTech 2011, Loutraki, Greece, June 28-30, 2011, Proceedings, Part II}},
	doi = {10.1007/978-3-642-22309-9_5},
	editor = {James J. Park and Laurence T. Yang and Changhoon Lee},
	isbn = {978-3-642-22309-9},
	keywords = {datasets, data, sentiment, analysis, nlp, microblogs, twitter},
	pages = {34--43},
	publisher = {Springer Berlin Heidelberg},
	title = {{Good Friends, Bad News - Affect and Virality in Twitter}},
	url = {http://dx.doi.org/10.1007/978-3-642-22309-9_5},
	year = {2011},
}

@article{higham:1993a,
	abstract = {The usual recursive summation technique is just one of several ways of computing the sum of \\(n\\) floating point numbers. Five summation methods and their variations are analyzed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases.},
	author = {Nicholas J. Higham},
	doi = {10.1137/0914050},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {numeric, computing, accuracy, precision, summation, algorithms, numerical, floating-point, double, float, ieee754},
	month = {07},
	number = {4},
	pages = {783--799},
	title = {{The Accuracy of Floating Point Summation}},
	url = {http://dx.doi.org/10.1137/0914050},
	volume = {14},
	year = {1993},
}

@book{ueberhuber:1997a,
	author = {Christoph W. Ueberhuber},
	doi = {10.1007/978-3-642-59118-1},
	isbn = {978-3-540-62058-7},
	keywords = {numerical, computation, compute, math, mathematics, methods},
	publisher = {Springer-Verlag Berlin Heidelberg},
	title = {{Numerical Computation 1: Methods, Software, and Analysis}},
	year = {1997},
}

@article{chen:2016a,
	abstract = {We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.},
	author = {Jiahao Chen and Jarrett Revels},
	doi = {},
	journal = {CoRR},
	keywords = {performance, perf, benchmarks, benchmark, measure, numeric computing, julia},
	month = {08},
	number = {},
	pages = {},
	title = {{Robust benchmarking in noisy environments}},
	url = {http://arxiv.org/abs/1608.04295},
	volume = {abs/1608.04295},
	year = {2016},
}

@article{joehnk:1964,
	abstract= {},
	author = {M.D. J{\u{o}}hnk},
	doi = {},
	journal = {Metrika},
	keywords = {probability theory, sampling, random numbers},
	month = {},
	pages = {5-15},
	title = {{Erzeugung von betaverteilten und gammaverteilten Zufallszahlen}},
	url = {http://eudml.org/doc/175224},
	volume = {8},
	year = {1964},
}

@misc{ward:2002a,
	author = {Grady Ward},
	keywords = {names, list, english, american, british, first name, given name, female, woman, girl},
	note = {},
	title = {{Moby Word II}},
	url = {http://www.gutenberg.org/files/3201/3201.txt},
	year = {2002}
}

@misc{frbsf:wagerigidity,
	author = {{Federal Reserve Bank of San Francisco}},
	keywords = {wages, wage rigidity, federal reserve, employment, data, dataset, economics, economy, workers},
	note = {Wage rates for U.S. workers that have not changed jobs within the year.},
	title = {{Wage Rigidity}},
	url = {http://www.frbsf.org/economic-research/indicators-data/nominal-wage-rigidity/},
	year = {2017}
}

@misc{azari:1929a,
	author = {F{\'{e}}d{\`{e}}le Azari},
	keywords = {image, img, data, dataset, airplane, flight, flying, transport, transportation},
	note = {F{\'{e}}d{\`{e}}le Azari's gelatin silver print of an airplane, viewed from above looking down. Sourced from Getty's Open Content Program.},
	title = {{(no title)}},
	url = {http://www.getty.edu/art/collection/objects/134512/fedele-azari-airplane-viewed-from-above-looking-down-italian-1914-1929/},
	year = {1929}
}

@misc{blossfeldt:1928a,
	author = {Karl Blossfeldt},
	keywords = {image, img, data, dataset, flower, plant},
	note = {Karl Blossfeldt's gelatin silver print Acanthus mollis. Sourced from Getty's Open Content Program.},
	title = {{Acanthus mollis}},
	url = {http://www.getty.edu/art/collection/objects/35443/karl-blossfeldt-acanthus-mollis-german-1928/},
	year = {1928}
}

@misc{blossfeldt:1928b,
	author = {Karl Blossfeldt},
	keywords = {image, img, data, dataset, flower, plant},
	note = {Karl Blossfeldt's gelatin silver print Allium ostrowskianum. Sourced from Getty's Open Content Program.},
	title = {{Allium ostrowskianum}},
	url = {http://www.getty.edu/art/collection/objects/35448/karl-blossfeldt-allium-ostrowskianum-knoblauchpflanze-german-1928/},
	year = {1928}
}

@misc{osullivan:1871a,
	author = {Timothy H. O'Sullivan},
	keywords = {image, img, data, dataset, river, us, united states, canyon, place, nature},
	note = {Timothy H. O'Sullivan's albumen silver print Black Ca{\~{n}}on, Colorado River, From Camp 8, Looking Above. Sourced from Getty's Open Content Program.},
	title = {{Black Ca{\~{n}}on, Colorado River, From Camp 8, Looking Above}},
	url = {http://www.getty.edu/art/collection/objects/40209/timothy-h-o'sullivan-black-canon-colorado-river-from-camp-8-looking-above-american-1871/},
	year = {1871}
}

@misc{lange:1940a,
	author = {Dorothea Lange},
	keywords = {image, img, data, dataset, home, house, dust bowl, west},
	note = {Dorothea Lange's gelatin silver print of an abandoned Dust Bowl home. Sourced from Getty's Open Content Program.},
	title = {{Abandoned Dust Bowl Home}},
	url = {http://www.getty.edu/art/collection/objects/128362/dorothea-lange-abandoned-dust-bowl-home-american-about-1935-1940/},
	year = {1940}
}

@misc{braun:1870a,
	author = {Adolphe Braun},
	keywords = {image, img, data, dataset, alpine, landscape, french, france, nature, place},
	note = {Adolphe Braun's carbon print of a French alpine landscape. Sourced from Getty's Open Content Program.},
	title = {{(no title)}},
	url = {http://www.getty.edu/art/collection/objects/54324/adolphe-braun-alpine-landscape-french-1865-1870/},
	year = {1870}
}

@misc{muybridge:1887a,
	author = {Eadweard J. Muybridge},
	keywords = {image, img, data, dataset, cat, animal, moving},
	note = {Eadweard J. Muybridge's collotype of a house cat (in twenty-four views). Sourced from Getty's Open Content Program.},
	title = {{Animal Locomotion}},
	url = {http://www.getty.edu/art/collection/objects/40918/eadweard-j-muybridge-animal-locomotion-american-1887/},
	year = {1887}
}

@misc{muybridge:1887b,
	author = {Eadweard J. Muybridge},
	keywords = {image, img, data, dataset, male, nude, human, man, moving},
	note = {Eadweard J. Muybridge's collotype of a nude male moving in place (in forty-eight views). Sourced from Getty's Open Content Program.},
	title = {{Animal Locomotion}},
	url = {http://www.getty.edu/art/collection/objects/40907/eadweard-j-muybridge-animal-locomotion-american-1887/},
	year = {1887}
}

@misc{emerson:1888a,
	author = {Peter Henry Emerson},
	keywords = {image, img, data, dataset, animals, sheep, pastoral, countryside, england, uk, united kingdom, britain, english, british},
	note = {Peter Henry Emerson's photogravure of sheep in a pastoral setting. Sourced from Getty's Open Content Program.},
	title = {{A March Pastoral}},
	url = {http://www.getty.edu/art/collection/objects/141994/peter-henry-emerson-a-march-pastoral-suffolk-british-1888/},
	year = {1888}
}

@misc{beato:1865a,
	author = {Felice Beato},
	keywords = {image, img, data, dataset, boats, boat, river, japan, nagasaki},
	note = {Felice Beato's albumen silver print of boats in a river in Nagasaki. Sourced from Getty's Open Content Program.},
	title = {{(no title)}},
	url = {http://www.getty.edu/art/collection/objects/241797/felice-beato-boats-in-river-nagasaki-british-about-1865/},
	year = {1865}
}

@inproceedings{hu:2004a,
	abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
	acmid = {1014073},
	address = {New York, NY, USA},
	author = {Minqing Hu and Bing Liu},
	booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/1014052.1014073},
	isbn = {1-58113-888-1},
	keywords = {reviews, sentiment classification, summarization, text mining},
	location = {Seattle, WA, USA},
	numpages = {10},
 	pages = {168--177},
	publisher = {ACM},
	series = {KDD '04},
	title = {{Mining and Summarizing Customer Reviews}},
	url = {http://doi.acm.org/10.1145/1014052.1014073},
	year = {2004},
}

@inproceedings{liu:2005a,
	abstract = {The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.},
	acmid = {1060797},
	address = {New York, NY, USA},
	author = {Bing Liu and Minqing Hu and Junsheng Cheng},
	booktitle = {Proceedings of the 14th International Conference on World Wide Web},
	doi = {10.1145/1060745.1060797},
	isbn = {1-59593-046-9},
	keywords = {information extraction, opinion analysis, sentiment analysis, visualization},
	location = {Chiba, Japan},
	numpages = {10},
 	pages = {342--351},
	publisher = {ACM},
	series = {WWW '05},
	title = {{Opinion Observer: Analyzing and Comparing Opinions on the Web}},
	url = {http://doi.acm.org/10.1145/1060745.1060797},
	year = {2005},
}

@inbook{liu:2010a,
	author = {Bing Liu},
	booktitle = {Handbook of Natural Language Processing},
	edition = {2nd},
	editor = {Nitin Indurkhya and Fred J. Damerau},
	isbn = {9781420085921},
	keywords = {natural language processing, nlp, sentiment, subjectivity, text mining, opinion analysis, information extraction},
	pages = {627--66},
	publisher = {Chapman \& Hall/CRC},
	title = {{Sentiment Analysis and Subjectivity}},
	url = {https://www.crcpress.com/Handbook-of-Natural-Language-Processing-Second-Edition/Indurkhya-Damerau/p/book/9781420085921},
	year = {2010},
}

@book{minard:1869a,
	author = {Charles Joseph Minard},
	keywords = {cartography, napoleon, visualization, dataviz, graphics, charts, maps},
	publisher = {Ecole nationale des ponts et chauss{\'{e}}es},
	title = {{Tableaux graphiques et cartes figuratives}},
	url = {http://patrimoine.enpc.fr/document/ENPC01_Fol_10975?image=54#bibnum},
	year = {1869},
}

@book{wilkinson:2005a,
	author = {Leland Wilkinson},
	doi = {10.1007/0-387-28695-0},
	isbn = {978-0-387-24544-7},
	keywords = {visualization, dataviz, graphics, charts, information},
	publisher = {Springer-Verlag New York},
	title = {{The Grammar of Graphics}},
	url = {http://www.springer.com/gp/book/9780387245447},
	year = {2005},
}

@book{nightingale:1859a,
	address = {London, United Kingdom},
	author = {Florence Nightingale},
	keywords = {visualization, dataviz, graphics, charts, war, infographics},
	publisher = {{John W. Parker and Son}},
	title = {{A contribution to the sanitary history of the British army during the late war with Russia}},
	url = {https://curiosity.lib.harvard.edu/contagion/catalog/36-990101646750203941},
	year = {1859},
}

@misc{savoy:2005a,
	author = {Jacques Savoy},
	keywords = {text mining, nlp, natural language processing, language, lang, stop words},
	title = {{IR Multilingual Resources at UniNE}},
	url = {http://members.unine.ch/jacques.savoy/clef/},
	year = {2005}
}

@misc{mason:spamassassin,
	author = {Justin Mason},
	keywords = {text mining, nlp, natural language processing, language, lang, email, e-mail, mail, corpus, spam, ham, classification},
	title = {{Spam Assassin Public Mail Corpus}},
	url = {http://spamassassin.apache.org/old/publiccorpus/readme.html},
	year = {2006}
}

@article{gould:1965a,
	abstract = {},
	author = {H.W. Gould},
	doi = {},
	journal = {Fibonacci Quarterly},
	keywords = {fibonacci, fibo, integer, sequences, number theory},
	month = {},
	number = {3},
	pages = {177--183},
	title = {{Non-Fibonacci Numbers}},
	url = {http://www.fq.math.ca/Scanned/3-3/gould.pdf},
	volume = {},
	year = {1965},
}

@article{farhi:2011a,
	abstract = {We show among others that the formula: \\( \left \lfloor{ n + \log_\varphi \biggl( \sqrt{5}(\log_\varphi (n\sqrt{5}) + n) − 5 + \tfrac{3}{n} \biggr) − 2} \right \rfloor,\ n \geq 2 \\), (where \\(\varphi\\) denotes the golden ratio and \\( \left \lfloor \right \rfloor \\) denotes the integer part) generates the non-Fibonacci numbers.},
	author = {Bakir Farhi},
	doi = {},
	journal = {{arXiv}},
	keywords = {fibonacci, fibo, integer, sequences, number theory},
	month = {may},
	number = {},
	pages = {1--5},
	title = {{An explicit formula generating the non-Fibonacci numbers}},
	url = {https://arxiv.org/abs/1105.1127},
	volume = {abs/1105.1127 [math.NT]},
	year = {2011},
}

@mastersthesis{holloway:1988a,
	abstract = {A study of the running time of several known algorithms and several new algorithms to compute the nth element of the Fibonacci sequence is presented. Since the size of the \\( n^{th} \\) Fibonacci number grows exponentially with \\( n \\), the number of bit operations, instead of the number of integer operations, was used as the unit of time. The number of bit operations used to compute \\( f_n \\) is reduced to less than \\( \tfrac{1}{2} \\) of the number of bit operations used to multiply two \\( n \\) bit numbers. The algorithms were programmed in Ibuki Common Lisp and timing runs were made on a Sequent Balance 21000. Multiplication was implemented using the standard \\( n^2 \\) algorithm. Times for the various algorithms are reported as various constants times \\( n^2 \\). An algorithm based on generating factors of Fibonacci numbers had the smallest constant. The Fibonacci sequence, arranged in various ways, is searched for redundant information that could be eliminated to reduce the number of operations. Cycles in the \\( b^{th} \\) bit of \\( f_n \\) were discovered but are not yet completely understood.},
	author = {James L. Holloway},
	keywords = {fibonacci, fibo, fib, sequence, algorithm, algo},
	school = {Oregon State University},
	title = {{Algorithms for Computing Fibonacci Numbers Quickly}},
	url = {http://ir.library.oregonstate.edu/xmlui/bitstream/handle/1957/39942/HollowayJamesL1989.pdf},
	year = {1988},
}

@article{baudin:2012a,
	abstract = {The most widely used algorithm for floating point complex division, known as Smith's method, may fail more often than expected. This document presents two improved complex division algorithms. We present a proof of the robustness of the first improved algorithm. Numerical simulations show that this algorithm performs well in practice and is significantly more robust than other known implementations. By combining additional scaling methods with this first algorithm, we were able to create a second algorithm, which rarely fails.},
	author = {Michael Baudin and Robert L. Smith},
	doi = {},
	journal = {{arXiv}},
	keywords = {floating-point, arithmetic, complex, division, algorithm, mathematics, math, numeric computing, julia, python, complex numbers},
	month = {oct},
	number = {},
	pages = {1--25},
	title = {{A Robust Complex Division in Scilab}},
	url = {https://arxiv.org/abs/1210.4539},
	volume = {abs/1210.4539 [cs.MS]},
	year = {2012},
}

@article{priest:2004a,
	abstract = {We develop a simple method for scaling to avoid overflow and harmful underflow in complex division. The method guarantees that no overflow will occur unless at least one component of the quotient must overflow, otherwise the normwise error in the computed result is at most a few units in the last place. Moreover, the scaling requires only four floating point multiplications and a small amount of integer arithmetic to compute the scale factor. Thus, on many modern CPUs, our method is both safer and faster than Smith's widely used algorithm.},
	acmid = {1039814},
	address = {New York, NY, USA},
	author = {Douglas M. Priest},
	doi = {10.1145/1039813.1039814},
	issn = {0098-3500},
	journal = {ACM Trans. Math. Softw.},
	keywords = {complex, division, overflow, underflow, floating-point, arithmetic, algorithm, mathematics, math},
	month = {dec},
	number = {4},
	numpages = {13},
	pages = {389--401},
	publisher = {ACM},
	title = {{Efficient Scaling for Complex Division}},
	url = {http://doi.acm.org/10.1145/1039813.1039814},
	volume = {30},
	year = {2004},
}

@phdthesis{pugh:2004a,
	abstract = {This thesis is an analysis of C . Lanczos' approximation of the classical gamma function Γ(z + 1) as given in his 1964 paper "A Precision Approximation of the Gamma Function". The purposes of this study are: (i) to explain the details of Lanczos' paper, including proof's of all claims made by the author; (ii) to address the question of how best to implement the approximation method in practice; and (iii) to generalize the methods used in the derivation of the approximation.},
	author = {Glendon R. Pugh},
	keywords = {gamma, lanczos, approximation, math, mathematics},
	school = {University of British Columbia},
	title = {{An analysis of the Lanczos gamma approximation}},
	url = {https://web.viu.ca/pughg/phdThesis/phdThesis.pdf},
	year = {2004}
}

@article{smith:1962a,
	abstract = {},
	acmid = {368661},
	address = {New York, NY, USA},
	author = {Robert L. Smith},
	doi = {10.1145/368637.368661},
	issn = {0001-0782},
	journal = {Commun. ACM},
	keywords = {complex, division, math, mathematics, algorithm, arithmetic},
	month = {aug},
	number = {8},
	pages = {435--},
	publisher = {ACM},
	title = {{Algorithm 116: Complex Division}},
	url = {http://doi.acm.org/10.1145/368637.368661},
	volume = {5},
	year = {1962},
}

@article{stewart:1985a,
	abstract = {An algorithm for computing the quotient of two complex numbers is modified to make it more robust in the presence of underflows.},
	address = {New York, NY, USA},
	acmid = {214414},
	author = {G. W. Stewart},
	doi = {10.1145/214408.214414},
	issn = {0098-3500},
	journal = {ACM Trans. Math. Softw.},
	keywords = {complex, division, computer arithmetic, exponent exception, algorithm, mathematics, math},
	month = {sep},
	number = {3},
	numpages = {4},
	pages = {238--241},
	publisher = {ACM},
	title = {{A Note on Complex Division}},
	url = {http://doi.acm.org/10.1145/214408.214414},
	volume = {11},
	year = {1985},
}

@article{fousse:2007a,
	abstract = {This article presents a multiple-precision binary floating-point library, written in the ISO C language, and based on the GNU MP library. Its particularity is to extend to arbitrary-precision, ideas from the IEEE 754 standard, by providing correct rounding and exceptions. We demonstrate how these strong semantics are achieved---with no significant slowdown with respect to other arbitrary-precision tools---and discuss a few applications where such a library can be useful.},
	acmid = {1236468},
	address = {New York, NY, USA},
	articleno = {13},
	author = {Laurent Fousse and Guillaume Hanrot and Vincent Lef\`{e}vre and Patrick P{\'e}lissier and Paul Zimmermann},
	doi = {10.1145/1236463.1236468},
	issn = {0098-3500},
	issue_date = {June 2007},
	journal = {ACM Trans. Math. Softw.},
	keywords = {ieee 754, multiple-precision arithmetic, correct rounding, elementary function, floating-point arithmetic, portable software},
	month = {jun},
	number = {2},
	publisher = {ACM},
	title = {{MPFR: A Multiple-precision Binary Floating-point Library with Correct Rounding}},
	url = {http://doi.acm.org/10.1145/1236463.1236468},
	volume = {33},
	year = {2007},
}

@article{anscombe:1973a,
	abstract = {},
	author = {Francis J. Anscombe},
	ISSN = {00031305},
	journal = {{The American Statistician}},
	keywords = {anscombe, anscombe's quartet, quartet, statistics, stats, exploratory data analysis, eda, data visualization, dataviz, datavis},
	number = {1},
	pages = {17--21},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Graphs in Statistical Analysis},
	URL = {http://www.jstor.org/stable/2682899},
	volume = {27},
	year = {1973}
}

@article{kjolstad:2017a,
	abstract = {Tensor algebra is a powerful tool with applications in machine learning, data analytics, engineering and the physical sciences. Tensors are often sparse and compound operations must frequently be computed in a single kernel for performance and to save memory. Programmers are left to write kernels for every operation of interest, with different mixes of dense and sparse tensors in different formats. The combinations are infinite, which makes it impossible to manually implement and optimize them all. This paper introduces the first compiler technique to automatically generate kernels for any compound tensor algebra operation on dense and sparse tensors. The technique is implemented in a C++ library called taco. Its performance is competitive with best-in-class hand-optimized kernels in popular libraries, while supporting far more tensor operations.},
	acmid = {3133901},
	address = {New York, NY, USA},
	articleno = {77},
	author = {Fredrik Kjolstad and Shoaib Kamil and Stephen Chou and David Lugato and Saman Amarasinghe},
	doi = {10.1145/3133901},
	issn = {2475-1421},
	issue_date = {October 2017},
	journal = {Proc. ACM Program. Lang.},
	keywords = {code generation, iteration graphs, linear algebra, merge lattices, parallelism, performance, sparse data structures, tensor algebra, tensors},
	number = {OOPSLA},
	month = {oct},
	numpages = {29},
	pages = {77:1--77:29},
	publisher = {ACM},
	title = {{The Tensor Algebra Compiler}},
	url = {http://doi.acm.org/10.1145/3133901},
	volume = {1},
	year = {2017},
}

@article{dekker:1971a,
	abstract = {A technique is described for expressing multilength floating-point arithmetic in terms of singlelength floating point arithmetic, i.e. the arithmetic for an available (say: single or double precision) floating-point number system. The basic algorithms are exact addition and multiplication of two singlelength floating-point numbers, delivering the result as a doublelength floating-point number. A straight-forward application of the technique yields a set of algorithms for doublelength arithmetic which are given as ALGOL 60 procedures.},
	author = {T. J. Dekker},
	day = {01},
	doi = {10.1007/BF01397083},
	issn = {0945-3245},
	journal = {Numerische Mathematik},
	keywords = {floating-point, precision, numerical, numeric computing, algorithms, arithmetic},
	month = {Jun},
	number = {3},
	pages = {224--242},
	title = {{A floating-point technique for extending the available precision}},
	url = {https://doi.org/10.1007/BF01397083},
	volume = {18},
	year = {1971},
}

@inproceedings{caliskan-islam:2015a,
	abstract = {Source code authorship attribution is a significant privacy threat to anonymous code contributors. However, it may also enable attribution of successful attacks from code left behind on an infected system, or aid in resolving copyright, copyleft, and plagiarism issues in the programming fields. In this work, we investigate machine learning methods to de-anonymize source code authors of C/C++ using coding style. Our Code Stylometry Feature Set is a novel representation of coding style found in source code that reflects coding style from properties derived from abstract syntax trees. Our random forest and abstract syntax tree-based approach attributes more authors (1,600 and 250) with significantly higher accuracy (94% and 98%) on a larger data set (Google Code Jam) than has been previously achieved. Furthermore, these novel features are robust, difficult to obfuscate, and can be used in other programming languages, such as Python. We also find that (i) the code resulting from difficult programming tasks is easier to attribute than easier tasks and (ii) skilled programmers (who can complete the more difficult tasks) are easier to attribute than less skilled programmers.},
 	acmid = {2831160},
 	address = {Berkeley, CA, USA},
	author = {Aylin Caliskan-Islam and Richard Harang and Andrew Liu and Arvind Narayanan and Clare Voss and Fabian Yamaguchi and Rachel Greenstadt},
	booktitle = {Proceedings of the 24th USENIX Conference on Security Symposium},
	isbn = {978-1-931971-232},
 	keywords = {code style, stylometry, static analysis, javascript, automation},
	location = {Washington, D.C.},
 	numpages = {16},
 	pages = {255--270},
 	publisher = {USENIX Association},
	series = {SEC'15},
 	title = {{De-anonymizing Programmers via Code Stylometry}},
	url = {http://dl.acm.org/citation.cfm?id=2831143.2831160},
 	year = {2015},
}

@inproceedings{wittern:2016a,
	abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node. js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
	acmid = {2901743},
	address = {New York, NY, USA},
	author = {Erik Wittern and Philippe Suter and Shiram Rajagopalan},
	booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
	doi = {10.1145/2901739.2901743},
	isbn = {978-1-4503-4186-8},
	keywords = {javascript, nodejs, node.js, node-js, software ecosystem analysis, npm, package management, package},
	location = {Austin, Texas},
	numpages = {11},
	pages = {351--361},
	publisher = {ACM},
	series = {MSR '16},
	title = {{A Look at the Dynamics of the JavaScript Package Ecosystem}},
	url = {http://doi.acm.org/10.1145/2901739.2901743},
	year = {2016},
}

@inproceedings{abdalkareem:2017a,
	abstract = {Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call `trivial packages'. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages. Therefore, in this paper, we mine more than 230,000 npm packages and 38,000 JavaScript applications in order to study the prevalence of trivial packages. We found that trivial packages are common and are increasing in popularity, making up 16.8% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons and drawbacks of their use. Our survey revealed that trivial packages are used because they are perceived to be well implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce. To objectively verify the survey results, we empirically validate the most cited reason and drawback and find that, contrary to developers' beliefs, only 45.2% of trivial packages even have tests. However, trivial packages appear to be `deployment tested' and to have similar test, usage and community interest as non-trivial packages. On the other hand, we found that 11.5% of the studied trivial packages have more than 20 dependencies. Hence, developers should be careful about which trivial packages they decide to use.},
	address = {New York, NY, USA},
	acmid = {3106267},
	author = {Rabe Abdalkareem and Olivier Nourry and Sultan Wehaibi and Suhaib Mujahid and Emad Shihab},
	booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
	doi = {10.1145/3106237.3106267},
	isbn = {978-1-4503-5105-8},
	keywords = {javascript, node.js, nodejs, npm, software development, dependencies, deps, dependency management, code quality},
	location = {Paderborn, Germany},
	numpages = {11},
	pages = {385--395},
	publisher = {ACM},
	series = {ESEC/FSE 2017},
	title = {{Why Do Developers Use Trivial Packages? An Empirical Case Study on Npm}},
	url = {http://doi.acm.org/10.1145/3106237.3106267},
	year = {2017},
}

@techreport{park:1992a,
	abstract = {This report presents guidelines for defining, recording, and reporting two frequently used measures of software size—physical source lines and logical source statements. We propose a general framework for constructing size definitions and use it to derive operational methods for reducing misunderstandings in measurement results. We show how the methods can be applied to address the information needs of different users while maintaining a common definition of software size.},
	author = {Robert E. Park},
	institution = {Software Engineering Institute, Carnegie Mellon University},
	keywords = {static analysis, software development, cocomo, sloc, lloc},
	title = {{Software Size Measurement: A Framework for Counting Source Statements}},
	url = {https://resources.sei.cmu.edu/asset_files/TechnicalReport/1992_005_001_16082.pdf},
	year = {1992},
}

@techreport{nguyen:2007a,
	abstract = {Source Lines of Code (SLOC or LOC) is one of the most widely used sizing metrics in industry and literature. It is the key input for most of major cost estimation models such as COCOMO, SLIM, and SEER-SEM. Although the SEI and the IEEE have established SLOC definitions and guidelines to standardize counting practice, inconsistency in SLOC measurements still exists in industry and research. This problem causes the incomparability of SLOC metric among organizations and the inaccuracy of cost estimation. This report presents a set of counting standards that defines what and how to count SLOC. Our experience with the development and use of the USC CodeCount™ toolset, a popular utility that automates the SLOC counting process, suggests that this problem can be alleviated by the use of a reasonable and unambiguous counting standard guide and with the support of a configurable counting tool. },
	author = {Vu Nguyen and Sophia Deeds-Rubin and Thomas Tan and Barry W. Boehm},
	institution = {Center for Systems and Software Engineering, University of Southern California},
	keywords = {static analysis, software development, cocomo, sloc, lloc},
	title = {{A SLOC Counting Standard}},
	url = {http://csse.usc.edu/TECHRPTS/2007/usc-csse-2007-737/usc-csse-2007-737.pdf},
	year = {2007},
}

@mastersthesis{ourada:1991a,
	abstract = {This study was a calibration, validation and comparison of four software effort estimation models. The four models evaluated were REVIC, SASET, SEER, and COSTMODL. A historical database was obtained from Space Systems Division, in Los Angeles, and used as the input data. Two software environments were selected, one used to calibrate and validate the models, and the other to show the performance of the models outside their environment of calibration. REVIC and COSTMODL are COCOMO derivatives and were calibrated using Dr. Boehm's, procedure. SASET and SEER were found to be uncalibratable for this effort. Accuracy of all the models was significantly low; none of the models performed as expected. REVIC and COSTMODL actually performed better against the comparison data than the data from the calibration. SASET and SEER were very inconsistent across both environments.},
	author = {Gerald L. Ourada},
	keywords = {cocomo, software estimation, man-months, sloc, lloc},
	school = {Air Force Institute of Technology},
	title = {{Software Cost Estimating Models: A Calibration, Validation, and Comparison}},
	url = {http://www.dtic.mil/dtic/tr/fulltext/u2/a246677.pdf},
	year = {1991},
}

@article{kemerer:1987a,
	abstract = {Practitioners have expressed concern over their inability to accurately estimate costs associated with software development. This concern has become even more pressing as costs associated with development continue to increase. As a result, considerable research attention is now directed at gaining a better understanding of the software-development process as well as constructing and evaluating software cost estimating tools. This paper evaluates four of the most popular algorithmic models used to estimate software costs (SLIM, COCOMO, Function Points, and ESTIMACS). Data on 15 large completed business data-processing projects were collected and used to test the accuracy of the models' ex post effort estimation. One important result was that Albrecht's Function Points effort estimation model was validated by the independent data provided in this study [3]. The models not developed in business data-processing environments showed significant need for calibration. As models of the software-development process, all of the models tested failed to sufficiently reflect the underlying factors affecting productivity. Further research will be required to develop understanding in this area.},
	acmid = {22906},
 	address = {New York, NY, USA},
 	author = {Chris F. Kemerer},
	doi = {10.1145/22899.22906},
	issn = {0001-0782},
	issue_date = {May 1987},
	journal = {Communications of the ACM},
	keywords = {cocomo, software estimation, sloc, lloc, development},
	month = {may},
	number = {5},
	numpages = {14},
	pages = {416--429},
 	publisher = {ACM},
	title = {{An Empirical Validation of Software Cost Estimation Models}},
	url = {http://doi.acm.org/10.1145/22899.22906},
	volume = {30},
	year = {1987},
}

@article{cleveland:1981,
	abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, \\( (x_i, y_i), i = 1, \ldots, n \\), in which the fitted value at \\( x_k \\) is the value of a polynomial fit to the data using weighted least squares, where the weight for  \\( (x_i, y_i) \\) is large if \\( x_i \\) is close to \\( x_k \\) and small if it is not.},
	author = {William S. Cleveland},
	doi = {10.2307/2683591},
	isbn = {00031305},
	issn = {15372731},
	journal = {American Statistician},
	keywords = {regression,stats,statistics,graphics,scatterplots},
	number = {1},
	pages = {54--55},
	pmid = {24495673},
	title = {{Lowess: A program for smoothing scatterplots by robust locally weighted regression}},
	volume = {35},
	year = {1981}
}

@article{cleveland:1979,
	abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, \\( (x_i, y_i), i = 1, \ldots, n \\), in which the fitted value at \\( x_k \\) is the value of a polynomial fit to the data using weighted least squares, where the weight for  \\( (x_i, y_i) \\) is large if \\( x_i \\) is close to \\( x_k \\) and small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology.},
	author = {William S. Cleveland},
	doi = {10.1080/01621459.1979.10481038},
	isbn = {01621459},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {regression,stats,statistics,graphics,scatterplots},
	number = {368},
	pages = {829--836},
	title = {{Robust Locally and Smoothing Weighted Regression Scatterplots}},
	volume = {74},
	year = {1979}
}

@article{alakkari:2014a,
	abstract = {In this paper, we present an online adaptive PCA algorithm that is able to compute the full dimensional eigenspace per new time-step of sequential data. The algorithm is based on a one-step update rule that considers all second order correlations between previous samples and the new time-step. Our algorithm has O(n) complexity per new time-step in its deterministic mode and O(1) complexity per new time-step in its stochastic mode. We test our algorithm on a number of time-varying datasets of different physical phenomena. Explained variance curves indicate that our technique provides an excellent approximation to the original eigenspace computed using standard PCA in batch mode. In addition, our experiments show that the stochastic mode, despite its much lower computational complexity, converges to the same eigenspace computed using the deterministic mode.},
	author = {Salaheddin Alakkari and John Dingliana},
	doi = {},
	journal = {arXiv},
	keywords = {pca, incremental, on-line, online, statistics, stats, principal component analysis},
	month = {sep},
	number = {},
	pages = {},
	title = {{Adaptive PCA for Time-Varying Data}},
	url = {https://arxiv.org/abs/1709.02373},
	volume = {abs/1709.02373},
	year = {2017},
}

@article{chan:1983a,
	abstract = {The problem of computing the variance of a sample of \\(N\\) data points \\({x_i}\\) may be difficult for certain data sets, particularly when \\(N\\) is large and the variance is small. We present a survey of possible algorithms and their round-off error bounds, including some new analysis for computations with shifted data. Experimental results confirm these bounds and illustrate the dangers of some algorithms. Specific recommendations are made as to which algorithm should be used in various contexts.},
	author = {Tony F. Chan and Gene H. Golub and Randall J. LeVeque},
	doi = {10.1080/00031305.1983.10483115},
	issn = {00031305},
	journal = {The American Statistician},
	keywords = {statistics, stats, algorithms, variance, standard deviation},
	month = {aug},
	number = {3},
	pages = {242-247},
	publisher = {American Statistical Association, Taylor & Francis, Ltd.},
	title = {{Algorithms for Computing the Sample Variance: Analysis and Recommendations}},
	url = {http://www.jstor.org/stable/2683386},
	volume = {37},
	year = {1983},
}

@article{ling:1974a,
	abstract = {Several one-pass and two-pass algorithms for the computation of sample means and variances are compared by their performance on sets of randomly generated data and systematically generated data with random noise. The relation between the performance of each algorithm and the coefficient of variation of the population from which random data sets are generated is explored.},
	author = {Robert F. Ling},
	doi = {10.2307/2286154},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {statistics, stats, algorithm, algo, mean, variance, average, incremental, on-line, online},
	month = {dec},
	number = {348},
	pages = {859-866},
	publisher = {American Statistical Association, Taylor & Francis, Ltd.},
	title = {{Comparison of Several Algorithms for Computing Sample Means and Variances}},
	url = {http://www.jstor.org/stable/2286154},
	volume = {69},
	year = {1974},
}

@inproceedings{seljebotn:2009a,
	abstract = {Cython has recently gained popularity as a tool for conveniently performing numerical computations in the Python environment, as well as mixing efficient calls to natively compiled libraries with Python code. We discuss Cython's features for fast NumPy array access in detail through examples and benchmarks. Using Cython to call natively compiled scientific libraries as well as using Cython in parallel computations is also given consideration. We conclude with a note on possible directions for future Cython development.},
	address = {Pasadena, CA, USA},
	author = {Dag Sverre Seljebotn},
	booktitle = {Proceedings of the 8th Python in Science Conference},
	doi = {},
	editor = {Ga\"el Varoquaux and St\'efan van der Walt and Jarrod Millman},
	keywords = {cython, python, numpy, scientific computing, strided arrays, ndarray, performance, algorithms},
	numpages = {8},
	pages = {15--22},
	title = {{Fast numerical computations with Cython}},
	url = {http://conference.scipy.org/proceedings/SciPy2009/paper_2},
	year = {2009},
}

@article{goualard:2014a,
	abstract = {The algorithm that computes the midpoint of an interval with floating-point bounds requires some careful devising to handle all possible inputs correctly. We review several implementations from prominent C/C++ interval arithmetic packages and analyze their potential failure to deliver the expected results. We then show how to amend them to avoid common pitfalls. The results presented are also relevant to noninterval arithmetic computation such as the implementation of bisection methods. Enough background on IEEE 754 floating-point arithmetic is provided for this article to serve as a practical introduction to the analysis of floating-point computation.},
	acmid = {2493882},
	address = {New York, NY, USA},
	articleno = {11},
	author = {Goualard, Fr{\'e}d{\'e}ric},
	doi = {10.1145/2493882},
	journal = {ACM Transactions on Mathematical Software},
	issn = {0098-3500},
	issue_date = {February 2014},
	keywords = {floating-point, IEEE 754, interval arithmetic, midpoint, rounding error, algorithms, algo},
	month = {mar},
	number = {2},
	numpages = {25},
	pages = {11:1--11:25},
	publisher = {ACM},
	title = {{How Do You Compute the Midpoint of an Interval?}},
	url = {http://doi.acm.org/10.1145/2493882},
	volume = {40},
	year = {2014},
}

@inproceedings{kahan:1987a,
	abstract = {Zero has a usable sign bit on some computers, but not on others. This accident of computer arithmetic influences the definition and use of familiar complex elementary functions like square root, arctan and arccosh whose domains are the whole complex plane with a slit or two drawn in it. The Principal Values of those functions are defined in terms of the logarithm function from which they inherit discontinuities across the slit(s). These discontinuities are crucial for applications to conformal maps with corners. The behaviour of those functions on their slits can be read off immediately from defining Principal Expressions introduced in this paper for use by analysts. Also introduced herein are programs that implement the functions fairly accurately despite roundoff and other numerical exigencies. Except at logarithmic branch points, those functions can all be continuous up to and onto their boundary slits when zero has a sign that behaves as specified by IEEE standards for floating-point arithmetic; but those functions must be discontinuous on one side of each slit when zero is unsigned. Thus does the sign of zero lay down a trail from computer hardware through programming language compilers, run-time support libraries and applications programmers to, finally, mathematical analysts.},
	address = {Oxford, England, UK},
	author = {William M Kahan},
	booktitle = {The State of the Art in Numerical Analysis: Proceedings of the 3rd Joint IMA/SIAM Conference},
	doi = {},
	editor = {Arieh Iserles and Michael James David Powell},
	isbn = {9780198536147},
	keywords = {ieee754, floating-point, branch cuts, signed zeros, numerical analysis, mathematics},
	location = {Cambridge, England, UK},
	numpages = {46},
	pages = {165--211},
	publisher = {Institute of Mathematics and Its Applications Conference Series, Oxford University Press},
	title = {{Branch cuts for complex elementary functions, or much ado about nothing's sign bit}},
	url = {https://people.freebsd.org/~das/kahan86branch.pdf},
	volume = {9},
	year = {1987},
}

@article{kim:2016a,
	abstract = {The mean absolute percentage error (MAPE) is one of the most widely used measures of forecast accuracy, due to its advantages of scale-independency and interpretability. However, MAPE has the significant disadvantage that it produces infinite or undefined values for zero or close-to-zero actual values. In order to address this issue in MAPE, we propose a new measure of forecast accuracy called the mean arctangent absolute percentage error (MAAPE). MAAPE has been developed through looking at MAPE from a different angle. In essence, MAAPE is a slope as an angle, while MAPE is a slope as a ratio, considering a triangle with adjacent and opposite sides that are equal to an actual value and the difference between the actual and forecast values, respectively. MAAPE inherently preserves the philosophy of MAPE, overcoming the problem of division by zero by using bounded influences for outliers in a fundamental manner through considering the ratio as an angle instead of a slope. The theoretical properties of MAAPE are investigated, and the practical advantages are demonstrated using both simulated and real-life data.},
	author = {Sungil Kim and Heeyoung Kim},
	doi = {10.1016/j.ijforecast.2015.12.003},
	issn = {0169-2070},
	journal = {International Journal of Forecasting},
	keywords = {accuracy measure, forecast evaluation, intermittent demand, mape, statistics, stats, accuracy, forecasting, modeling},
	number = {3},
	pages = {669--679},
	title = {{A new metric of absolute percentage error for intermittent demand forecasts}},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207016000121},
	volume = {32},
	year = {2016},
}

@article{robbins:1968a,
	abstract = {},
	author = {H. Robbins and M. Sobel and N. Starr},
	doi = {10.2307/2238912},
	issn = {00034851},
	journal = {The Annals of Mathematical Statistics},
	keywords = {},
	month = {},
	number = {1},
	pages = {88--92},
	title = {{A Sequential Procedure for Selecting the Largest of k Means}},
	url = {http://www.jstor.org/stable/2238912},
	volume = {39},
	year = {1968},
}

@inproceedings{liberty:2016a,
	abstract = {This paper shows that one can be competitive with the k-means objective while operating online. In this model, the algorithm receives vectors v1, ..., vn one by one in an arbitrary order. For each vector vt the algorithm outputs a cluster identifier before receiving vt+1. Our online algorithm generates O(k log n log γn) clusters whose expected k-means cost is O(W* log n). Here, W* is the optimal k-means cost using k clusters and γ is the aspect ratio of the data. The dependence on γ is shown to be unavoidable and tight. We also show that, experimentally, it is not much worse than k-means++ while operating in a strictly more constrained computational model.},
	author = {Edo Liberty and Ram Sriharsha and Maxim Sviridenko},
	booktitle = {2016 Proceedings of the Eighteenth Workshop on Algorithm Engineering and Experiments (ALENEX)},
	doi = {10.1137/1.9781611974317.7},
	keywords = {machine learning, ml, k-means, kmeans, clustering, streaming, online},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{An Algorithm for Online K-Means Clustering}},
	url = {https://doi.org/10.1137/1.9781611974317.7},
	year = {2016},
}

@article{bahmani:2012a,
	abstract = {Over half a century old and showing no signs of aging, k-means remains one of the most popular data processing algorithms. As is well-known, a proper initialization of k-means is crucial for obtaining a good final solution. The recently proposed k-means++ initialization algorithm achieves this, obtaining an initial set of centers that is provably close to the optimum solution. A major downside of the k-means++ is its inherent sequential nature, which limits its applicability to massive data: one must make k passes over the data to find a good initial set of centers. In this work we show how to drastically reduce the number of passes needed to obtain, in parallel, a good initialization. This is unlike prevailing efforts on parallelizing k-means that have mostly focused on the post-initialization phases of k-means. We prove that our proposed initialization algorithm k-means|| obtains a nearly optimal solution after a logarithmic number of passes, and then show that in practice a constant number of passes suffices. Experimental evaluation on real-world large-scale data demonstrates that k-means|| outperforms k-means++ in both sequential and parallel settings.},
	acmid = {2180915},
	author = {Bahman Bahmani and Benjamin Moseley and Andrea Vattani and Ravi Kumar and Sergei Vassilvitskii},
	doi = {10.14778/2180912.2180915},
	issn = {2150-8097},
	issue_date = {March 2012},
	journal = {Proceedings of the VLDB Endowment},
	keywords = {machine learning, ml, k-means, kmeans, clustering},
	month = {mar},
	number = {7},
	numpages = {12},
	pages = {622--633},
	publisher = {VLDB Endowment},
	title = {{Scalable K-means++}},
	year = {2012},
	url = {http://dx.doi.org/10.14778/2180912.2180915},
	volume = {5},
}

@inproceedings{pelleg:2000a,
	abstract = {Despite its popularity for general clustering, K-means suffers three major shortcomings; it scales poorly computationally, the number of clusters K has to be supplied by the user, and the search is prone to local minima. We propose solutions for the first two problems, and a partial remedy for the third. Building on prior work for algorithmic acceleration that is not based on approximation, we introduce a new algorithm that efficiently, searches the space of cluster locations and number of clusters to optimize the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC) measure. The innovations include two new ways of exploiting cached sufficient statistics and a new very efficient test that in one K-means sweep selects the most promising subset of classes for refinement. This gives rise to a fast, statistically founded algorithm that outputs both the number of classes and their parameters. Experiments show this technique reveals the true number of classes in the underlying distribution, and that it is much faster than repeatedly using accelerated K-means for different values of K.},
	acmid = {657808},
	address = {San Francisco, CA, USA},
	author = {Dan Pelleg and Andrew W. Moore},
	booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	isbn = {1-55860-707-2},
	keywords = {machine learning, ml, kmeans, k-means, xmeans, x-means, clustering},
	numpages = {8},
	pages = {727--734},
	publisher = {Morgan Kaufmann Publishers Inc.},
	series = {ICML '00},
	title = {{X-means: Extending K-means with Efficient Estimation of the Number of Clusters}},
	url = {http://dl.acm.org/citation.cfm?id=645529.657808},
	year = {2000},
}

@inproceedings{arthur:2007a,
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	acmid = {1283494},
	address = {Philadelphia, PA, USA},
	author = {David Arthur and Sergei Vassilvitskii},
	booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
	isbn = {978-0-898716-24-5},
	keywords = {kmeans, k-means, kmeans++, machine learning, ml, clustering},
	location = {New Orleans, Louisiana},
	numpages = {9},
	pages = {1027--1035},
	publisher = {{Society for Industrial and Applied Mathematics}},
	series = {SODA '07},
	title = {{K-means++: The Advantages of Careful Seeding}},
	url = {http://dl.acm.org/citation.cfm?id=1283383.1283494},
	year = {2007},
}

@article{celebi:2013a,
	abstract = {K-means is undoubtedly the most widely used partitional clustering algorithm. Unfortunately, due to its gradient descent nature, this algorithm is highly sensitive to the initial placement of the cluster centers. Numerous initialization methods have been proposed to address this problem. In this paper, we first present an overview of these methods with an emphasis on their computational efficiency. We then compare eight commonly used linear time complexity initialization methods on a large and diverse collection of data sets using various performance criteria. Finally, we analyze the experimental results using non-parametric statistical tests and provide recommendations for practitioners. We demonstrate that popular initialization methods often perform poorly and that there are in fact strong alternatives to these methods.},
	acmid = {2369887},
	address = {Tarrytown, NY, USA},
	author = {M. Emre Celebi and Hassan A. Kingravi and Patricio A.Vela},
	doi = {10.1016/j.eswa.2012.07.021},
	issn = {0957-4174},
	issue_date = {January, 2013},
	journal = {Expert Systems with Applications: An International Journal},
	keywords = {cluster center initialization, partitional clustering, sum of squared error criterion, k-means, machine learning, ml, kmeans, clustering},
	month = {jan},
	number = {1},
	numpages = {11},
	pages = {200--210},
	publisher = {Pergamon Press, Inc.},
	title = {{A Comparative Study of Efficient Initialization Methods for the K-means Clustering Algorithm}},
	url = {http://dx.doi.org/10.1016/j.eswa.2012.07.021},
	volume = {40},
	year = {2013},
}

@inproceedings{su:2004a,
	abstract = {The performance of K-means clustering depends on the initial guess of partition. We motivate theoretically and experimentally the use of a deterministic divisive hierarchical method, which we refer to as PCA-Part (principal components analysis partitioning) for initialization. The criterion that K-means clustering minimizes is the SSE (sum-squared-error) criterion. The first principal direction (the eigenvector corresponding to the largest eigenvalue of the covariance matrix) is the direction which contributes the largest SSE. Hence, a good candidate direction to project a cluster for splitting is, then, the first principal direction. This is the basis for PCA-Part initialization method. Our experiments reveal that generally PCA-Part leads K-means to generate clusters with SSE values close to the minimum SSE values obtained by one hundred random start runs. In addition, this deterministic initialization method often leads K-means to faster convergence (less iterations) compared to random methods. Furthermore, we also theoretically show and confirm experimentally on synthetic data when PCA-Part may fail.},
	author = {Ting Su and J. Dy},
	booktitle = {16th IEEE International Conference on Tools with Artificial Intelligence},
	doi = {10.1109/ICTAI.2004.7},
	issn = {1082-3409},
	keywords = {pattern classification, pattern clustering, principal component analysis, unsupervised learning, k-means clustering, deterministic divisive hierarchical method, deterministic initialization, principal components analysis partitioning, sum-squared-error criterion, artificial intelligence, clustering algorithms, convergence, covariance matrix, eigenvalues and eigenfunctions, greedy algorithms, partitioning algorithms, pattern analysis, principal component analysis, sorting},
	month = {nov},
	number = {},
	pages = {784--786},
	title = {{A deterministic method for initializing K-means clustering}},
	volume = {},
	year = {2004},
}

@inproceedings{macqueen:1967a,
	abstract = {},
	address = {Berkeley, California, USA},
	author = {J. MacQueen},
	booktitle = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics},
	pages = {281--297},
	publisher = {University of California Press},
	title = {{Some methods for classification and analysis of multivariate observations}},
	url = {https://projecteuclid.org/euclid.bsmsp/1200512992},
	year = {1967},
}

@article{lloyd:1982a,
	abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2^{b}quanta,b=1,2, \cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
	acmid = {2269955},
	address = {Piscataway, NJ, USA},
	author = {S. Lloyd},
	doi = {10.1109/TIT.1982.1056489},
	issn = {0018-9448},
	issue_date = {March 1982},
	journal = {IEEE Transactions on Information Theory},
	keywords = {machine learning, ml, k-means, kmeans, clustering},
	month = {sep},
	number = {2},
	numpages = {9},
	pages = {129--137},
	publisher = {IEEE Press},
	title = {{Least Squares Quantization in PCM}},
	url = {http://dx.doi.org/10.1109/TIT.1982.1056489},
	volume = {28},
	year = {1982},
}

@article{forgy:1965a,
	author = {E. Forgy},
	journal = {Biometrics},
	keywords = {clustering, k-means, kmeans, machine learning, ml},
	number = {3},
	pages = {768--769},
	title = {{Cluster Analysis of Multivariate Data: Efficiency versus Interpretability of Classification}},
	volume = {21},
	year = {1965},
}

@inproceedings{hamerly:2002a,
	abstract = {We investigate here the behavior of the standard k-means clustering algorithm and several alternatives to it: the k-harmonic means algorithm due to Zhang and colleagues, fuzzy k-means, Gaussian expectation-maximization, and two new variants of k-harmonic means. Our aim is to find which aspects of these algorithms contribute to finding good clusterings, as opposed to converging to a low-quality local optimum. We describe each algorithm in a unified framework that introduces separate cluster membership and data weight functions. We then show that the algorithms do behave very differently from each other on simple low-dimensional synthetic datasets and image segmentation tasks, and that the k-harmonic means method is superior. Having a soft membership function is essential for finding high-quality clusterings, but having a non-constant data weight function is useful also.},
	acmid = {584890},
	address = {New York, NY, USA},
	author = {Greg Hamerly and Charles Elkan},
	booktitle = {Proceedings of the Eleventh International Conference on Information and Knowledge Management},
	doi = {10.1145/584792.584890},
	isbn = {1-58113-492-4},
	keywords = {clustering quality, k-harmonic means, k-means, unsupervised classification, machine learning, ml},
	location = {McLean, Virginia, USA},
	numpages = {8},
	pages = {600--607},
	publisher = {ACM},
	series = {CIKM '02},
	title = {{Alternatives to the K-means Algorithm That Find Better Clusterings}},
	url = {http://doi.acm.org/10.1145/584792.584890},
	year = {2002},
}

@article{matsumoto:1998a,
	abstract = {A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 219937 −1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 ≤ v ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p2) where p is the degree of the polynomial.We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.},
	acmid = {272995},
	address = {New York, NY, USA},
	author = {Makoto Matsumoto and Takuji Nishimura},
	doi = {10.1145/272991.272995},
	issn = {1049-3301},
	issue_date = {January 1998},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	keywords = {prng, mersenne twister, pseudorandom number, algorithm, k-distribution, m-sequences, gfsr, mt19937, mersenne primes, tgfsr, finite fields, incomplete array, inversive-decimation method, multiple-recursive matrix method, primitive polynomials, random number generation, tempering},
	month = {jan},
	number = {1},
	numpages = {28},
	pages = {3--30},
	publisher = {ACM},
	title = {{Mersenne Twister: A 623-dimensionally Equidistributed Uniform Pseudo-random Number Generator}},
	url = {http://doi.acm.org/10.1145/272991.272995},
	volume = {8},
	year = {1998},
}

@article{harase:2017a,
	abstract = {The 32-bit Mersenne Twister generator MT19937 is a widely used random number generator. To generate numbers with more than 32 bits in bit length, and particularly when converting into 53-bit double-precision floating-point numbers in [0,1) in the IEEE 754 format, the typical implementation concatenates two successive 32-bit integers and divides them by a power of 2. In this case, the 32-bit MT19937 is optimized in terms of its equidistribution properties (the so-called dimension of equidistribution with v-bit accuracy) under the assumption that one will mainly be using 32-bit output values, and hence the concatenation sometimes degrades the dimension of equidistribution compared with the simple use of 32-bit outputs. In this paper, we analyze such phenomena by investigating hidden 𝔽2-linear relations among the bits of high-dimensional outputs. Accordingly, we report that MT19937 with a specific lag set fails several statistical tests, such as the overlapping collision test, matrix rank test, and Hamming independence test.},
	author = {Shin Harase},
	doi = {},
	journal = {arXiv},
	keywords = {mersenne twister, prng, pseudorandom number, double-precision, ieee754, rng, mt19937},
	month = {sep},
	number = {},
	numpages = {15},
	pages = {},
	title = {{Conversion of Mersenne Twister to double-precision floating-point numbers}},
	url = {https://arxiv.org/abs/1708.06018},
	volume = {abs/1708.06018},
	year = {2017},
}

@article{doornik:2007a,
	abstract = {Conversion of unsigned 32-bit random integers to double precision floating point is discussed. It is shown that the standard practice can be unnecessarily slow and inflexible. It is argued that simulation experiments could benefit from making better use of the available precision.},
	acmid = {1189759},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Jurgen A. Doornik},
	doi = {10.1145/1189756.1189759},
	issn = {1049-3301},
	issue_date = {January 2007},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	keywords = {prng, pseudorandom number, rng, double, floating-point, ieee754, algorithm},
	month = {jan},
	number = {1},
	numpages = {},
	pages = {},
	publisher = {ACM},
	title = {{Conversion of High-period Random Numbers to Floating Point}},
	url = {http://doi.acm.org/10.1145/1189756.1189759},
	volume = {1},
	year = {2007},
}

@article{box:1964a,
	abstract = {In the analysis of data it is often assumed that observations y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>n</sub> are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
	author = {G. E. P. Box and D. R. Cox},
	doi = {},
	ISSN = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	keywords = {box-cox, transformation, box, cox, transform, power-transform, power, math},
	month = {apr},
	number = {2},
	pages = {211--252},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {{An Analysis of Transformations}},
	url = {http://www.jstor.org/stable/2984418},
	volume = {26},
	year = {1964},
}

@book{chauvenet:1868,
	address = {London, England},
	author = {William Chauvenet},
	edition = {5},
	keywords = {astronomy},
	month = {dec},
	publisher = {{Tr\"{u}bner \& Co}},
	title = {{A Manual of Spherical and Practical Astronomy}},
	volume = {5},
	year = {1868},
}

@article{grubbs:1969a,
	abstract = {Procedures are given for determining statistically whether the highest observation, the lowest observation, the highest and lowest observations, the two highest observations, the two lowest observations, or more of the observations in the sample are statistical outliers. Both the statistical formulae and the application of the procedures to examples are given, thus representing a rather complete treatment of tests for outliers in single samples. This paper has been prepared primarily as an expository and tutorial article on the problem of detecting outlying observations in much experimental work. We cover only tests of significance in this paper.},
	author = {Frank E. Grubbs},
	doi = {10.1080/00401706.1969.10490657},
	journal = {Technometrics},
	keywords = {grubbs, outlier, outliers, hypothesis testing, statistics},
	month = {feb},
	number = {1},
	pages = {1--21},
	publisher = {Taylor & Francis},
	title = {{Procedures for Detecting Outlying Observations in Samples}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1969.10490657},
	volume = {11},
	year = {1969},
}

@article{tietjen:1972a,
	abstract = {Several widely used tests for outlying observations are reviewed. Problems of repeated application and "masking" are described. Suggested as appropriate to over-come these problems are two new statistics: \\(L_k\\) which is based on the \\(k\\) largest (observed) values and \\(E_k\\) which is based on the \\(k\\) largest (in absolute value) residuals. Tables of approximate critical values for these statistics are given for 0.01, 0.025, 0.05, and 0.10 levels of significance and for sample size \\(n =\\) 3 (1) 20 (5) 50.},
	author = {Gary L. Tietjen and  Roger H. Moore},
	doi = {10.1080/00401706.1972.10488948},
	journal = {Technometrics},
	keywords = {grubbs, outlier, outliers, hypothesis testing, statistics},
	month = {aug},
	number = {3},
	pages = {583--597},
	publisher = {Taylor & Francis},
	title = {{Some Grubbs-Type Statistics for the Detection of Several Outliers}},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/00401706.1972.10488948},
	volume = {14},
	year = {1972},
}

@article{grubbs:1950a,
	abstract = {},
	author = {Frank E. Grubbs},
	doi = {10.1214/aoms/1177729885},
	journal = {The Annals of Mathematical Statistics},
	keywords = {grubbs, outliers, outlier, hypothesis testing, statistics},
	month = {mar},
	number = {1},
	pages = {27--58},
	publisher = {The Institute of Mathematical Statistics},
	title = {{Sample Criteria for Testing Outlying Observations}},
	url = {https://doi.org/10.1214/aoms/1177729885},
	volume = {21},
	year = {1950},
}

@inproceedings{suthaharan:2010a,
	abstract = {Security of wireless sensor networks (WSN) is an important research area in computer and communications sciences. Anomaly detection is a key challenge in ensuring the security of WSN. Several anomaly detection algorithms have been proposed and validated recently using labeled datasets that are not publicly available. Our group proposed an ellipsoid-based anomaly detection algorithm but demonstrated its performance using synthetic datasets and real Intel Berkeley Research Laboratory and Grand St. Bernard datasets which are not labeled with anomalies. This approach requires manual assignment of the anomalies' positions based on visual estimates for performance evaluation. In this paper, we have implemented a single-hop and multi-hop sensor-data collection network. In both scenarios we generated real labeled data for anomaly detection and identified different types of anomalies. These labeled sensor data and types of anomalies are useful for research, such as machine learning, and this information will be disseminated to the research community.},
	author = {Shan Suthaharan and Mohammed Alzahrani and Sutharshan Rajasegarar and Christopher Leckie and Marimuthu Palaniswami},
	booktitle = {{Proceedings of the Sixth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP 2010)}},
	keywords = {machine learning, dataset, sensor, anomaly detection, network, data},
	location = {Brisbane, Australia},
	month = {dec},
	publisher = {IEEE},
	title = {{Labelled data collection for anomaly detection in wireless sensor networks}},
	year = {2010},
}

@article{harrison:1978a,
	abstract = {This paper investigates the methodological problems associated with the use of housing market data to measure the willingness to pay for clean air. With the use of a hedonic housing price model and data for the Boston metropolitan area, quantitative estimates of the willingness to pay for air quality improvements are generated. Marginal air pollution damages (as revealed in the housing market) are found to increase with the level of air pollution and with household income. The results are relatively sensitive to the specification of the hedonic housing price equation, but insensitive to the specification of the air quality demand equation.},
	author = {David Harrison and Daniel L Rubinfeld},
	doi = {10.1016/0095-0696(78)90006-2},
	issn = {0095-0696},
	journal = {Journal of Environmental Economics and Management},
	keywords = {datasets, data, linear regression, machine learning, house prices},
	month = {mar},
	number = {1},
	pages = {81--102},
	title = {{Hedonic housing prices and the demand for clean air}},
	url = {http://www.sciencedirect.com/science/article/pii/0095069678900062},
	volume = {5},
	year = {1978},
}

@article{gilley:1996a,
	abstract = {},
	author = {Otis W. Gilley and R.Kelley Pace},
	doi = {10.1006/jeem.1996.0052},
	journal = {Journal of Environmental Economics and Management},
	keywords = {datasets, data, linear regression, machine learning, house prices},
	month = {nov},
	number = {3},
	pages = {403--405},
	title = {{On the Harrison and Rubinfeld Data}},
	url = {http://www.sciencedirect.com/science/article/pii/S0095069696900522},
	volume = {31},
	year = {1996},
}

@article{pace:1997a,
	abstract = {Using the well-known Harrison and Rubinfeld (1978) hedonic pricing data, this manuscript demonstrates the substantial benefits obtained by modeling the spatial dependence of the errors. Specifically, the estimated errors on the spatial autoregression fell by 44{\%} relative to OLS. The spatial autoregression corrects predicted values by a nonparametric estimate of the error on nearby observations and thus mimics the behavior of appraisers. The spatial autoregression, by formally incorporating the areal configuration of the data to increase predictive accuracy and estimation efficiency, has great potential in real estate empirical work.},
	author = {R. Kelley Pace and Otis W. Gilley},
	day = {1},
	doi = {10.1023/A:1007762613901},
	issn = {1573-045X},
	journal = {The Journal of Real Estate Finance and Economics},
	keywords = {datasets, data, linear regression, machine learning, house prices},
	month = {may},
	number = {3},
	pages = {333-340},
	title = {{Using the Spatial Configuration of the Data to Improve Estimation}},
	url = {https://doi.org/10.1023/A:1007762613901},
	volume = {14},
	year = {1997},
}

@book{berndt:1991a,
	author = {Ernst R. Berndt},
	keywords = {econometrics, economics},
	publisher = {Addison Wesley Longman Publishing Co},
	title = {{The Practice of Econometrics}},
	year = {1991},
}

@article{fukushima:2009a,
	abstract = {As a preparation step to compute Jacobian elliptic functions efficiently, we created a fast method to calculate the complete elliptic integral of the first and second kinds, K(m) and E(m), for the standard domain of the elliptic parameter, 0 < m < 1. For the case 0 < m < 0.9, the method utilizes 10 pairs of approximate polynomials of the order of 9--19 obtained by truncating Taylor series expansions of the integrals. Otherwise, the associate integrals, K(1 − m) and E(1 − m), are first computed by a pair of the approximate polynomials and then transformed to K(m) and E(m) by means of Jacobi's nome, q, and Legendre's identity relation. In average, the new method runs more-than-twice faster than the existing methods including Cody's Chebyshev polynomial approximation of Hastings type and Innes' formulation based on q-series expansions. Next, we invented a fast procedure to compute simultaneously three Jacobian elliptic functions, sn(u|m), cn(u|m), and dn(u|m), by repeated usage of the double argument formulae starting from the Maclaurin series expansions with respect to the elliptic argument, u, after its domain is reduced to the standard range, 0 ≤ u < K(m)/4, with the help of the new method to compute K(m). The new procedure is 25--70{\%} faster than the methods based on the Gauss transformation such as Bulirsch's algorithm, sncndn, quoted in the Numerical Recipes even if the acceleration of computation of K(m) is not taken into account.},
	author = {Toshio Fukushima},
	day = {25},
	doi = {10.1007/s10569-009-9228-z},
	issn = {1572-9478},
	journal = {Celestial Mechanics and Dynamical Astronomy},
	keywords = {complete elliptic integral, function approximation, math, special function, elliptic, numerical, algorithm},
	month = {oct},
	number = {4},
	pages = {305},
	title = {{Fast computation of complete elliptic integrals and Jacobian elliptic functions}},
	url = {https://doi.org/10.1007/s10569-009-9228-z},
	volume = {105},
	year = {2009},
}

@article{fukushima:2015a,
	abstract = {Piecewise minimax rational function approximations with the single and double precision accuracies are developed for (i) K(m) and E(m), the complete elliptic integral of the first and second kind, respectively, and (ii) B(m)≡(E(m)−(1−m)K(m))/m and D(m)≡(K(m)−E(m))/m, two associate complete elliptic integrals of the second kind. The maximum relative error is one and 5 machine epsilons in the single and double precision computations, respectively. The new approximations run faster than the exponential function. When compared with the previous methods (Fukushima, 2009; Fukushima, 2011), which have been the fastest among the existing double precision procedures, the new method requires around a half of the memory and runs 1.7–2.2 times faster.},
	author = {Toshio Fukushima},
	doi = {10.1016/j.cam.2014.12.038},
	issn = {0377-0427},
	journal = {Journal of Computational and Applied Mathematics},
	keywords = {complete elliptic integral, function approximation, logarithmic singularity, minimax approximation, rational function approximation, math, special function, elliptic, numerical, algorithm},
	month = {jul},
	number = {},
	pages = {71--76},
	title = {{Precise and fast computation of complete elliptic integrals by piecewise minimax rational function approximation}},
	url = {http://www.sciencedirect.com/science/article/pii/S0377042715000023},
	volume = {282},
	year = {2015},
}

@article{porter:1980,
	abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length.},
	author = {Michael F. Porter},
	journal = {Program},
	doi = {10.1108/eb046814},
	issn = {00330337},
	number = {3},
	pages = {130--137},
	title = {{An algorithm for suffix stripping}},
	volume = {13},
	year = {1980},
}

@misc{aschwanden:2016a,
	author = {Christie Aschwanden},
	keywords = {fivethirtyeight, 538, p-hacking, p-values, statistics, stats, data mining, nutrition, correlation, spurious correlation, causation, big data},
	title = {{You Can’t Trust What You Read About Nutrition}},
	url = {https://fivethirtyeight.com/features/you-cant-trust-what-you-read-about-nutrition/},
	year = {2016},
}

@misc{bialik:2016a,
	author = {Carl Bialik},
	keywords = {fivethirtyeight, 538, names, us, usa, united states, baby},
	title = {{Some People Are Too Superstitious To Have A Baby On Friday The 13th}},
	url = {https://fivethirtyeight.com/features/some-people-are-too-superstitious-to-have-a-baby-on-friday-the-13th/},
	year = {2016},
}

@article{lawson:1979a,
	abstract = {},
	address = {New York, NY, USA},
	author = {Charles L. Lawson and Richard J. Hanson and Fred T. Krogh and David Ronald Kincaid},
	doi = {10.1145/355841.355848},
	issn = {0098-3500},
	issue_date = {September 1979},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {},
	month = {sep},
	number = {3},
	numpages = {2},
	pages = {324--325},
	publisher = {Association for Computing Machinery},
	title = {{Algorithm 539: Basic Linear Algebra Subprograms for Fortran Usage [F1]}},
	url = {https://doi.org/10.1145/355841.355848},
	volume = {5},
	year = {1979},
}

@article{neumaier:1974a,
	abstract = {The rounding‐error arising during summation can be interpreted as a measure for the quality of the procedure used. In the following, a‐priori‐bounds for this rounding‐error are used to compare several summation procedures, e.g. the common procedure and the method of Kahan‐Babuška.},
	author = {Arnold Neumaier},
	doi = {10.1002/zamm.19740540106},
	journal = {Zeitschrift für Angewandte Mathematik und Mechanik},
	keywords = {summation, numerical analysis, algorithms, math, kahan, precision, floating-point arithmetic},
	month = {},
	number = {1},
	pages = {39--51},
	title = {{Rounding Error Analysis of Some Methods for Summing Finite Sums}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/zamm.19740540106},
	volume = {54},
	year = {1974},
}

@article{klein:2005a,
	abstract = {In this article, we combine recursive summation techniques with Kahan-Babu{\v s}ka type balancing strategies {$[$}1{$]$}, {$[$}7{$]$} to get highly accurate summation formulas. An i-th algorithm have only error beyond 1upl and thus allows to sum many millions of numbers with high accuracy. The additional afford is a small multiple of the naive summation. In addition we show that these algorithms could be modified to provide tight upper and lower bounds for use with interval arithmetic.},
	author = {Andreas Klein},
	doi = {10.1007/s00607-005-0139-x},
	journal = {Computing},
	keywords = {summation, numerical analysis, algorithms, math, kahan, precision, floating-point arithmetic},
	month = {nov},
	number = {3},
	pages = {279--293},
	title = {{A Generalized Kahan-Babu{\v s}ka-Summation-Algorithm}},
	url = {https://doi.org/10.1007/s00607-005-0139-x},
	volume = {76},
	year = {2005},
}

@article{higham:1993a,
	abstract = {The usual recursive summation technique is just one of several ways of computing the sum of n floating point numbers. Five summation methods and their variations are analyzed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases.},
	author = {Nicholas J. Higham},
	doi = {10.1137/0914050},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {summation, numerical analysis, algorithms, math, precision, floating-point arithmetic},
	month = {},
	number = {4},
	pages = {783--799},
	title = {{The Accuracy of Floating Point Summation}},
	url = {https://doi.org/10.1137/0914050},
	volume = {14},
	year = {1993},
}

@article{ogita:2005a,
	abstract = {Algorithms for summation and dot product of floating-point numbers are presented which are fast in terms of measured computing time. We show that the computed results are as accurate as if computed in twice or K-fold working precision, $K\ge 3$. For twice the working precision our algorithms for summation and dot product are some 40% faster than the corresponding XBLAS routines while sharing similar error estimates. Our algorithms are widely applicable because they require only addition, subtraction, and multiplication of floating-point numbers in the same working precision as the given data. Higher precision is unnecessary, algorithms are straight loops without branch, and no access to mantissa or exponent is necessary.},
	author = {Takeshi Ogita and Siegfried M Rump and Shin'ichi Oishi},
	doi = {10.1137/030601818},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {summation, numerical analysis, algorithms, math, precision, floating-point arithmetic, dot product},
	month = {},
	number = {6},
	pages = {1955--1988},
	title = {{Accurate Sum and Dot Product}},
	url = {https://doi.org/10.1137/030601818},
	volume = {26},
	year = {2005},
}

@article{rump:2009a,
	abstract = {We present two new algorithms FastAccSum and FastPrecSum, one to compute a faithful rounding of the sum of floating-point numbers and the other for a result “as if” computed in K-fold precision. Faithful rounding means the computed result either is one of the immediate floating-point neighbors of the exact result or is equal to the exact sum if this is a floating-point number. The algorithms are based on our previous algorithms AccSum and PrecSum and improve them by up to 25%. The first algorithm adapts to the condition number of the sum; i.e., the computing time is proportional to the difficulty of the problem. The second algorithm does not need extra memory, and the computing time depends only on the number of summands and K. Both algorithms are the fastest known in terms of flops. They allow good instruction-level parallelism so that they are also fast in terms of measured computing time. The algorithms require only standard floating-point addition, subtraction, and multiplication in one working precision, for example, double precision.},
	author = {Siegfried M Rump},
	doi = {10.1137/080738490},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {summation, numerical analysis, algorithms, math, precision, floating-point arithmetic},
	month = {},
	number = {5},
	pages = {3466--3502},
	title = {{Ultimately Fast Accurate Summation}},
	url = {https://doi.org/10.1137/080738490},
	volume = {31},
	year = {2009},
}

@article{zhu:2010a,
	abstract = {We present a novel, online algorithm for exact summation of a stream of floating-point numbers. By “online” we mean that the algorithm needs to see only one input at a time, and can take an arbitrary length input stream of such inputs while requiring only constant memory. By “exact” we mean that the sum of the internal array of our algorithm is exactly equal to the sum of all the inputs, and the returned result is the correctly-rounded sum. The proof of correctness is valid for all inputs (including nonnormalized numbers but modulo intermediate overflow), and is independent of the number of summands or the condition number of the sum. The algorithm asymptotically needs only 5 FLOPs per summand, and due to instruction-level parallelism runs only about 2--3 times slower than the obvious, fast-but-dumb “ordinary recursive summation” loop when the number of summands is greater than 10,000. Thus, to our knowledge, it is the fastest, most accurate, and most memory efficient among known algorithms. Indeed, it is difficult to see how a faster algorithm or one requiring significantly fewer FLOPs could exist without hardware improvements. An application for a large number of summands is provided.},
	author = {Yong-Kang Zhu and Wayne B Hayes},
	doi = {10.1145/1824801.1824815},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {summation, numerical analysis, algorithms, math, precision, floating-point arithmetic},
	month = {sep},
	number = {3},
	pages = {1--13},
	title = {{Algorithm 908: Online Exact Summation of Floating-Point Streams}},
	url = {https://doi.org/10.1145/1824801.1824815},
	volume = {37},
	year = {2010},
}

@article{rump:2008a,
	abstract = {Given a vector of floating-point numbers with exact sum s, we present an algorithm for calculating a faithful rounding of s, i.e., the result is one of the immediate floating-point neighbors of s. If the sum s is a floating-point number, we prove that this is the result of our algorithm. The algorithm adapts to the condition number of the sum, i.e., it is fast for mildly conditioned sums with slowly increasing computing time proportional to the logarithm of the condition number. All statements are also true in the presence of underflow. The algorithm does not depend on the exponent range. Our algorithm is fast in terms of measured computing time because it allows good instruction-level parallelism, it neither requires special operations such as access to mantissa or exponent, it contains no branch in the inner loop, nor does it require some extra precision: The only operations used are standard floating-point addition, subtraction, and multiplication in one working precision, for example, double precision. Certain constants used in the algorithm are proved to be optimal.},
	author = {Siegfried M Rump and Takeshi Ogita and Shin'ichi Oishi},
	doi = {10.1137/050645671},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {summation, numerical analysis, algorithms, math, precision, floating-point arithmetic},
	month = {},
	number = {1},
	pages = {189–-224},
	title = {{Accurate Floating-Point Summation Part I: Faithful Rounding}},
	url = {https://doi.org/10.1137/050645671},
	volume = {31},
	year = {2008},
}

@article{rump:2008b,
	abstract = {In Part II of this paper we first refine the analysis of error-free vector transformations presented in Part I. Based on that we present an algorithm for calculating the rounded-to-nearest result of $s:=\sum p_i$ for a given vector of floating-point numbers $p_i$, as well as algorithms for directed rounding. A special algorithm for computing the sign of s is given, also working for huge dimensions. Assume a floating-point working precision with relative rounding error unit eps. We define and investigate a K-fold faithful rounding of a real number r. Basically the result is stored in a vector $\mathtt{Res}_{\nu}$ of K nonoverlapping floating-point numbers such that $\sum\mathtt{Res}_{\nu}$ approximates r with relative accuracy $\mathtt{eps}^K$, and replacing $\mathtt{Res}_K$ by its floating-point neighbors in $\sum\mathtt{Res}_{\nu}$ forms a lower and upper bound for r. For a given vector of floating-point numbers with exact sum s, we present an algorithm for calculating a K-fold faithful rounding of s using solely the working precision. Furthermore, an algorithm for calculating a faithfully rounded result of the sum of a vector of huge dimension is presented. Our algorithms are fast in terms of measured computing time because they allow good instruction-level parallelism, they neither require special operations such as access to mantissa or exponent, they contain no branch in the inner loop, nor do they require some extra precision. The only operations used are standard floating-point addition, subtraction, and multiplication in one working precision, for example, double precision. Certain constants used in the algorithms are proved to be optimal.},
	author = {Siegfried M Rump and Takeshi Ogita and Shin'ichi Oishi},
	doi = {10.1137/07068816X},
	journal = {SIAM Journal on Scientific Computing},
	keywords = {summation, numerical analysis, algorithms, math, precision, floating-point arithmetic},
	month = {},
	number = {2},
	pages = {1269--1302},
	title = {{Accurate Floating-Point Summation Part II: Sign, K-Fold Faithful and Rounding to Nearest}},
	url = {https://doi.org/10.1137/07068816X},
	volume = {31},
	year = {2008},
}

@article{west:1979a,
	abstract = {A method of improved efficiency is given for updating the mean and variance of weighted sampled data when an additional data value is included in the set. Evidence is presented that the method is stable and at least as accurate as the best existing updating method.},
	author = {D H D West},
	doi = {10.1145/359146.359153},
	journal = {Communications of the ACM},
	keywords = {updating estimates, variance, standard deviation, mean, removing data},
	month = {sep},
	number = {9},
	pages = {532--535},
	title = {{Updating Mean and Variance Estimates: An Improved Method}},
	url = {https://doi.org/10.1145/359146.359153},
	volume = {22},
	year = {1979},
}

@article{chan:1979a,
	abstract = {Four algorithms for the numerical computation of the standard deviation of (unweighted) sampled data are analyzed. Two of the algorithms are well-known in the statistical and computational literature; the other two are new algorithms specifically intended for automatic computation. Our discussion is expository, with emphasis on reaching a suitable definition of “accuracy.” Each of the four algorithms is analyzed for the conditions under which it will be accurate. We conclude that all four algorithms will provide accurate answers for many problems, but two of the algorithms, one new, one old, are substantially more accurate on difficult problems than are the other two.},
	author = {Tony F. Chan and John Gregg Lewis},
	doi = {10.1145/359146.359152},
	journal = {Communications of the ACM},
	keywords = {least squares, rounding error analysis, mean, condition number, updating estimates, standard deviation, variance},
	month = {sep},
	number = {9},
	pages = {526--531},
	title = {{Computing Standard Deviations: Accuracy}},
	url = {https://doi.org/10.1145/359146.359152},
	volume = {2},
	year = {1979},
}

@inproceedings{chan:1982a,
	abstract = {A general formula is presented for computing the sample variances for a sample of size m + n given the means and variances for two subsamples of sizes m and n. This formula is used in the construction of a pairwise algorithm for computing the variance. Other applications are discussed as well, including the use of updating formulae in a parallel computing environment. We present numerical results and rounding error analyzes for several numerical schemes.},
	author = {Tony F Chan and Gene H Golub and Randall J LeVeque},
	booktitle = {COMPSTAT 1982 5th Symposium held at Toulouse 1982},
	doi = {10.1007/978-3-642-51461-6_3},
	keywords = {least squares, rounding error analysis, mean, condition number, updating estimates, standard deviation, variance},
	publisher = {Physica-Verlag HD},
	title = {{Updating Formulae and a Pairwise Algorithm for Computing Sample Variances}},
	url = {https://doi.org/10.1007/978-3-642-51461-6_3},
	year = {1982},
}

@article{youngs:1971a,
	abstract = {Sum and sum-of-product algorithms, some designed to minimize significance error, are compared and recommendations for usage are offered. A simplified method of rounding is presented and the benefit of using it demonstrated. Results are presented for the IBM 360 as well as the IBM 7040 computers with obvious application to some other machines.},
	author = {Edward A. Youngs and Elliot M. Cramer},
	doi = {10.1080/00401706.1971.10488826},
	journal = {Technometrics},
	keywords = {least squares, rounding error analysis, mean, condition number, updating estimates, standard deviation, variance},
	month = {},
	number = {3},
	pages = {657--665},
	title = {{Some Results Relevant to Choice of Sum and Sum-of-Product Algorithms}},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/00401706.1971.10488826},
	volume = {13},
	year = {1971},
}

@inproceedings{schubert:2018a,
	abstract = {With the advent of big data, we see an increasing interest in computing correlations in huge data sets with both many instances and many variables. Essential descriptive statistics such as the variance, standard deviation, covariance, and correlation can suffer from a numerical instability known as "catastrophic cancellation" that can lead to problems when naively computing these statistics with a popular textbook equation. While this instability has been discussed in the literature already 50 years ago, we found that even today, some high-profile tools still employ the instable version. In this paper, we study a popular incremental technique originally proposed by Welford, which we extend to weighted covariance and correlation. We also discuss strategies for further improving numerical precision, how to compute such statistics online on a data stream, with exponential aging, with missing data, and a batch parallelization for both high performance and numerical precision. We demonstrate when the numerical instability arises, and the performance of different approaches under these conditions. We showcase applications from the classic computation of variance as well as advanced applications such as stock market analysis with exponentially weighted moving models and Gaussian mixture modeling for cluster analysis that all benefit from this approach.},
	address = {New York, NY, USA},
	author = {Erich Schubert and Michael Gertz},
	booktitle = {Proceedings of the 30th International Conference on Scientific and Statistical Database Management},
	doi = {10.1145/3221269.3223036},
	isbn = {9781450365055},
	keywords = {least squares, rounding error analysis, mean, updating estimates, standard deviation, variance, covariance, correlation},
	publisher = {Association for Computing Machinery},
	title = {{Numerically Stable Parallel Computation of (Co-)Variance}},
	url = {https://doi.org/10.1145/3221269.3223036},
	year = {2018},
}

@article{neely:1966a,
	abstract = {Several algorithms for computation of basic statistics are compared by their performance on systematically generated test data. The statistics calculated were the mean, standard deviation and correlation coefficient. For each statistic, the algorithm included the usual computing formulas, correction due to an accumulated error term, and a recursive computation of the current value of the statistic. The usual computing formulas were also evaluated in double precision. Large errors were noted for some calculation using the usual computing formulas. The most reliable technique was correction of the initial estimate by use of an accumulated error term. To eliminate the need for making two passes on the data, it was suggested that the initial estimate of the mean be obtained from a subset of the data.},
	author = {Peter M. Neely},
	doi = {10.1145/365719.365958},
	journal = {Communications of the ACM},
	keywords = {mean, updating estimates, standard deviation, variance, algorithms, numerical analysis},
	month = {jul},
	number = {7},
	pages = {496--499},
	publisher = {Association for Computing Machinery},
	title = {{Comparison of Several Algorithms for Computation of Means, Standard Deviations and Correlation Coefficients}},
	url = {https://doi.org/10.1145/365719.365958},
	volume = {9},
	year = {1966},
}

@article{welford:1962a,
	abstract = {},
	author = {B. P. Welford},
	doi = {10.1080/00401706.1962.10490022},
	journal = {Technometrics},
	keywords = {mean, updating estimates, standard deviation, variance, algorithms, numerical analysis},
	month = {aug},
	number = {3},
	pages = {419--420},
	publisher = {Taylor & Francis},
	title = {{Note on a Method for Calculating Corrected Sums of Squares and Products}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1962.10490022},
	volume = {4},
	year = {1962},
}

@article{vanreeken:1968a,
	abstract = {},
	author = {A. J. {van Reeken}},
	doi = {10.1145/362929.362961},
	journal = {Communications of the ACM},
	keywords = {algorithm, Neely's comparisons, truncation error, computation of statistics, mean, variance, numerical},
	month = {mar},
	number = {3},
	pages = {149--150},
	title = {{Letters to the Editor: Dealing with Neely's Algorithms}},
	url = {https://doi.org/10.1145/362929.362961},
	volume = {11},
	year = {1968},
}

@inproceedings{ciura:2001a,
	abstract = {This paper presents the results of using sequential analysis to find increment sequences that minimize the average running time of Shellsort, for array sizes up to several thousand elements. The obtained sequences outperform by about 3{\%} the best ones known so far, and there is a plausible evidence that they are the optimal ones.},
	author = {Marcin Ciura},
	booktitle = {Fundamentals of Computation Theory},
	doi = {10.1007/3-540-44669-9_12},
	keywords = {sorting, algorithm, algorithms, shellshort},
	pages = {106--117},
	publisher = {Springer Berlin Heidelberg},
	title = {{Best Increments for the Average Case of Shellsort}},
	url = {https://doi.org/10.1007/3-540-44669-9_12},
	year = {2001},
}

@article{shell:1959a,
	abstract = {},
	author = {Donald L Shell},
	doi = {10.1145/368370.368387},
	journal = {Communications of the ACM},
	keywords = {sorting, algorithm, algorithms, shellsort},
	month = {jul},
	number = {7},
	pages = {30--32},
	publisher = {Association for Computing Machinery},
	title = {{A High-Speed Sorting Procedure}},
	url = {https://doi.org/10.1145/368370.368387},
	volume = {2},
	year = {1959},
}

@article{sedgewick:1986a,
	abstract = {A direct relationship between Shellsort and the classical "problem of Frobenius" from additive number theory is used to derive a sequence of O(log N) increments for Shellsort for which the worst case running time is O(N^(4/3)). The previous best-known upper bound for sequences of O(log N) increments was O(N^(3/2)), which was shown by Pratt to be tight for a large family of sequences, including those commonly used in practice. The new upper bound is of theoretical interest because it suggests that increment sequences might exist which admit even better upper bounds, and of practical interest because the increment sequences which arise outperform those common used, even for random files.},
	author = {Robert Sedgewick},
	doi = {10.1016/0196-6774(86)90001-5},
	journal = {Journal of Algorithms},
	keywords = {sorting, algorithms, algorithm, shellsort},
	month = {jun},
	number = {2},
	pages = {159--173},
	title = {{A new upper bound for Shellsort}},
	url = {https://doi.org/10.1016/0196-6774(86)90001-5},
	volume = {7},
	year = {1986},
}

@article{williams:1964a,
	abstract = {},
	address = {New York, NY, USA},
	author = {John William and Joseph Williams},
	doi = {10.1145/512274.512284},
	journal = {Communications of the ACM},
	keywords = {algorithms, sorting, algorithm, heap, heapsort},
	month = {jun},
	number = {6},
	pages = {347--349},
	publisher = {Association for Computing Machinery},
	title = {{Algorithm 232: Heapsort}},
	url = {https://doi.org/10.1145/512274.512284},
	volume = {7},
	year = {1964},
}

@article{floyd:1964a,
	abstract = {},
	address = {New York, NY, USA},
	author = {Robert W Floyd},
	doi = {10.1145/355588.365103},
	journal = {Communications of the ACM},
	keywords = {algorithms, sorting, algorithm, heap, heapsort, treesort},
	month = {dec},
	number = {12},
	pages = {701},
	publisher = {Association for Computing Machinery},
	title = {{Algorithm 245: Treesort}},
	url = {https://doi.org/10.1145/355588.365103},
	volume = {7},
	year = {1964},
}

@inproceedings{zhang:2004a,
	abstract = {Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.},
	address = {New York, NY, USA},
	author = {Tong Zhang},
	booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
	doi = {10.1145/1015330.1015332},
	keywords = {machine learning, ml, classification, stochastic gradient descent, sgd, regression, linear regression},
	location = {Banff, Alberta, Canada},
	pages = {116},
	publisher = {Association for Computing Machinery},
	title = {{Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms}},
	url = {https://doi.org/10.1145/1015330.1015332},
	year = {2004},
}

@techreport{rosenblatt:1957a,
	abstract = {},
	address = {Buffalo, NY, USA},
	author = {Frank Rosenblatt},
	institution = {Cornell Aeronautical Laboratory},
	keywords = {neural networks, machine learning, ml, loss functions},
	number = {85-460-1},
	title = {{The Perceptron--a perceiving and recognizing automaton}},
	url = {},
	year = {1957},
}

@article{shalevshwartz:2011a,
	abstract = {We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy ${\epsilon}$ is ${\tilde{O}(1 / \epsilon)}$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require ${\Omega(1 / \epsilon^2)}$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with $1/\lambda$, where $\lambda$ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is ${\tilde{O}(d/(\lambda \epsilon))}$, where $d$ is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.},
	author = {Shai Shalev-Shwartz and Yoram Singer and Nathan Srebro and Andrew Cotter },
	doi = {10.1007/s10107-010-0420-4},
	journal = {Mathematical Programming},
	keywords = {machine learning, svm, ml, classification, regression, loss functions},
	month = {03},
	number = {1},
	pages = {3--30},
	title = {{Pegasos: primal estimated sub-gradient solver for SVM}},
	url = {https://doi.org/10.1007/s10107-010-0420-4},
	volume = {127},
	year = {2011},
}
