@techreport{beebe:2002,
	abstract = {These notes describe an implementation of an algorithm for accurate computation of expm1(x) = exp(x) − 1, one of the new elementary functions introduced in the 1999 ISO C Standard, but already available in most UNIX C implementations. A test package modeled after the Cody and Waite Elementary Function Test Package, ELEFUNT, is developed to evaluate the accuracy of implementations of expm1(x).},
	author = {Nelson H.F. Beebe},
	keywords = {mathematics, math, special function, exponential, exp},
	institution = {University of Utah},
	title = {{Computation of expm1(x) = exp(x) − 1}},
	url = {http://www.math.utah.edu/~beebe/reports/expm1.pdf},
	year = {2002},
}

@article{blair:1976,
	abstract = {This report presents near-minimax rational approximations for the inverse of the error function inverf x, for  \\( 0 \leqslant x \leqslant 1 - {10^{ - 10000}} \\), with relative errors ranging down to  \\( {10^{ - 23}}\\). An asymptotic formula for the region \\( x \to 1\\) is also given.},
	author = {J.M. Blair and C.A. Edwards and J.H. Johnson},
	doi = {10.1090/S0025-5718-1976-0421040-7},
	journal = {Mathematics of Computation},
	keywords = {Rational Chebyshev approximations, inverse error function, minimal Newton form, erf, erfinv, mathematics, math, special function},
	pages = {827--830},
	title = {{Rational Chebyshev approximations for the inverse of the error function}},
	url = {http://www.ams.org/journals/mcom/1976-30-136/S0025-5718-1976-0421040-7/},
	volume = {30},
	year = {1976},
}

@inproceedings{borwein:1991,
	abstract = {A very simple class of algorithms for the computation of the Riemann-zeta function to arbitrary precision in arbitrary domains is proposed. These algorithms compete with the standard methods based on Euler-Maclaurin summation, are easier to implement and are easier to analyze.},
	author = {P. Borwein},
	booktitle = {Conference Proceedings},
	keywords = {Riemann zeta function, computation, high precision, algorithm, mathematics, math, special function, riemann, zeta},
	publisher = {Canadian Mathematical Society},
	title = {{An Efficient Algorithm for the Riemann Zeta Function}},
	year = {1991},
}

@article{carlitz:1963,
	author = {L. Carlitz},
	journal = {Pacific Journal of Mathematics},
	keywords = {mathematics, math, error function, erf},
	number = {2},
	pages = {459--470},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{The inverse of the error function.}},
	url = {http://msp.org/pjm/1963/13-2/pjm-v13-n2-p06-s.pdf},
	volume = {13},
	year = {1963},
}

@mastersthesis{segarra:2006,
	abstract = {Identified as one of the 7 Millennium Problems, the Riemann zeta hypothesis has successfully evaded mathematicians for over 100 years. Simply stated, Riemann conjectured that all of the nontrivial zeroes of his zeta function have real part equal to 1/2. This thesis attempts to explore the theory behind Riemann’s zeta function by first starting with Euler’s zeta series and building up to Riemann’s function. Along the way we will develop the math required to handle this theory in hopes that by the end the reader will have immersed themselves enough to pursue their own exploration and research into this fascinating subject.},
	author = {Elan Segarra},
	keywords = {math, mathematics, riemann zeta, riemann, zeta, special function},
	school = {Harvey Mudd College},
	title = {{An Exploration of the Riemann Zeta Function and its Application to the Theory of Prime Number Distribution}},
	url = {https://www.math.hmc.edu/seniorthesis/archives/2006/esegarra/esegarra-2006-thesis.pdf},
	year = {2006}
}

@article{fransen:1980,
	abstract = {In this paper we determine numerical values to 80D of the coefficients in the Taylor series expansion \\( {\Gamma ^m}(s + x) = \Sigma _0^\infty {g_k}(m,s){x^k}\\) for certain values of \\(m\\) and \\(s\\) and use these values to calculate  \\( \Gamma (p/q)\;(p,q = 1,2, \ldots ,10;\;p < q)\\) and  \\({\min _{x > 0}}\Gamma (x)\\) to 80D. Finally, we obtain a high-precision value of the integral  \\(\smallint _0^\infty {(\Gamma (x))^{ - 1}}\;dx\\).},
	author = {Arne Frans\'{e}n and Staffan Wrigge},
	doi = {10.1090/S0025-5718-1980-0559204-5},
	journal = {Mathematics of Computation},
	keywords = {Special functions, Gamma function, Riemann Zeta function, gamma, mathematics, math, special function},
	pages = {553--566},
	title = {{High-precision values of the gamma function and of some related coefficients}},
	url = {http://www.ams.org/journals/mcom/1980-34-150/S0025-5718-1980-0559204-5/},
	volume = {34},
	year = {1980},
}

@article{goldberg:1991,
	abstract = {Floating-point arithmetic is considered as esoteric subject by many people. This is rather surprising, because floating-point is ubiquitous in computer systems: Almost every language has a floating-point datatype; computers from PCs to supercomputers have floating-point accelerators; most compilers will be called upon to compile floating-point algorithms from time to time; and virtually every operating system must respond to floating-point exceptions such as overflow. This paper presents a tutorial on the aspects of floating-point that have a direct impact on designers of computer systems. It begins with background on floating-point representation and rounding error, continues with a discussion of the IEEE floating point standard, and concludes with examples of how computer system builders can better support floating point.},
	acmid = {103163},
	address = {New York, NY, USA},
	author = {David Goldberg},
	doi = {10.1145/103162.103163},
	journal = {ACM Comput. Surv.},
	issn = {0360-0300},
	issue_date = {March 1991},
	keywords = {NaN, denormalized number, exception, floating-point, floating-point standard, gradual underflow, guard digit, overflow, relative error, rounding error, rounding mode, ulp, underflow, computation, ieee754, math, double, float},
	month = {mar},
	number = {1},
	numpages = {44},
	pages = {5--48},
	publisher = {ACM},
	title = {What Every Computer Scientist Should Know About Floating-point Arithmetic},
	url = {http://doi.acm.org/10.1145/103162.103163},
	volume = {23},
	year = {1991},
}

@unpublished{gourdon:2003,
	author = {Xavier Gourdon and Pascal Sebah},
	keywords = {riemann zeta, riemann, zeta, math, mathematics, special function},
	note = {Numerical evaluation of the Riemann Zeta-function.},
	title = {{Numerical evaluation of the Riemann Zeta-function}},
	year = {2003},
}

@article{khajah:1994,
	abstract = {We describe a machine independent Fortran subroutine which performs the four basic arithmetic operations with a degree of accuracy prescribed by the user. Tables of Chebyshev expansions of orders 48 and 50 for some basic mathematical functions are obtained as a result of applying this subroutine in conjunction with the recursive formulation of the Tau Method. A recently devised technique for the sharp determination of upper and lower error bounds for Tau Method approximations (see [1]) enables us to find the degree n required to achieve a prescribed accuracy ϵ over a given interval [a,b]. A number of practical illustrations are given.},
	author = {H.G. Khajah and E.L. Ortiz},
	doi = {10.1016/0898-1221(94)90148-1},
	issn = {0898-1221},
	journal = {Computers & Mathematics with Applications},
	keywords = {numeric computing, precision, programming, computation},
	number = {7},
	pages = {41--57},
	title = {Ultra-high precision computations},
	url = {http://www.sciencedirect.com/science/article/pii/0898122194901481},
	volume = {27},
	year = {1994},
}

@phdthesis{pugh:2004,
	abstract = {This thesis is an analysis of C. Lanczos’ approximation of the classical gamma function Γ(z+1) as given in his 1964 paper \textit{A Precision Approximation of the Gamma Function}. The purposes of this study are: (i) to explain the details of Lanczos’ paper, including proofs of all claims made by the author; (ii) to address the question of how best to implement the approximation method in practice; and (iii) to generalize the methods used in the derivation of the approximation.},
	author = {Glendon Ralph Pugh},
	keywords = {gamma, lanczos, math, mathematics, special function},
	school = {The University of British Columbia},
	title = {{An Analysis of the Lanczos Gamma Approximation}},
	url = {https://web.viu.ca/pughg/phdThesis/phdThesis.pdf},
	year = {2004}
}

@article{meurer:2016,
	abstract = {SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become the standard symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select domain specific submodules. The supplementary materials provide additional examples and further outline details of the architecture and features of SymPy.},
	author = {A. Meurer and C.P. Smith and M. Paprocki and O. \u{C}ert\'{i}k and S.B. Kirpichev and M. Rocklin and A. Kumar and S. Ivanov and J.K. Moore and S. Singh and T. Rathnayake and S. Vig and B.E. Granger and R.P. Muller and F. Bonazzi and H. Gupta and S. Vats and F. Johansson and F. Pedregosa and M.J. Curry and A.R. Terrel and \u{S}. Rou\u{c}ka and A. Saboo and I. Fernando and S. Kulal and R. Cimrman and A. Scopatz},
	doi = {10.7287/peerj.preprints.2083v3},
	journal = {PeerJ Preprints},
	keywords = {symbolic, python, computer algebra system},
	month = {jun},
	number = {2083},
	title = {{SymPy: Symbolic computing in Python.}},
	url = {https://github.com/sympy/sympy-paper},
	volume = {4},
	year = {2016},
}

@unpublished{cao:high,
	abstract = {This paper presents the design and implementation of several fundamental dense linear algebra (DLA) algorithms in OpenCL. In particular, these are linear system solvers and eigenvalue problem solvers. Further, we give an overview of the clMAGMA library, an open source, high performance OpenCL library that incorporates the developments presented, and in general provides to heterogeneous architectures the DLA functionality of the popular LAPACK library. The LAPACK-compliance and use of OpenCL simplify the use of clMAGMA in applications, while providing them with portably performant DLA. High performance is obtained through use of the high-performance OpenCL BLAS, hardware and OpenCL-specific tuning, and a hybridization methodology where we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware components by minimizing data movements and mapping algorithmic requirements to the architectural strengths of the various heterogeneous hardware components.},
    author = {Chongxiao Cao and Jack Dongarra and Peng Du and Mark Gates and Piotr Luszczek and Stanimire Tomov},
    keywords = {gpu, opencl, blas, numeric computing, computation, lapack},
    note = {netlib},
    title = {{clMAGMA: High Performance Dense Linear Algebra with OpenCL}},
}

@misc{dawson:2013,
	author = {Bruce Dawson},
	keywords = {ieee754, floating-point, math, mathematics, numeric computing, computation},
	title = {{Floating-Point Determinism}},
	url = {https://randomascii.wordpress.com/2013/07/16/floating-point-determinism/},
	year = {2013}
}

@misc{godfrey:2001,
	author = {Paul Godfrey},
	keywords = {lanczos, gamma, special function, math, mathematics},
	title = {{A note on the computation of the convergent Lanczos complex Gamma approximation.}},
	url = {http://my.fit.edu/~gabdo/gamma.txt},
	year = {2001}
}

@misc{toth:gamma,
	author = {Viktor T. Toth},
	keywords = {lanczos, gamma, special function, math, mathematics},
	title = {{The Gamma function}},
	url = {http://www.rskey.org/CMS/index.php/the-library/11},
}

@article{vigna:2014,
	abstract = {xorshift\sup{*} generators are a variant of Marsaglia's xorshift generators that eliminate linear artifacts typical of generators based on \\(Z/2Z\\)-linear operations using multiplication by a suitable constant. Shortly after high-dimensional xorshift\sup{*} generators were introduced, Saito and Matsumoto suggested a different way to eliminate linear artifacts based on addition in \\(Z/2^{32}Z\\), leading to the XSadd generator. Starting from the observation that the lower bits of XSadd are very weak, as its reverse fails systematically several statistical tests, we explore xorshift+, a variant of XSadd using 64-bit operations, which leads, in small dimension, to extremely fast high-quality generators.},
	author = {Sebastiano Vigna},
	doi = {},
	journal = {CoRR},
	keywords = {random, prng, rng, marsaglia, xorshift, uniform, rand},
	month = {apr},
	number = {},
	pages = {},
	title = {{Further scramblings of Marsaglia's xorshift generators}},
	url = {https://arxiv.org/abs/1404.0390},
	volume = {abs/1404.0390},
	year = {2014},
}

@article{marsaglia:2003,
	abstract = {Description of a class of simple, extremely fast random number generators (RNGs) with periods \\(2^k −1\\) for \\(k = 32, 64, 96, 128, 160, 192\\). These RNGs seem to pass tests of randomness very well.},
	author = {George Marsaglia},
	doi = {10.18637/jss.v008.i14},
	journal = {Journal of Statistical Software},
	keywords = {random, rand, marsaglia, prng, rng, xorshift, uniform},
	note = {xoshift},
	number = {14},
	title = {{Xorshift RNGs}},
	url = {https://www.jstatsoft.org/article/view/v008i14},
	volume = {8},
	year = {2003},
}

@article{panneton:2005,
	abstract = {G. Marsaglia recently introduced a class of very fast xorshift random number generators, whose implementation uses three “xorshift” operations. They belong to a large family of generators based on linear recurrences modulo 2, which also includes shift-register generators, the Mersenne twister, and several others. In this article, we analyze the theoretical properties of xorshift generators, search for the best ones with respect to the equidistribution criterion, and test them empirically. We find that the vast majority of xorshift generators with only three xorshift operations, including those having good equidistribution, fail several simple statistical tests. We also discuss generators with more than three xorshifts.},
	acmid = {1113319},
	address = {New York, NY, USA},
	author = {Fran\c{c}ois Panneton and Pierre L'Ecuyer},
	doi = {10.1145/1113316.1113319},
	issn = {1049-3301},
	journal = {ACM Transactions on Modeling and Computer Simulation},
	issue_date = {October 2005},
	keywords = {random number generation, linear feedback shift register, linear recurrence modulo 2, xorshift, prng, rng, random, rand},
	month = {oct},
	number = {4},
	numpages = {16},
	pages = {346--361},
	publisher = {ACM},
	title = {On the Xorshift Random Number Generators},
	url = {http://doi.acm.org/10.1145/1113316.1113319},
	volume = {15},
	year = {2005},
}

@article{hellekalek:1998,
	abstract = {Every random number generator has its advantages and deficiencies. There are no "safe" generators. The practitioner's problem is how to decide which random number generator will suit his needs best. In this paper, we will discuss criteria for good random number generators: theoretical support, empirical evidence and practical aspects. We will study several recent algorithms that perform better than most generators in actual use. We will compare the different methods and supply numerical results as well as selected pointers and links to important literature and other sources. Additional information on random number generation, including the code of most algorithms discussed in this paper is available from our web-server under the address http://random.mat.sbg.ac.at/.},
	acmid = {284161},
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {P. Hellekalek},
	doi = {10.1016/S0378-4754(98)00078-0},
	issn = {0378-4754},
	issue_date = {June 1998},
	journal = {Mathematics and Computers in Simulation},
	month = {jun},
	number = {5-6},
	numpages = {21},
	pages = {485--505},
	publisher = {Elsevier Science Publishers B. V.},
	title = {{Good Random Number Generators Are (Not So) Easy to Find}},
	url = {http://dx.doi.org/10.1016/S0378-4754(98)00078-0},
	volume = {46},
	year = {1998},
}

@article{ahrens:1974,
	abstract = {Accurate computer methods are evaluated which transform uniformly distributed random numbers into quantities that follow gamma, beta, Poisson, binomial and negative-binomial distributions. All algorithms are designed for variable parameters. The known convenient methods are slow when the parameters are large. Therefore new procedures are introduced which can cope efficiently with parameters of all sizes. Some algorithms require sampling from the normal distribution as an intermediate step. In the reported computer experiments the normal deviates were obtained from a recent method which is also described.},
	author = {J.H. Ahrens and U. Dieter},
	doi = {10.1007/BF02293108},
	issn = {1436-5057},
	journal = {Computing},
	keywords = {random, prng, rng, rand, beta, computation, pseudorandom, number, generator},
	number = {3},
	pages = {223--246},
	title = {{Computer methods for sampling from gamma, beta, poisson and bionomial distributions}},
	url = {http://dx.doi.org/10.1007/BF02293108},
	volume = {12},
	year = {1974},
}

@article{hormann:1993a,
	abstract = {The transformed rejection method, a combination of inversion and rejection, which can be applied to various continuous distributions, is well suited to generate binomial random variates as well. The resulting algorithms are simple and fast, and need only a short set-up. Among the many possible variants two algorithms are described and tested: BTRS a short but nevertheless fast rejection algorithm and BTRD which is more complicated as the idea of decomposition is utilized. For BTRD the average number of uniforms required to return one binomial deviate less between 2.5 and 1.4 which is considerably lower than for any of the known uniformly fast algorithms. Timings for a C-implementation show that for the case that the parameters of the binomial distribution vary from call to call BTRD is faster than the current state of the art algorithms. Depending on the computer, the speed of the uniform generator used and the binomial parameters the savings are between 5 and 40 percent.},
	author = {Wolfgang H\"{o}rmann},
	doi = {10.1080/00949659308811496},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rand, prng, rng, binomial, pseudorandom, number, generator},
	number = {1-2},
	pages = {101-110},
	title = {{The generation of binomial random variates}},
	url = {http://dx.doi.org/10.1080/00949659308811496},
	volume = {46},
	year = {1993},
}

@article{box:1958,
	abstract = {},
	author = {G. E. P. Box and Mervin E. Muller},
	doi = {10.1214/aoms/1177706645},
	issn = {00034851},
	journal = {The Annals of Mathematical Statistics},
	keywords = {random, prng, rng, pseudorandom, rand, normal, gaussian, number, generator},
	month = {jun},
	number = {2},
	pages = {610--611},
	publisher = {The Institute of Mathematical Statistics},
	title = {{A Note on the Generation of Random Normal Deviates}},
	url = {http://dx.doi.org/10.1214/aoms/1177706645},
	volume = {29},
	year = {1958}
}

@article{bell:1968,
	abstract = {},
	acmid = {363547},
	address = {New York, NY, USA},
	author = {James R. Bell},
	doi = {10.1145/363397.363547},
	issn = {0001-0782},
	issue_date = {July 1968},
	journal = {Communications of the ACM},
	keywords = {frequency distribution, normal deviates, normal distribution, probability distribution, random, random number, random number generator, simulation, prng, rng},
	month = {jul},
	number = {7},
	pages = {498--},
	publisher = {ACM},
	title = {{Algorithm 334: Normal Random Deviates}},
	url = {http://doi.acm.org/10.1145/363397.363547},
	volume = {11},
	year = {1968},
}

@article{knop:1969,
	abstract = {},
	acmid = {362996},
	address = {New York, NY, USA},
	author = {R. Knop},
	doi = {10.1145/362946.362996},
	issn = {0001-0782},
	issue_date = {May 1969},
	journal = {Communications of the ACM},
	keywords = {frequency distribution, normal deviates, normal distribution, probability distribution, random, random number, random number generator, simulation},
	month = {may},
	number = {5},
	pages = {281--},
	publisher = {ACM},
	title = {{Remark on Algorithm 334 [G5]: Normal Random Deviates}},
	url = {http://doi.acm.org/10.1145/362946.362996},
	volume = {12},
	year = {1969},
}

@article{marsaglia:1964a,
	abstract = {A normal random variable \\(X\\) may be generated in terms of uniform random variables \\(u_1\\), \\(u_2\\), in the following simple way: 86 percent of the time, put \\(X = 2(u_1 + u_2 + u_3 - 1.5)\\),11 percent of the time, put \\(X = 1.5(u_1 + u_2 - 1)\\), and the remaining 3 percent of the time, use a more complicated procedure so that the resulting mixture is correct. This method takes only half again as long as the very fastest methods, is much simpler, and requires very little storage space.},
	author = {G. Marsaglia and T. A. Bray},
	doi = {10.1137/1006063},
	issn = {00361445},
	journal = {SIAM Review},
	keywords = {prng, rng, random, rand, pseudorandom, number, generator, normal, gaussian},
	month = {sep},
	number = {3},
	pages = {260--264},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{A Convenient Method for Generating Normal Variables}},
	url = {http://epubs.siam.org/doi/abs/10.1137/1006063},
	volume = {6},
	year = {1964},
}

@article{thomas:2007,
	abstract = {Rapid generation of high quality Gaussian random numbers is a key capability for simulations across a wide range of disciplines. Advances in computing have brought the power to conduct simulations with very large numbers of random numbers and with it, the challenge of meeting increasingly stringent requirements on the quality of Gaussian random number generators (GRNG). This article describes the algorithms underlying various GRNGs, compares their computational requirements, and examines the quality of the random numbers with emphasis on the behaviour in the tail region of the Gaussian probability density function.},
	acmid = {1287622},
	address = {New York, NY, USA},
	articleno = {11},
	author = {David B. Thomas and Wayne Luk and Philip H.W. Leong and John D. Villasenor},
	doi = {10.1145/1287620.1287622},
	issn = {0360-0300},
	issue_date = {2007},
	journal = {ACM Computing Surveys},
	keywords = {gaussian, random numbers, normal, simulation, random, rand, prng, rng, pseudorandom},
	month = {nov},
	number = {4},
	publisher = {ACM},
	title = {{Gaussian Random Number Generators}},
	url = {http://doi.acm.org/10.1145/1287620.1287622},
	volume = {39},
	year = {2007},
}

@article{marsaglia:2000a,
	abstract = {We offer a procedure for generating a gamma variate as the cube of a suitably scaled normal variate. It is fast and simple, assuming one has a fast way to generate normal variables. In brief: generate a normal variate \\(x\\) and a uniform variate \\(U\\) until \\(\ln(U) < 0.5 + d - dv + \ln(v)\\), then return \\(dv\\). Here, the gamma parameter is \\(\alpha \geq 1\\), and \\(v = (1 + x/ *** with \\(d = \alpha - 1/3\\). The efficiency is high, exceeding \\(0.951, 0.981, 0.992, 0.996\\) at \\(\alpha = 1,2,4,8\\). The procedure can be made to run faster by means of a simple squeeze that avoids the two logarithms most of the time; return \\(dv\\) if \\(U < 1 - 0.0331\\). We give a short C program for any \\(\alpha \geq 1\\), and show how to boost an \\(\alpha \lt 1\\) into an \\(\alpha \gt 1\\). The gamma procedure is particularly fast for C implementation if the normal variate is generated in-line, via the #define feature. We include such an inline version, based on our ziggurat method. With it, and an inline uniform generator, gamma variates can be produced in 400MHz CPUs at better than 1.3 million per second, with the parameter \\(\alpha\\) changing from call to call.},
	acmid = {358414},
	address = {New York, NY, USA},
	author = {George Marsaglia and Wai Wan Tsang},
	doi = {10.1145/358407.358414},
	issue_date = {Sept. 2000},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {gamma distribution, random number generation, ziggurat method, random, prng, rng, pseudorandom, gamma},
	month = {sep},
	number = {3},
	numpages = {10},
	pages = {363--372},
	publisher = {ACM},
	issn = {0098-3500},
	title = {{A Simple Method for Generating Gamma Variables}},
	url = {http://doi.acm.org/10.1145/358407.358414},
	volume = {26},
	year = {2000},
}

@article{kachitvichyanukul:1985,
    abstract = {The paper presents an exact, uniformly fast algorithm for generating random variates from the hypergeometric distribution. The overall algorithm framework is acceptance/ rejection and is implemented via composition. Three subdensities are used, one is uniform and the other two are exponential. The algorithm is compared with algorithms based on sampling without replacement, inversion, and aliasing. A comprehensive survey of existing algorithms is also given.},
	author = {Voratas. Kachitvichyanukul and Burce Schmeiser},
	doi = {10.1080/00949658508810839},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rand, prng, rng, pseudorandom, number, generator, hypergeometric},
	number = {2},
	pages = {127-145},
	title = {{Computer generation of hypergeometric random variates}},
	url = {http://dx.doi.org/10.1080/00949658508810839},
	volume = {22},
	year = {1985},
}

@article{nadler:2006,
	abstract = {\\(\em{Ziggurat}\\) and \\(\em{Monty Python}\\) are two fast and elegant methods proposed by Marsaglia and Tsang to transform uniform random variables to random variables with normal, exponential and other common probability distributions. While the proposed methods are theoretically correct, we show that there are various design flaws in the uniform pseudo random number generators (PRNG's) of their published implementations for both the normal and Gamma distributions. These flaws lead to non-uniformity of the resulting pseudo-random numbers and consequently to noticeable deviations of their outputs from the required distributions. In addition, we show that the underlying uniform PRNG of the published implementation of MATLAB's \texttt{randn}, which is also based on the Ziggurat method, is not uniformly distributed with correlations between consecutive pairs. Also, we show that the simple linear initialization of the registers in MATLAB's \texttt{randn} may lead to non-trivial correlations between output sequences initialized with different (related or even random unrelated) seeds. These, in turn, may lead to erroneous results for stochastic simulations.},
	author = {Boaz Nadler},
	doi = {},
	journal = {arXiv},
	keywords = {random, prng, rng, marsaglia, ziggurat, gaussian, monty python, uniform, rand},
	month = {mar},
	number = {},
	pages = {},
	title = {{Design Flaws in the Implementation of the Ziggurat and Monty Python methods (and some remarks on MATLAB randn)}},
	url = {https://arxiv.org/abs/math/0603058},
	volume = {abs/math/0603058},
	year = {2006},
}

@article{mcfarland:2016,
    abstract = {The ziggurat algorithm is a very fast rejection sampling method for generating pseudorandom numbers (PRNs) from statistical distributions. In the algorithm, rectangular sampling domains are layered on top of each other (resembling a ziggurat) to encapsulate the desired probability density function. Random values within these layers are sampled and then returned if they lie beneath the graph of the probability density function. Here, we present an implementation where ziggurat layers reside completely beneath the probability density function, thereby eliminating the need for any rejection test within the ziggurat layers. In the new algorithm, small overhanging segments of probability density remain to the right of each ziggurat layer, which can be efficiently sampled with triangularly shaped sampling domains. Median runtimes of the new algorithm for exponential and normal variates is reduced to 58\% and 53\%, respectively (collective range: 41–93\%). An accessible C library, along with extensions into Python and MATLAB/Octave are provided.},
	author = {Christopher D. McFarland},
	doi = {10.1080/00949655.2015.1060234},
	journal = {Journal of Statistical Computation and Simulation},
	keywords = {random, rang, prng, rng, pseudorandom, ziggurat, gaussian, normal},
	number = {7},
	pages = {1281--1294},
	title = {{A modified ziggurat algorithm for generating exponentially and normally distributed pseudorandom numbers}},
	url = {http://dx.doi.org/10.1080/00949655.2015.1060234},
	volume = {86},
	year = {2016},
}

@unpublished{doornik:2005,
	abstract = {The ziggurat is an efficient method to generate normal random samples. It is shown that the standard Ziggurat fails a commonly used test. An improved version that passes the test is introduced. Flexibility is enhanced by using a plug-in uniform random number generator. An efficient double-precision version of the ziggurat algorithm is developed that has a very high period.},
	author = {Jurgen A. Doornik},
	keywords = {random, rand, prng, rng, ziggurat, normal, gaussian, pseudorandom},
	note = {Provides an improved algorithm for generating normally distributed pseudorandom numbers via a ziggurat algorithm.},
	title = {{An Improved Ziggurat Method to Generate Normal Random Samples}},
	url = {https://www.doornik.com/research/ziggurat.pdf},
	year = {2005},
}

@article{marsaglia:2000b,
	abstract = {We provide a new version of our ziggurat method for generating a random variable from a given decreasing density. It is faster and simpler than the original, and will produce, for example, normal or exponential variates at the rate of 15 million per second with a C version on a 400MHz PC. It uses two tables, integers \\(k_i\\), and reals \\(w_i\\). Some 99% of the time, the required \\(x\\) is produced by: Generate a random 32-bit integer \\(j\\) and let \\(i\\) be the index formed from the rightmost 8 bits of j. If \\(j < k\\), return \\(x = j \cdot w_i\\). We illustrate with C code that provides for inline generation of both normal and exponential variables, with a short procedure for setting up the necessary tables.},
	author = {George Marsaglia and Wai Wan Tsang},
	doi = {10.18637/jss.v005.i08},
	issn = {1548-7660},
	journal = {Journal of Statistical Software},
	keywords = {random, rand, prng, rng, pseudorandom, number, generator, ziggurat, normal, gaussian},
	number = {1},
	pages = {1--7},
	title = {{The Ziggurat Method for Generating Random Variables}},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v005i08},
	volume = {5},
	year = {2000},
}

@article{marsaglia:1964b,
	abstract = {},
	author = {George Marsaglia},
	doi = {10.1080/00401706.1964.10490150},
	journal = {Technometrics},
	keywords = {random, prng, rng, pseudorandom, number, generator, rand, normal, gaussian},
	number = {1},
	pages = {101-102},
	title = {{Generating a Variable from the Tail of the Normal Distribution}},
	url = {http://dx.doi.org/10.1080/00401706.1964.10490150},
	volume = {6},
	year = {1964},
}

@article{park:1988,
	abstract = {Practical and theoretical issues are presented concerning the design, implementation, and use of a good, minimal standard random number generator that will port to virtually all systems.},
	acmid = {63042},
	address = {New York, NY, USA},
	author = {S. K. Park and K. W. Miller},
	doi = {10.1145/63039.63042},
	issn = {0001-0782},
	issue_date = {Oct. 1988},
	journal = {Communications of the ACM},
	keywords = {random, prng, rng, pseudorandom, number, generator},
	month = {oct},
	number = {10},
	numpages = {10},
	pages = {1192--1201},
	publisher = {ACM},
	title = {{Random Number Generators: Good Ones Are Hard to Find}},
	url = {http://doi.acm.org/10.1145/63039.63042},
	volume = {31},
	year = {1988},
}

@book{press:1992,
	author = {William H. Press and Brian P. Flannery and Saul A. Teukolsky and William T. Vetterling},
	isbn = {0521431085},
	keywords = {computing, software, programming},
	publisher = {Cambridge University Press},
	title = {{Numerical Recipes in C: The Art of Scientific Computing, Second Edition}},
	year = {1992},
}

@article{bays:1976,
	abstract = {},
	acmid = {355670},
	address = {New York, NY, USA},
	author = {Carter Bays and S. D. Durham},
	doi = {10.1145/355666.355670},
	issue_date = {March 1976},
	journal = {ACM Transactions on Mathematical Software},
	keywords = {lcg, random, rnd, minstd, rand, rng, prng, pseudorandom, number, generator, linear congruential generator},
	month = {mar},
	number = {1},
	numpages = {6},
	issn = {0098-3500},
	pages = {59--64},
	publisher = {ACM},
	title = {{Improving a Poor Random Number Generator}},
	url = {http://doi.acm.org/10.1145/355666.355670},
	volume = {2},
	year = {1976},
}

@book{herzog:2002,
	author = {T.N. Herzog and G. Lord},
	isbn = {9781566984331},
	keywords = {finance, monte carlo, prng, rng, random, minstd, minstd-shuffle},
	lccn = {2002015049},
	pages = {19--},
	publisher = {ACTEX Publications},
	title = {{Applications of Monte Carlo Methods to Finance and Insurance}},
	url = {https://books.google.com/books?id=vC7I\_gdX-A0C},
	year = {2002},
}

@book{knuth:1997,
	address = {Boston, MA, USA},
	author = {Donald E. Knuth},
	isbn = {0-201-89684-2},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	title = {{The Art of Computer Programming, Volume 2 (3rd Ed.): Seminumerical Algorithms}},
	year = {1997},
}

@article{hormann:1993b,
	abstract = {The transformed rejection method, a combination of the inversion and the rejection method, which is used to generate non-uniform random numbers from a variety of continuous distributions can be applied to discrete distributions as well. For the Poisson distribution a short and simple algorithm is obtained which is well suited for large values of the Poisson parameter \\(\mu\\), even when \\(\mu\\) may vary from call to call. The average number of uniform deviates required is lower than for any of the known uniformly fast algorithms. Timings for a C implementation show that the algorithm needs only half of the code but is - for \\(\mu\\) not too small - at least as fast as the current state-of-the-art algorithms.},
	author = {W. H\"{o}rmann},
	doi = {10.1016/0167-6687(93)90997-4},
	issn = {0167-6687},
	journal = {Insurance: Mathematics and Economics},
	keywords = {Rejection method, Inversion, Decomposition, Poisson variate generation, Uniformly fast algorithm},
	note = {},
	number = {1},
	pages = {39--45},
	title = {{The transformed rejection method for generating Poisson random variables}},
	url = {http://www.sciencedirect.com/science/article/pii/0167668793909974},
	volume = {12},
	year = {1993},
}

@article{riehle:2012,
	abstract = {The design of software development tools follows from what the developers of such tools believe is true about software development. A key aspect of such beliefs is the size of code contributions (commits) to a software project. In this paper, we show that what tool developers think is true about the size of code contributions is different by more than an order of magnitude from reality. We present this reality, called the commit size distribution, for a large sample of open source and selected closed source projects. We suggest that these new empirical insights will help improve software development tools by aligning underlying design assumptions closer with reality.},
	author = {Dirk Riehle and Carsten Kolassa and Michel A. Salim},
	issn = {1617-5468},
	journal = {Software Engineering 2012, GI-Edition Lecture Notes in Informatics},
	keywords = {software, git, commits, developer, computer science},
	pages ={59--70},
	title = {{Developer Belief vs. Reality: The Case of the Commit Size Distribution}},
	url = {http://arxiv.org/abs/1408.4644},
	volume = {abs/1408.4644},
	year = {2012},
}

@inproceedings{arafat:2009a,
	abstract = {With the growing economic importance of open source, we need to improve our understanding of how open source software development processes work. The analysis of code contributions to open source projects is an important part of such research. In this paper we analyze the size of code contributions to more than 9,000 open source projects. We review the total distribution and distinguish three categories of code contributions using a size-based heuristic: single focused commits, aggregate team contributions, and repository refactorings. We find that both the overall distribution and the individual categories follow a power law. We also suggest that distinguishing these commit categories by size will benefit future analyses.},
	author = {Oliver Arafat and Dirk Riehle},
	booktitle = {Proceedings of the 42nd Hawaii International Conference on System Sciences},
	keywords = {software, computer science, metrics, development, developer, git, commits},
	publisher = {IEEE Computer Society},
	title = {{The Commit Size Distribution of Open Source Software}},
	year = {2009},
}

@inbook{hofmann:2009,
	abstract = {The quantitative analysis of software projects can provide insights that let us better understand open source and other software development projects. An important variable used in the analysis of software projects is the amount of work being contributed, the commit size. Unfortunately, post-facto, the commit size can only be estimated, not measured. This paper presents several algorithms for estimating the commit size. Our performance evaluation shows that simple, straightforward heuristics are superior to the more complex text-analysis-based algorithms. Not only are the heuristics significantly faster to compute, they also deliver more accurate results when estimating commit sizes. Based on this experience, we design and present an algorithm that improves on the heuristics, can be computed equally fast, and is more accurate than any of the prior approaches.},
	address = {Berlin, Heidelberg},
	author = {Philipp Hofmann and Dirk Riehle},
	bookTitle = {{Open Source Ecosystems: Diverse Communities Interacting: 5th IFIP WG 2.13 International Conference on Open Source Systems, OSS 2009, Sk\"{o}vde, Sweden, June 3-6, 2009. Proceedings}},
	doi = {10.1007/978-3-642-02032-2_11},
	editor = {Cornelia Boldyreff and Kevin Crowston and Bj\"{o}rn Lundell and Anthony I. Wasserman},
	isbn = {978-3-642-02032-2},
	pages = {105--115},
	publisher = {Springer Berlin Heidelberg},
	title = {{Estimating Commit Sizes Efficiently}},
	url = {http://dx.doi.org/10.1007/978-3-642-02032-2_11},
	year = {2009},
}

@article {joanes:1998,
	abstract = {Over the years, various measures of sample skewness and kurtosis have been proposed. Comparisons are made between those measures adopted by well-known statistical computing packages, focusing on bias and mean-squared error for normal samples, and presenting some comparisons from simulation results for non-normal samples.},
	author = {D. N. Joanes and C. A. Gill},
	doi = {10.1111/1467-9884.00122},
	issn = {1467-9884},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	keywords = {bias, kurtosis, mean-squared error, skewness, algorithms, math, mathematics, statistics, stats},
	number = {1},
	pages = {183--189},
	publisher = {Blackwell Publishers Ltd},
	title = {{Comparing measures of sample skewness and kurtosis}},
	url = {http://dx.doi.org/10.1111/1467-9884.00122},
	volume = {47},
	year = {1998},
}

@inproceedings{hattori:2008,
	abstract = {Information contained in versioning system commits has been frequently used to support software evolution research. Concomitantly, some researchers have tried to relate commits to certain activities, e.g., large commits are more likely to be originated from code management activities, while small ones are related to development activities. However, these characterizations are vague, because there is no consistent definition of what is a small or a large commit. In this paper, we study the nature of commits in two dimensions. First, we define the size of commits in terms of number of files, and then we classify commits based on the content of their comments. To perform this study, we use the history log of nine large open source projects.},
	author = {Lile P. Hattori and Michele Lanza},
	booktitle = {ASE Workshops 2008. 23rd IEEE/ACM International Conference on Automated Software Engineering - Workshops, 2008.},
	doi = {10.1109/ASEW.2008.4686322},
	keywords = {history, software systems, frequency, statistical distributions, inspection, informatics, research and development management, content management, project management, engineering management, metrics, git, commits},
	publisher = {IEEE},
	title = {{On the nature of commits}},
	year = {2008},
}

@manual{cocomo2,
	author = {Barry Boehm},
	edition = {1.4},
	keywords = {software, estimation, management, development},
	organization = {University of Southern California},
	title = {{COCOMO II Model Definition Manual}},
	url = {http://www.dmi.usherb.ca/~frappier/IFT721/COCOMOII.PDF}
}

@inproceedings{arafat:2009b,
	abstract = {The development processes of open source soft-ware are different from traditional closed source development processes. Still, open source software is frequently of high quality. This raises the question of how and why open source software creates high quality and whether it can maintain this quality for ever larger project sizes. In this paper, we look at one particular quality indicator, the density of comments in open source software code. We find that successful open source projects follow a consistent practice of documenting their source code, and we find that the comment density is independent of team and project size.},
	acmid = {1640047},
	address = {New York, NY, USA},
	author = {Oliver Arafat and Dirk Riehle},
	booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
	doi = {10.1145/1639950.1640047},
	isbn = {978-1-60558-768-4},
	keywords = {comment density, data mining, empirical software engineering, open source, software evolution},
	location = {Orlando, Florida, USA},
	numpages = {8},
	pages = {857--864},
	publisher = {ACM},
	series = {OOPSLA '09},
	title = {{The Commenting Practice of Open Source}},
	url = {http://doi.acm.org/10.1145/1639950.1640047},
	year = {2009},
}

@inbook{kolassa:2013a,
	abstract = {A fundamental unit of work in programming is the code contribution (“commit”) that a developer makes to the code base of the project in work. We use statistical methods to derive a model of the probabilistic distribution of commit sizes in open source projects and we show that the model is applicable to different project sizes. We use both graphical as well as statistical methods to validate the goodness of fit of our model. By measuring and modeling a fundamental dimension of programming we help improve software development tools and our understanding of software development.},
	address = {Berlin, Heidelberg},
	author = {Carsten Kolassa and Dirk Riehle and Michel A. Salim},
	bookTitle = {SOFSEM 2013: Theory and Practice of Computer Science: 39th International Conference on Current Trends in Theory and Practice of Computer Science, \v{S}pindler\r{u}v Ml\'{y}n, Czech Republic, January 26-31, 2013. Proceedings},
	editor = {Peter van Emde Boas and Frans C. A. Groen and Giuseppe F. Italiano and Jerzy Nawrocki and Harald Sack},
	doi = {10.1007/978-3-642-35843-2_6},
	isbn = {978-3-642-35843-2},
	keywords = {software, development, git, commits, management, estimation},
	pages = {52--66},
	publisher = {Springer Berlin Heidelberg},
	title = {{A Model of the Commit Size Distribution of Open Source}},
	url = {http://dx.doi.org/10.1007/978-3-642-35843-2_6},
	year = {2013},
}

@inproceedings{kolassa:2013b,
	abstract = {A fundamental unit of work in programming is the code contribution ("commit") that a developer makes to the code base of the project in work. An author's commit frequency describes how often that author commits. Knowing the distribution of all commit frequencies is a fundamental part of understanding software development processes. This paper presents a detailed quantitative analysis of commit frequencies in open-source software development. The analysis is based on a large sample of open source projects, and presents the overall distribution of commit frequencies.\n\nWe analyze the data to show the differences between authors and projects by project size; we also includes a comparison of successful and non successful projects and we derive an activity indicator from these analyses. By measuring a fundamental dimension of programming we help improve software development tools and our understanding of software development. We also validate some fundamental assumptions about software development.},
	acmid = {2491073},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Carsten Kolassa and Dirk Riehle and Michel A. Salim},
	booktitle = {Proceedings of the 9th International Symposium on Open Collaboration},
	doi = {10.1145/2491055.2491073},
	isbn = {978-1-4503-1852-5},
	keywords = {software, development, management, estimation, commits, git, metrics},
	location = {Hong Kong, China},
	numpages = {8},
	pages = {18:1--18:8},
	publisher = {ACM},
	series = {WikiSym '13},
	title = {{The Empirical Commit Frequency Distribution of Open Source Projects}},
	url = {http://doi.acm.org/10.1145/2491055.2491073},
	year = {2013},
}

@article{hope:1968,
	abstract = {The use of Monte Carlo test procedures for significance testing, with smaller reference sets than are now generally used, is advocated. It is shown that, for given \\(\alpha = 1/n\\), \\(n\\) a positive integer, the power of the Monte Carlo test procedure is a monotone increasing function of the size of the reference set, the limit of which is the power of the corresponding uniformly most powerful test. The power functions and efficiency of the Monte Carlo test to the uniformly most powerful test are discussed in detail for the case where the test criterion is \\(N(\gamma, 1)\\). The cases when the test criterion is Student's t-statistic and when the test statistic is exponentially distributed are considered also.},
	author = {Adery C. A. Hope},
	issn = {00359246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	keywords = {chisq, chisquare, chi-squared, chi-square, goodness-of-fit, monte carlo, simulate, simulation},
	number = {3},
	pages = {582--598},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {{A Simplified Monte Carlo Significance Test Procedure}},
	url = {http://www.jstor.org/stable/2984263},
	volume = {30},
	year = {1968}
}

@article{patefield:1981,
	author = {W. M. Patefield},
	issn = {1467-9876},
	journal = {Applied Statistics},
	keywords = {algorithm, algo, statistics, stats, chi-square, chi2, chi-squared, chisquare, chisquared, contingency table},
	month = {mar},
	number = {1},
	pages = {91--97},
	title = {{Statistical Algorithms: {Algorithm AS 159}: An Efficient Method of Generating Random \\(R \times C\\) Tables with Given Row and Column Totals}},
	url = {http://lib.stat.cmu.edu/apstat/159},
	volume = {30},
	year = {1981},
}

@book{devroye:1986,
	author = {Luc Devroye},
	doi = {10.1007/978-1-4613-8643-8},
	isbn = {978-1-4613-8643-8},
	keywords = {random, pseudorandom, prng, rng, rand, generator, algorithms},
	publisher = {Springer New York},
	title = {{Non-Uniform Random Variate Generation}},
	url = {http://www.nrbook.com/devroye/},
	year = {1986},
}

@misc{v8:3006,
	abstract = {In Mac Chrome 33.0.1706.0 canary, Math.cos(Math.pow(2,120)) returns 0.47796506772457525. In Chromium ToT from today, after a V8 roll with the new sin/cos implementation using table lookup and interpolation, this now returns 0. The true value evaluated to full precision is closer to -0.925879. This also causes a test regression in webaudio that uses sin. This is highly unexpected that the new implementation causes a sine wave saved to a 16-bit wav file to produce different values.},
	keywords = {math, trig, trigonometry, native, standard, chrome, chromium, v8, bug},
	title = {{Inaccurate sin/cos values}},
	url = {https://bugs.chromium.org/p/v8/issues/detail?id=3006},
	year = {2013}
}

@misc{chromium:320097,
	abstract = {From examining the source code, the cause is likely some optimization recently introduced into V8 around Math.sin or Math.cos. It may be that the change in behavior is perfectly valid; this demo is known to exercise the full range of floating-point values. However, investigation is needed to confirm that a regression hasn't been introduced.},
	keywords = {math, sin, sine, cos, cosine, trig, trigonometry, bug, standard, chrome, chromium, v8},
	title = {{V8 version 3.23.4 changes results of trigonometric functions}},
	url = { https://bugs.chromium.org/p/chromium/issues/detail?id=320097},
	year = {2013}
}

@misc{v8:3089,
	abstract = {Let x = Math.pow(2,120). Math.sin(x) = 0.2446152181180111. Math.sin(-x) = -0.2970278622893754. You can argue whether there's any significance to Math.sin(x), but since sin(-x) = -sin(x) for all x, Math.sin should satisfy the same identity for any real x. Math.tan has the same issue, but it will be fixed if Math.sin is fixed.},
	keywords = {math, bug, sine, sin, standard, ieee754, floating-point, chrome, v8, trig, trigonometry},
	title = {{Math.sin(-x) is not -Math.sin(x)}},
	url = {https://bugs.chromium.org/p/v8/issues/detail?id=3089},
	year = {2014}
}
