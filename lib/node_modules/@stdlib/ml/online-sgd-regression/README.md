# Online Regression

> Online regression via [Stochastic Gradient Descent][stochastic-gradient-descent].


<section class="usage">

## Usage

``` javascript
var onlineSGDRegression = require( '@stdlib/ml/online-sgd-regression' );
```

#### onlineSGDRegression( \[options\] )

Creates an online linear regression model fitted via [stochastic gradient descent][stochastic-gradient-descent]. The module performs [L2 regularization][l2-regularization] of the model coefficients, shrinking them towards zero by penalizing the squared [euclidean norm][euclidean-norm] of the coefficients.

``` javascript
var randu = require( '@stdlib/math/base/random/randu' );
var normal = require( '@stdlib/math/base/random/normal' );
var model = onlineSGDRegression();

var x1;
var x2;
var i;
var y;

// Update model as data comes in...
for ( i = 0; i < 100000; i++ ) {
    x1 = randu();
    x2 = randu();
    y = 3.0 * x1 + -3.0 * x2 + 2.0 + normal( 0.0, 1.0 );
    model.update( [ x1, x2 ], y );
}
```

The function accepts the following `options`:

* __learningRate__: `string` denoting the learning rate to use. Can be `constant`, `pegasos` or `basic`. Default: `basic`.
* __loss__: `string` denoting the loss function to use. Can be `squaredError`, `epsilonInsensitive` or `huber`. Default: `squaredError`.
* __epsilon__: insensitivity parameter. Default: `0.1`.
* __lambda__: regularization parameter. Default: `1e-3`.
* __eta0__: constant learning rate. Default: `0.02`.
* __intercept__: `boolean` indicating whether to include an intercept. Default: `true`.

``` javascript
var model = onlineSGDRegression({
    'loss': 'squaredError',
    'lambda': 1e-4
});
```

The `learningRate` decides how fast or slow the weights will be updated towards the optimal weights. Let `i` denote the current iteration of the algorithm (i.e. the number of data points having arrived). The possible learning rates are:

| Option          | Definition                |
|:---------------:|:-------------------------:|
| basic (default) |  1000.0 / ( i + 1000.0 )  |
| constant        |  eta0                     |
| pegasos         |  1.0 / ( lambda * i )     |

The used loss function is specified via the `loss` option. The available options are:

* __epsilonInsensitive__: Penalty is the absolute value of the error whenever the absolute error exceeds epsilon and zero otherwise.
* __huber__: Squared-error loss for observations with error smaller than epsilon in magnitude, linear loss otherwise. Should be used in order to decrease the influence of outliers on the model fit.
* __squaredError__: Squared error loss, i.e. the squared difference of the observed and fitted values.

The `lambda` parameter determines the amount of shrinkage inflicted on the model coefficients:

``` javascript
var createRandom = require( '@stdlib/math/base/random/randu' ).factory;

var model;
var coefs;
var opts;
var rand;
var x1;
var x2;
var i;
var y;

opts = { 'seed': 23 };
rand = createRandom( opts );

model = onlineSGDRegression({
    'lambda': 1e-5
});

for ( i = 0; i < 100; i++ ) {
    x1 = rand();
    x2 = rand();
    y = 3.0 * x1 + -3.0 * x2 + 2.0;
    model.update( [ x1, x2 ], y );
}

coefs = model.coefs;
// returns [ ~2.9997, ~-2.9994, ~1.9999 ]

rand = createRandom( opts );
model = onlineSGDRegression({
    'lambda': 1e-2
});

for ( i = 0; i < 100; i++ ) {
    x1 = rand();
    x2 = rand();
    y = 3.0 * x1 + -3.0 * x2 + 2.0;
    model.update( [ x1, x2 ], y );
}

coefs = model.coefs;
// returns [ ~2.7446, ~-2.5017, ~1.9323 ]
```

Higher values of `lambda` reduce the variance of the model coefficient estimates at the expense of introducing bias.

By default, the model contains an `intercept` term. To omit the `intercept`, set the corresponding option to `false`:

``` javascript
var model = onlineSGDRegression({
    'intercept': false
});
model.update( [ 1.4, 0.5 ], 2.0 );

var dim = model.coefs.length;
// returns 2

model = onlineSGDRegression();
model.update( [ 1.4, 0.5 ], 2.0 );

dim = model.coefs.length;
// returns 3
```

If `intercept` is `true`, an element equal to one is implicitly added to each `x` vector. Hence, this module performs regularization of the intercept term.

#### Model

Returned models have the following properties and methods...

#### model.update( x, y )

Update the model coefficients in light of incoming data. `y` must be a numeric response value, `x` a `numeric array` of predictors. The number of predictors is decided upon first invocation of this method. All subsequent calls must supply `x` vectors of the same dimensionality.

``` javascript
model.update( [ 1.0, 0.0 ], 5.0 );
```

#### model.predict( x )

Predict the response for a new feature vector `x`, where `x` must be a `numeric array` of predictors. Given feature vector `x = [x_0, x_1, ...]` and model coefficients `c = [c_0, c_1, ...]`, the prediction is equal to `x_0*c_0 + x_1*c_1 + ... + c_intercept`.

``` javascript
var yhat = model.predict( [ 0.5, 2.0 ] );
// returns <number>
```

#### model.coefs

Getter for the model coefficients / feature weights stored in an `array`. The coefficients are ordered as `[c_0, c_1,..., c_intercept]`, where `c_0` corresponds to the first feature in `x` and so on.

``` javascript
var coefs = model.coefs;
// returns <Array>
```

</section>

<!-- /.usage -->


<section class="notes">

## Notes

* Stochastic gradient descent is sensitive to the scaling of the features. One is best advised to either scale each attribute to `[0,1]` or `[-1,1]` or to transform them into z-scores with zero mean and unit variance. One should keep in mind that the same scaling has to be applied to test vectors in order to obtain accurate predictions.
* Since this module performs regularization of the intercept term, scaling the response variable to an appropriate scale is also highly recommended.

</section>

<!-- /.notes -->


<section class="examples">

## Examples

``` javascript
var randu = require( '@stdlib/math/base/random/randu' );
var normal = require( '@stdlib/math/base/random/normal' );
var onlineSGDRegression = require( '@stdlib/ml/online-sgd-regression' );

var model;
var rnorm;
var x1;
var x2;
var y;
var i;

rnorm = normal.factory( 0.0, 1.0 );

// Create model:
model = onlineSGDRegression({
    'lambda': 1e-7,
    'loss': 'squaredError',
    'intercept': true
});

// Update model as data comes in...
for ( i = 0; i < 10000; i++ ) {
    x1 = randu();
    x2 = randu();
    y = 3.0 * x1 + -3.0 * x2 + 2.0 + rnorm();
    model.update( [ x1, x2 ], y );
}

// Extract model coefficients:
console.log( model.coefs );

// Predict new observations:
console.log( 'y_hat = %d; x1 = %d; x2 = %d', model.predict( [0.9,0.1] ), 0.9, 0.1 );
console.log( 'y_hat = %d; x1 = %d; x2 = %d', model.predict( [0.1,0.9] ), 0.1, 0.9 );
console.log( 'y_hat = %d; x1 = %d; x2 = %d', model.predict( [0.9,0.9] ), 0.9, 0.9 );
```

</section>

<!-- /.examples -->


<section class="links">

[euclidean-norm]: https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm
[l2-regularization]: https://en.wikipedia.org/wiki/Tikhonov_regularization
[stochastic-gradient-descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent

</section>

<!-- /.links -->
